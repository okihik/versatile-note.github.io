[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m a recent graduate with a Master’s degree in Natural Resources and Environmental Studies from the University of Northern BC, where I worked under the supervision of Dr. Balbinder Deo. My research focused on estimating food loss and waste in restaurant operations through an in-depth case study—a topic that bridges my academic interests with real-world sustainability challenges.\nCurrently, I’m seeking opportunities to apply my research skills and passion for environmental studies in a professional setting. My academic journey has equipped me with expertise in quantitative analysis and a deep understanding of resource management, while my hands-on experience has given me practical insights into the food service industry.\n\n\n\nThe word Himagineer blends the Japanese concept of “Hima” (暇), representing leisure or free time, with the English term “Imagineer,” signifying someone who creates and shares innovative ideas. For me, Himagineer encapsulates a personal philosophy: it’s during these moments of “Hima” – my leisure time – that I dedicate myself to “imagineering” pleasant information. This means engaging in activities where I can create, innovate, and share insights, turning personal pursuits into a source of shared knowledge and inspiration.\n\n\n\nEconomics & Analysis\nConsumer Theory in Microeconomics\nManagerial Economics\nCosts and Inventory Analysis\nGenerational Accounting Model\nRegression Analysis in Statistics\nExperimental Design\nState-Space (Bayesian) Model\nEmerging Technologies\nText Analysis in Computational Social Science\nReal-World Application\nCooking!!\nI’ve spent over ten years working in Chinese and Japanese restaurants, gaining invaluable experience in food preparation, kitchen operations, and understanding the practical challenges of food waste reduction.\nThis unique combination of academic rigour and industry experience shapes my perspective on sustainability, economics, and the intersection of theory and practice. Through this blog, I’ll share insights from my research, thoughts on current topics in environmental studies, statistics, and economics, and perhaps a few culinary adventures along the way.\nThanks for stopping by, and I hope you find something here that sparks your interest!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Himagineer",
    "section": "",
    "text": "Reproducible Research Reporting\n\n\nPart 1: Concept and File Management\n\n\n\nLatex\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrafting Academic Reports: A LaTeX and Overleaf Guide\n\n\npart 2\n\n\n\nReporting\n\n\n\n\n\n\n\n\n\nJul 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrafting Academic Reports: A LaTeX and Overleaf Guide\n\n\npart 1\n\n\n\nReporting\n\n\n\n\n\n\n\n\n\nJul 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Quarto Blog\n\n\n\n\n\n\nQuarto Blog\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/create a Quarto blog/index.html",
    "href": "posts/create a Quarto blog/index.html",
    "title": "Create a Quarto Blog",
    "section": "",
    "text": "This is a tutorial on how to create a new Quarto blog using the RStudio interface.\nThis tutorial is based on this website, and inspired by this blog and this video.\nThe procedure for publishing a Quarto blog using GitHub Pages and GitHub Actions is as follows:"
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#project-creation-in-rstudio",
    "href": "posts/create a Quarto blog/index.html#project-creation-in-rstudio",
    "title": "Create a Quarto Blog",
    "section": "1. Project Creation in RStudio",
    "text": "1. Project Creation in RStudio\n\nCreate a new Quarto blog project in RStudio.\nName the directory with the desired URL (e.g., “yourusername.github.io” for a personal blog).\nEnsure “Create a git repository” and “Use renv with this project” are checked."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#github-repository-setup",
    "href": "posts/create a Quarto blog/index.html#github-repository-setup",
    "title": "Create a Quarto Blog",
    "section": "2. GitHub Repository Setup",
    "text": "2. GitHub Repository Setup\n\nCreate a new public repository on GitHub with the same name, “yourusername.github.io”, as the RStudio project directory."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#pushing-local-project-to-github",
    "href": "posts/create a Quarto blog/index.html#pushing-local-project-to-github",
    "title": "Create a Quarto Blog",
    "section": "3. Pushing Local Project to GitHub",
    "text": "3. Pushing Local Project to GitHub\n\nUse terminal to link the local project folder to the new GitHub repository.\nCommit and push the initial project files to the main branch on GitHub."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#creating-gh-pages-branch",
    "href": "posts/create a Quarto blog/index.html#creating-gh-pages-branch",
    "title": "Create a Quarto Blog",
    "section": "4. Creating gh-pages Branch",
    "text": "4. Creating gh-pages Branch\n\nOn GitHub, create a new branch named “gh-pages”."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#setting-up-github-actions-workflow",
    "href": "posts/create a Quarto blog/index.html#setting-up-github-actions-workflow",
    "title": "Create a Quarto Blog",
    "section": "5. Setting up GitHub Actions Workflow",
    "text": "5. Setting up GitHub Actions Workflow\n\nIn your RStudio project, create the folder structure: “.github/workflows/”.\nInside workflows, create a new text file named publish.yml.\nCopy the R EnV example code chunk from the Quarto documentation for publishing to GitHub Pages and paste it into publish.yml.\nSave, commit, and push these changes to the main branch on GitHub."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#configuring-github-pages",
    "href": "posts/create a Quarto blog/index.html#configuring-github-pages",
    "title": "Create a Quarto Blog",
    "section": "6. Configuring GitHub Pages",
    "text": "6. Configuring GitHub Pages\n\nOn GitHub, go to your repository’s Settings, then Pages.\nUnder “Branch”, select gh-pages and click Save."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#monitoring-workflow-and-visiting-site",
    "href": "posts/create a Quarto blog/index.html#monitoring-workflow-and-visiting-site",
    "title": "Create a Quarto Blog",
    "section": "7. Monitoring Workflow and Visiting Site",
    "text": "7. Monitoring Workflow and Visiting Site\n\nGo to the Actions tab on GitHub to monitor the running workflows.\nOnce all workflows have successfully completed, return to Settings and then Pages.\nClick “Visit site” to view your live Quarto website.\n\nAfter this setup, any future changes committed to the repository will automatically update the blog without local rendering."
  },
  {
    "objectID": "about.html#what-drives-my-curiosity",
    "href": "about.html#what-drives-my-curiosity",
    "title": "About me",
    "section": "",
    "text": "Economics & Analysis\nConsumer Theory in Microeconomics\nManagerial Economics\nCosts and Inventory Analysis\nGenerational Accounting Model\nRegression Analysis in Statistics\nExperimental Design\nState-Space (Bayesian) Model\nEmerging Technologies\nText Analysis in Computational Social Science\nReal-World Application\nCooking!!\nI’ve spent over ten years working in Chinese and Japanese restaurants, gaining invaluable experience in food preparation, kitchen operations, and understanding the practical challenges of food waste reduction.\nThis unique combination of academic rigour and industry experience shapes my perspective on sustainability, economics, and the intersection of theory and practice. Through this blog, I’ll share insights from my research, thoughts on current topics in environmental studies, statistics, and economics, and perhaps a few culinary adventures along the way.\nThanks for stopping by, and I hope you find something here that sparks your interest!"
  },
  {
    "objectID": "posts/academic reporting in tex/Academic Report.html",
    "href": "posts/academic reporting in tex/Academic Report.html",
    "title": "Crafting Academic Reports: A LaTeX and Overleaf Guide",
    "section": "",
    "text": "The Challenge of Academic Reporting\nThe academic world demands precision, clarity, and professionalism, and nowhere is this more evident than in the reports and assignments we submit. You’ve poured hours into your research, meticulously crunching numbers, dissecting texts, and forming insightful conclusions. But what happens when it’s time to present that hard work? Many students find themselves wrestling with word processors, battling formatting issues, misaligned equations, and stubborn bibliographies. Often, these technical struggles can detract from the quality of your valuable content.\n\n\nIntroducing LaTeX: The Gold Standard for Professional Documents\nWhat if there was a better way? A tool that handles complex typesetting with grace, ensures consistent formatting across your entire document, and lets you focus on what truly matters: your ideas. That tool is LaTeX. LaTeX isn’t just another word processor; it’s a document preparation system renowned in academia and science for its ability to produce stunning, professional-quality documents with unparalleled precision, especially when it comes to mathematical equations, complex figures, and intricate citations. It’s the gold standard for anyone serious about academic writing.\n\n\nOverleaf: Making LaTeX Accessible\nBut we get it—the idea of learning a new system, especially one that involves coding, can seem daunting. That’s where Overleaf comes in. Think of Overleaf as your collaborative, cloud-based hub for LaTeX. It strips away the installation hassles and steep learning curves, providing an intuitive online environment where you can write, compile, and share your LaTeX documents in real-time. It’s like Google Docs, but purpose-built for the power of LaTeX.\n\n\nWhat You’ll Learn in This Guide\nIn this guide, we’ll demystify the process of creating academic reports using LaTeX on Overleaf. We’ll explore why these tools are a game-changer for students and researchers alike. I’ll also walk you through a practical example based on an actual university assignment, providing concrete steps you can follow. By the end, you’ll be equipped to elevate your academic writing, producing reports that not only look professional but are also a joy to create."
  },
  {
    "objectID": "posts/academic reporting in tex/Academic Report1.html",
    "href": "posts/academic reporting in tex/Academic Report1.html",
    "title": "Crafting Academic Reports: A LaTeX and Overleaf Guide",
    "section": "",
    "text": "The Challenge of Academic Reporting\nThe academic world demands precision, clarity, and professionalism, and nowhere is this more evident than in the reports and assignments we submit. You’ve poured hours into your research, meticulously crunching numbers, dissecting texts, and forming insightful conclusions. But what happens when it’s time to present that hard work? Many students find themselves wrestling with word processors, battling formatting issues, misaligned equations, and stubborn bibliographies. Often, these technical struggles can detract from the quality of your valuable content.\n\n\nIntroducing LaTeX: The Gold Standard for Professional Documents\nWhat if there was a better way? A tool that handles complex typesetting with grace, ensures consistent formatting across your entire document, and lets you focus on what truly matters: your ideas. That tool is LaTeX. LaTeX isn’t just another word processor; it’s a document preparation system renowned in academia and science for its ability to produce stunning, professional-quality documents with unparalleled precision, especially when it comes to mathematical equations, complex figures, and intricate citations. It’s the gold standard for anyone serious about academic writing.\n\n\nOverleaf: Making LaTeX Accessible\nBut we get it—the idea of learning a new system, especially one that involves coding, can seem daunting. That’s where Overleaf comes in. Think of Overleaf as your collaborative, cloud-based hub for LaTeX. It strips away the installation hassles and steep learning curves, providing an intuitive online environment where you can write, compile, and share your LaTeX documents in real-time. It’s like Google Docs, but purpose-built for the power of LaTeX.\n\n\nWhat You’ll Learn in This Guide\nIn this guide, we’ll demystify the process of creating academic reports using LaTeX on Overleaf. We’ll explore why these tools are a game-changer for students and researchers alike. I’ll also walk you through a practical example based on an actual university assignment, providing concrete steps you can follow. By the end, you’ll be equipped to elevate your academic writing, producing reports that not only look professional but are also a joy to create."
  },
  {
    "objectID": "posts/academic reporting in tex 2/index.html",
    "href": "posts/academic reporting in tex 2/index.html",
    "title": "Crafting Academic Reports: A LaTeX and Overleaf Guide",
    "section": "",
    "text": "Procedure for Creating Your Academic Report in Overleaf\nHere’s a detailed, step-by-step procedure to recreate your academic report using Overleaf:\n\n\nStep 1: Create a New Project in Overleaf\nGo to Overleaf.com and log in or sign up.\nOn your dashboard, click “New Project”.\nSelect “Blank Project”.\nGive your project a meaningful name, e.g., “Academic Report Example.”\n\n\nStep 2: Set Up the Document Class and Packages (The Preamble)\nIn the main .tex file (usually main.tex), you’ll see a basic structure. Delete everything in main.tex.\nCopy and paste the following lines from your example code into the very beginning of your main.tex file. These lines define the document type and load essential packages:\nCode snippet\n\\documentclass{article}[12pt]\n\n\\usepackage[margin=1in]{geometry}\n\\usepackage[utf8]{inputenc}\n\\usepackage{bm,amsmath}\n\\usepackage{amsthm}\n\\usepackage{amsmath, amsfonts}\n\\usepackage[english]{babel}\n\\usepackage{comment}\n\\usepackage{tikz}\n\\usepackage{indentfirst}\n\\usepackage{apacite}\n\\usepackage{natbib}\n\\usepackage{setspace}\n\\usepackage{comment}\n\\usepackage{float}\n\\usepackage{bibentry}\n\\usetikzlibrary[patterns]\nExplanation:\n\n\\documentclass{article}\\[12pt\\]: Sets the document as an article with a 12pt font size.\n\\usepackage[margin=1in]{geometry}: Sets the page margins to 1 inch.\n\\usepackage[utf8]{inputenc}: Essential for handling various characters.\n\\usepackage{bm,amsmath},\\usepackage{amsthm},\\usepackage{amsmath, amsfonts}: These are crucial for advanced mathematical typesetting, including bold symbols and theorem environments.\n\\usepackage[english]{babel}: Sets the language to English for hyphenation and other linguistic rules.\n\\usepackage{comment}: Allows you to comment out large blocks of text.\n\\usepackage{tikz} and \\usetikzlibrary[patterns]: These are for creating high-quality vector graphics and diagrams directly within your LaTeX document, like the supply and demand curves in your example.\n\\usepackage{indentfirst}: Ensures the first paragraph of a section is indented.\n\\usepackage{apacite}, \\usepackage{natbib}, \\usepackage{bibentry}: These packages are for managing citations and bibliographies, specifically for APA style (apacite) and general flexible citation management (natbib).\n\\usepackage{setspace}: For controlling line spacing.\n\\usepackage{float}: Provides more control over figure and table placement.\n\n\n\nStep 3: Define Custom Commands and Environments (Still in the Preamble)\nImmediately after the\n\\usepackage\ncommands, add your custom definitions. These make your code more concise and consistent.\nCode snippet\n\\DeclareUnicodeCharacter{2212}{-}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\definecolor{solnblue}{rgb}{0,0,1}\n\\newenvironment{soln}{\\color{solnblue}}{}\n\\newcommand\\widebar[1]{\\mathop{\\overline{#1}}}\n\\renewcommand{\\vec}[1]{\\bm{#1}}\n\\newcommand{\\mat}[1]{\\bm{#1}}\n\\newcommand{\\tran}{^{\\mathstrut\\scriptscriptstyle\\top}}\nExplanation:\n\\DeclareUnicodeCharacter{2212}{-}: Handles a specific unicode character for a minus sign.\n\\DeclareMathOperator{\\E}{\\mathbb{E}}: Defines a custom operator for mathematical expectation.\n\\definecolor{solnblue}{rgb}{0,0,1} and \\newenvironment{soln}{\\color{solnblue}}{}: This creates a custom environment called soln that will display its content in blue, which is excellent for highlighting solutions or specific sections.\n\\\\newcommand and \\\\renewcommand: These define shorthand commands for common mathematical notations like a wide bar, vector, matrix, and transpose.\n\n\nStep 4: Add Title Information\nBefore \\begin{document}, include your title, authors, and date:\nCode snippet\n\\title{FRE 460: Assignment 5}\n\\author{Angie Pan, $13236147$\\\\\nAkihiko Mori, $50120161$}\n\\date{April 9, 2021}\nExplanation:\n\\title, \\author, and\n\\date define these standard report elements. The \\\\ in \\author creates a line break.\n\n\nStep 5: Start the Document Body\nEvery LaTeX document’s main content goes between\n\\begin{document} and \\end{document}. Add these lines:\nCode snippet\n\\begin{document}\n\\maketitle\n\\vspace{1em}\n\n% Part A content will go here\n\n\\end{document}\nExplanation:\n\\maketitle: Generates the title page based on the \\title, \\author, and \\date commands.\n\\vspace{1em}: Adds a small vertical space.\n\n\nStep 6: Add Sections and Subsections\nYou can structure your report using \\section* (for unnumbered sections) and \\subsection* (for unnumbered subsections). Insert these within your \\begin{document} and \\end{document}:\nCode snippet\n\\section*{Part A: Fair Trade}\n% ... content for Part A ...\n\\subsection*{A-1. Supply and Demand Curves: No Fair-Trade}\n% ... content for A-1 ...\n\\subsection*{A-2. Equilibrium: No Fair-Trade}\n% ... content for A-2 ...\nExplanation: The * after \\section or \\subsection prevents them from being numbered, which is common in assignment reports.\n\n\nStep 7: Incorporate Text and Equations\nType your regular text as you would in any document.\nFor equations, use environments like align* for multi-line, aligned equations. Notice how & is used for alignment and \\\\ for new lines within the equation.\nCode snippet\n\\noindent\\textit{If rounding is necessary, round only the final answer; don’t round at intermediate steps.}\\\\\nThe industry-level marginal cost curve for producing an agricultural product is as follows:\n$MPC_n = 2Q$, where $Q$ is tonnes of the product produced.\nGlobal consumers get use benefits from consuming the product, with $MPB_n=8000-3Q$.\n\n% Example of equation from A-2:\n\\bgroup \\color{solnblue} % This is your custom blue solution environment\n\\noindent For $Q_n$,\n\\begin{align*}\n    MPB_n(Q_n) &= MPC_n(Q_n)\\\\\n    8000-3Q_n  &= 2Q_n\\\\\n    Q_n &= \\frac{8000}{5}\\\\\n    &= 1600.\n\\end{align*}\nFor $P_n$,\n\\begin{align*}\n    P_n &= MPC_n(Q_n)\\\\\n      &= 2Q_n\\\\\n     &= 2\\cdot 1600\\\\\n    &= 3200.\n\\end{align*}\nTherefore, the equilibrium if there is no Fair-Trade label in the agriculture product is $(Q_n, P_n) = (1,600,\\$3,200)$.\n\\egroup \nExplanation: \\noindent prevents indentation. Math mode is entered with $ for inline math (MPC_n=2Q) or environments like align* for displayed equations.\n\n\nStep 8: Create Figures with TikZ\nFor your detailed graphs, use the tikzpicture environment within a figure environment. This allows you to draw shapes, lines, and add text precisely.\nCode snippet\n\\bgroup \\color{solnblue}\n    \\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.4]\n                \\draw[thick,&lt;-&gt;] (0,13) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                % ... rest of your TikZ code ...\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product}\n            \\label{fig:A-1}\n        \\end{center}\n    \\end{figure}\n\\egroup \nExplanation:\n\\begin{figure}[H]: Defines a floating figure. [H] attempts to place it “here” (exactly where it’s defined in the code).\n\\begin{center}: Centers the content within the figure.\n\\begin{tikzpicture}[scale=0.4]: Starts the drawing environment. scale=0.4 adjusts the size of the drawing.\n\\draw, \\node: Fundamental TikZ commands for drawing lines/arrows and placing text/labels.\n\\caption{...}: Adds a caption to your figure.\n\\label{fig:A-1}: Allows you to cross-reference the figure later using \\ref{fig:A-1}.\n\n\nStep 9: Add Tables (if applicable)\nYour code includes a tabular environment for a table. This is how you’d structure it:\nCode snippet\n\\bgroup \\color{solnblue}\n\\begin{table}[H]\n        \\begin{center}\n            \\begin{tabular}{llll}\n               & Before & After & $\\Delta$\\\\ \\hline \\\\\n            CS & b+c+f  & a+b   & a-(c+f) \\\\\n            PS & g+h    & c+d+g & c+d-h   \\\\ \\hline \\\\\n            TS &        &       & a+d-(h+f) \n        \\end{tabular}\n        \\end{center}\n    \\end{table}\n% ... rest of your table explanation text ...\n\\egroup\nExplanation:\n\\begin{table}[H]: Defines a floating table.\n\\begin{tabular}{llll}: Creates the table structure. l means left-aligned column, r for right, c for center. llll means four left-aligned columns.\n\\\\: New row in a table.\n&: Column separator.\n\\hline: Horizontal line in the table.\n\n\nStep 10: Compile Your Document\nIn Overleaf, click the “Recompile” button (or it might recompile automatically if “Auto Compile” is on).\nThe right panel will display the generated PDF. Check for errors in the “Logs and output files” section if the compilation fails.\n\n\nStep 11: Continue Adding Content\nRepeat steps 6-9 for each section of your report, referencing your .tex code as a guide.\nMake sure to close all environments (e.g., every \\bgroup \\color{solnblue} needs a \\egroup).\nUse % for single-line comments in your .tex file to explain your code, as you’ve done in your example (%Question Part A).\n\\documentclass{article}[12pt]\n\n\\usepackage[margin=1in]{geometry}\n\\usepackage[utf8]{inputenc}\n\\usepackage{bm,amsmath}\n\\usepackage{amsthm}\n\\usepackage{amsmath, amsfonts}\n\\usepackage[english]{babel}\n\\usepackage{comment}\n\\usepackage{tikz}\n\\usepackage{indentfirst}\n\\usepackage{apacite}\n\\usepackage{natbib}\n\\usepackage{setspace}\n\\usepackage{comment}\n\\usepackage{float}\n\\usepackage{bibentry}\n\\usetikzlibrary[patterns]\n\n\n\\DeclareUnicodeCharacter{2212}{-}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\n\\definecolor{solnblue}{rgb}{0,0,1}\n\\newenvironment{soln}{\\color{solnblue}}{}\n\n\\newcommand\\widebar[1]{\\mathop{\\overline{#1}}}\n\\renewcommand{\\vec}[1]{\\bm{#1}}\n\\newcommand{\\mat}[1]{\\bm{#1}}\n\\newcommand{\\tran}{^{\\mathstrut\\scriptscriptstyle\\top}} \n%\\setlength\n%\\parindent{0pt}\n\n\\title{FRE 460: Assignment 5}\n\\author{Angie Pan, $13236147$\\\\\nAkihiko Mori, $50120161$}\n\\date{April 9, 2021}\n\n\\begin{document}\n\\maketitle\n\\vspace{1em}\n\n%Part A\n%\\clearpage\n\\section*{Part A: Fair Trade}\n%Question Part A\n\\noindent\\textit{If rounding is necessary, round only the final answer; don’t round at intermediate steps.}\\\\\nThe industry-level marginal cost curve for producing an agricultural product is as follows: \n$MPC_n = 2Q$, where $Q$ is tonnes of the product produced.\nGlobal consumers get use benefits from consuming the product, with $MPB_n=8000-3Q$.\n\n%Question Q1\n\\subsection*{A-1. Supply and Demand Curves: No Fair-Trade}\n\\noindent (10 points) In the space below, depict the supply and demand curves for the product.\n%Question end\n%Start Answer\n\\begin{soln}\n    \\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.4]\n                \\draw[thick,&lt;-&gt;] (0,13) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \\node [below] at (5,0) {$Q_n=1600$};\n                \\node [left] at (0,5) {$P_n=3200$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\draw(0,0)--(9,9) node[right]{$MPC_n=S$};\n                \\draw(0,10)--(10,0);\n                \\node[above] at (11,1) {$MPB_n=D$};\n                \\draw[dashed](0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product}\n            \\label{fig:A-1}\n        \\end{center}\n    \\end{figure}\n\\end{soln}\n%Answer End\n%Q2\n\\clearpage\n\\subsection*{A-2. Equilibrium: No Fair-Trade}\n\\noindent (5 points) Calculate the equilibrium price and quantity; show your work. Denote these values as $P_n$ and $Q_n$. Indicate these values in your graph above. \n\n%Answer Start\n\\begin{soln}\n\\noindent For $Q_n$,\n\\begin{align*}\n    MPB_n(Q_n) &= MPC_n(Q_n)\\\\\n    8000-3Q_n  &= 2Q_n\\\\\n    Q_n &= \\frac{8000}{5}\\\\\n    &= 1600.\n\\end{align*}\nFor $P_n$,\n\\begin{align*}\n    P_n &= MPC_n(Q_n)\\\\\n      &= 2Q_n\\\\\n     &= 2\\cdot 1600\\\\\n    &= 3200.\n\\end{align*}\nTherefore, the equilibrium if there is no Fair-Trade label in the agriculture product is $(Q_n, P_n) = (1,600,\\$3,200)$.\n\\end{soln}\n%Answer End\n%Q3\n%\\clearpage\n\\subsection*{A-3. Graph: Fair Trade}\n\\noindent (10 points) Suppose a third-party Fair-Trade label becomes available, for which all producers are eligible at a transaction cost of $\\$150$/tonne. Assume that every consumer values the Fair Trade label and receives $\\$550$/tonne of “warm glow” from consuming a Fair Trade certified product (in addition to their use benefits from consumption).\nIn the space below, reproduce your figure---including $P_n$ and $Q_n$---from the previous page, and overlay the demand and supply curves associated with the Fair-Trade product on this new graph.\n%Start Answer\n\\begin{soln}\n\\noindent Let $w$ and $t$ be the value of warm glow and the cost of transaction cost for producers, respectively. Then,\n    \\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.45]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,7.5)  {$P_{ft}$};\n                \\node [left] at (0,5) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node [below] at (13,0) {$2850$};\n                \n                \\node at (2.5,9)   {$a$};\n                \\node at (0.8,8.2) {$b$};\n                \\node at (1.5,6.2) {$c$};\n                \\node at (4,6.8)   {$d$};\n                \\node at (5.1,6)   {$e$};\n                \\node at (4,5.4)   {$f$};\n                \\node at (1,4)     {$g$};\n                \\node at (2,3)     {$h$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (5.5,7.5)--(5.5,0);\n                \\draw [dashed] (5,5)--(5,0);\n                \\draw [] (0,7.5)--(5.5,7.5);\n                \\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product with Fair Trade label}\n            \\label{fig:A-3}\n        \\end{center}\n    \\end{figure}\n    \\noindent If no Fair-Trade label exists, the equilibrium is $(Q_n, P_n) = (1600,3200)$, but if Fair-Trade certification is introduced, new equilibrium point is $(Q_{ft}, P_{ft}) = (1680,3510)$.\n\\end{soln}\n% %Answer End\n\\vspace{1em}\n%Q4\n\\subsection*{A-4. Equilibrium: Fair Trade}\n\\noindent (10 points) Calculate the equilibrium price $P_{ft}$ and quantity $Q_{ft}$ for the fair-trade product; show your work. Indicate these values in your graph above.\n\n%Start Answer\n\\begin{soln}\n\\noindent Given $w$ is $\\$550$ per tonne, and $t$ is $\\$150$ per tonne. Let a matrix $\\mathbf{A}$, vectors $\\mathbf{b}$, and $\\mathbf{c}$ be \n$\\begin{pmatrix}\n    3 & 1\\\\ 2 & -1\n\\end{pmatrix}$, \n$\\begin{pmatrix}\n    Q_{ft}\\\\ P_{ft}\n\\end{pmatrix}$, and\n$\\begin{pmatrix}\n    8550\\\\ -150\n\\end{pmatrix}$, respectively.\nThen, $\\mathbf{Ab}=\\mathbf{c}$ is the system of demand and supply equations in the market with Fair Trade label. Solving for $\\mathbf{b}$ brings us the equilibrium point. Since $\\mathbf{b}=\\mathbf{A}^{-1}\\mathbf{c}$\nand $\\mathbf{A}^{-1} = \\frac{1}{5}\n\\begin{pmatrix}\n    1 & 1\\\\2 &-3\n\\end{pmatrix}$,\nthen\n\n\\begin{align*}\n    \\mathbf{b} = \\begin{pmatrix}\n    Q_{ft}\\\\ P_{ft}\n\\end{pmatrix} &=\n\\frac{1}{5}\n\\begin{pmatrix}\n     1 & 1\\\\2 &-3\n\\end{pmatrix}\n\\begin{pmatrix}\n    8550\\\\ -150\n\\end{pmatrix}\\\\\n    &= \\frac{1}{5}\\begin{pmatrix}\n        8400 \\\\ 17550\n    \\end{pmatrix}\n    = \\begin{pmatrix}\n        1680 \\\\ 3510\n    \\end{pmatrix}\n\\end{align*}\n% \\begin{align*}\n%     MPB_n(Q_{ft})+w &= MPC_n(Q_{ft})+t\\\\\n%     8550-3Q_{ft}  &= 2Q_{ft}+150\\\\\n%     Q_{ft} &= \\frac{8400}{5}\\\\\n%     &= 1680.\n% \\end{align*}\n% For $P_{ft}$,\n% \\begin{align*}\n%     P_{ft} &= MPC_n(Q_{ft}) + t\\\\\n%       &= 2Q_{ft} + 150\\\\\n%      &= 2\\cdot 1600 + 150\\\\\n%     &= 3510.\n% \\end{align*}\nThe equilibrium of Fair-Trade label introduced is $(Q_{ft}, P_{ft}) = (1,680,\\$3,510)$.\n\\end{soln}\n%End Answer\n\n%Q5\n%\\clearpage\n\\subsection*{A-5. $\\Delta$CS, $\\Delta$PS, Warm Glow, and Transaction Costs in Graph}\n\\noindent (5 points) On your graph from question 3, label the areas corresponding to the change in CS, the change in PS, the total increase in transaction costs, and the total increase in warm glow. Report these graphical sums (i.e., the areas represented by labels, not the numerical values) here:\n\n\\vspace{1em}\n\\begin{soln}\n\\begin{tabular}{l l}\n    $\\Delta CS$ &= $(a+b) - (b+c+f) = a - (c+f) &gt; 0$\\\\\n    $\\Delta PS$ &= $(c+d+g)-(g+h) = c+d - h &gt;0$\n\\end{tabular}\n\\end{soln}\n\\vspace{1em}\n\n\\noindent Also indicate the areas corresponding to the total amount of warm glow and transaction costs associated with fair trade labeling:\n\n\\vspace{1em}\n\\begin{soln}\n\\begin{tabular}{l l}\n    Warm Glow         &= $a+ d$\\\\\n    Transaction Costs &= $h+f$ \n\\end{tabular}\n\\end{soln}\n\\vspace{1em}\n\n%Start Answer\n\\begin{soln}\n\\begin{table}[H]\n        \\begin{center}\n            \\begin{tabular}{llll}\n               & Before & After & $\\Delta$\\\\ \\hline \\\\\n            CS & b+c+f  & a+b   & a-(c+f) \\\\\n            PS & g+h    & c+d+g & c+d-h   \\\\ \\hline \\\\\n            TS &        &       & a+d-(h+f) \n        \\end{tabular}\n        \\end{center}\n    \\end{table}\n\\noindent The change in total welfare is decomposed into two parts: warm glow and transaction costs. The area of $a+d$ is the gain from net warm glow capture, and the area of $h+f$ is net loss of transaction costs.\n\\end{soln}\n%End Answer\n\n\\clearpage\n\\subsection*{A-6. $\\Delta$CS, $\\Delta$PS, and $\\Delta$W with Math}\n\\noindent (10 points) Calculate the following mathematically (show your work):\n\n\\vspace{1em}\n\\begin{soln}\n\\begin{tabular}{l l}\n    $\\Delta CS$ &= +\\$393,600\\\\\n    $\\Delta PS$ &= +\\$262,400\\\\\n    $\\Delta W $ &= +\\$656,000\n\\end{tabular}\n\\end{soln}\n\\vspace{1em}\n\n%Answer Start\n\\begin{soln}\n\\noindent For $\\Delta CS$, the surplus of consumer before the introduction of Fair-Trade label is defined as the area of $b+c+f$, which is\n$$\\frac{(8000-3200)\\cdot1600}{2} = \\$3,840,000.$$\nAnd the consumer surplus after the introduction of Fair-Trade label is the are of $a+b$, which is\n$$\\frac{(8550-3510)\\cdot1680}{2} = \\$4,233,600.$$\nTherefore, the change of $\\Delta CS$ is found to be:\n\\begin{align*}\n    \\Delta CS &= a+b - (b+c+f)\\\\\n    &= a-(c+f)\\\\\n    &= 4,233,600 - 3,840,000\\\\\n    &= +\\$393,600.\n\\end{align*}\n\\noindent For $\\Delta PS$, the surplus of producer before the introduction of Fair-Trade label is the area of $g+h$, which is\n$$\\frac{(3200-0)1600}{2} = \\$2,560,000.$$\nAnd the producer surplus after the introduction of Fair-Trade label is the are of $c+d+g$, which is\n$$\\frac{(3510-150)1680}{2} = \\$2,822,400.$$\nTherefore, the change of $\\Delta PS$ is\n\\begin{align*}\n    \\Delta PS &= c+d+g-(g+h)\\\\\n    &= c+d-h\\\\\n    &= 2,822,400 - 2,560,000\\\\\n    &= +\\$262,400.\n\\end{align*}\n\\noindent Overall, the change of the total welfare of the introduction of Fair-Trade label is\n\\begin{align*}\n    \\Delta W &= \\Delta CS + \\Delta PS\\\\\n    &= a-(c+f)+(c+d-h)\\\\\n    &= a+d-(f+h)\\\\\n    &= 393,600 + 262,400\\\\\n    &= +\\$656,000.\n\\end{align*}\n\\end{soln}\n%Answer End\n\n%Q7\n\\clearpage\n\\subsection*{A-7. Insufficient Warm-Glow Premium Received}\n\\noindent (5 points) In your own words, explain in economic terms why/whether $P_{ft}$ is not $\\$550$/tonne higher than the $P_n$. \nBonus: how does the increase in production cost for Fair-Trade products contribute to the change the price?\n\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.4]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,7.7)  {$P_{ft}$};\n                \\node [left] at (0,6.7)  {$P'$};\n                \\node [left] at (0,5.7)  {$P_{ft}-t$};\n                \\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                \\draw [dashed] (0,5)--(5,5)--(5,0);\n                \\draw [dashed] (0,5.5)--(5.5,5.5);\n                \\draw [dashed] (0,6.5)--(6.5,6.5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Fair Trade and Price Premium}\n            \\label{fig:A-7}\n        \\end{center}\n    \\end{figure}\n\n\\doublespacing\nAlthough the price $P_{ft}$ becomes higher, the rise is not by the full amount of the warm glow. This amount received is lower than the amount paid because of two effects: consumption and production sides.\n\nFrom the consumption's point of view, consumers buy more products with seeking a lower price. From the equilibrium point $(Q_n, P_n)$, after Fair-Trade label implemented, consumer's willingness-to-pay increases by warm glow, $w=\\$550$, shifting their demand up $P_n+w$. Since consumers seek lower price by increasing their consumption, price moves along the demand curve downward. Due to the upward sloping marginal private production, at the new equilibrium point, new price $P'$ is higher than the old price $P_n$, but $P'&lt;P_n+w$.\n\nFrom the production standpoint, as long as the supply is upward-sloping curve, warm glow allows new equilibrium, $P'$, to be higher price and quantity. As the quantity supplied increases, more opportunity cost is incurred with the price hiking. Now, an additional production cost, such as certification or transaction costs, brings the product price to rise to $P'+t$. Then, consumers reduce their consumption and farmers decrease their quantity supplied as well. At the end point, the price converges to $P_{ft}$, where $P_{ft}&gt;P'&gt;P_n$.\n\nFigrure \\ref{fig:A-7} indicates that $P_n+w \\geq P_{ft}$ or $P_n+w-(t+P_n) \\geq P_{ft} -(t+P_n)$, so $w-t \\geq P_{ft} -(t+P_n)$. If warm glow is sufficiently higher than transaction costs, $w&gt;t$, then it is likely to have positive price premium. However, if $w&lt;t$, then the price premium must be negative. In this case which $w=\\$550&gt;t=\\$150$, farmer could gain positive price premium, $P_{ft}-t-P_n = \\$160$ at most, which is obviously less than $w=\\$550$.\n\n\\end{soln}\n%Answer End\n\n%Q8\n\\clearpage\n\\subsection*{A-8. Fair-Trade Label Analysis with Perfect Elastic Demand}\n\\noindent (20 points) Suppose that, instead of a downward-sloping MPB curve, the MPB curve for consuming the good is flat at $\\$A$/tonne. Assuming still that the transaction cost is $\\$150$/tonne and consumers’ warm glow is $\\$550$/tonne, show mathematically and graphically that the label raises producer welfare but has no effect on consumer surplus. Assume that the MPC remains the same as $MPC = 2Q$. (I.e., compare CS, PS with and without the Fair-Trade label when the MPB curve is flat in both scenarios.)\n\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.5]\n                \\draw[thick,&lt;-&gt;] (0,12) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                %\\node [left] at (0,13) {$8550$};\n                %\\node [left] at (0,10) {$8000$};\n                %\\node [left] at (0,7.5)  {$P_{ft}$};\n                %\\node [left] at (0,5.7)  {$P_{ft}-t$};\n                %\\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,8) {$A+w=P_{ft}$};\n                \\node [left] at (0,5) {$A=P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (5,0) {$Q_n$};\n                \\node [below] at (6.2,0) {$Q_{ft}$};\n                %\\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node at (2.5,6.5) {$a$};\n                \\node at (0.9,4.1) {$b$};\n                \\node at (2,3) {$c$};\n                \\node at (5.4,6.5){$d$};\n                \\node at (5.5,4.5){$E$};\n                \\node at (6,8.5){$E'$};\n                \n                \\draw (0,0)--(9,9)  node[right] {$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,8)--(9,8) node [right] {$MPB_n+w$};\n                \\draw (0,5)--(9,5) node [right] {$MPB_n$};\n                %\\node [above] at (10,1) {$MPB_n$};\n                %\\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (6,8)--(6,0);\n                \\draw [dashed] (5,5)--(5,0);\n                %\\draw [dashed] (0,5.5)--(5,5.5);\n            \\end{tikzpicture}\n            \\caption{Perfect Elastic Demand and Supply for An Agricultural Product with Fair Trade}\n            \\label{fig:A-8}\n        \\end{center}\n    \\end{figure}\n    \\begin{table}[H]\n        \\begin{center}\n            \\begin{tabular}{llll}\n               & Before & After & $\\Delta$\\\\ \\hline \\\\\n            CS & 0 & 0 & 0 \\\\\n            PS & b+c & a+b & a-c \\\\ \\hline \\\\\n            TS &  &  & a-c \n        \\end{tabular}\n        \\end{center}\n    \\end{table}\n\n\\noindent  Suppose MPB is flat, so the demand function of the agriculture product is $A=P=MPB$ and also $MPC$ is a linear function of quantity, Q, $MPC = B+sQ$, where $B=0$ if no Fair-Trade label, $B=\\$150$ otherwise. Also Let $w$ and $t$ be \"warm glow\" and \"transaction costs\", respectively.\\\\\nThen, there an equilibrium $E$ exists such that $E = (Q_n, A)$ if no Fair-Trade label, and $E'$ at $(Q_{ft}, A+w)$ otherwise. Then, before introducing Fair-Trade label, for $CS_{before}$,\n\\begin{align*}\n    CS_{before} &= \\int\\displaylimits_{Q\\leq Q_n} MPB\\left(Q\\right) \\mathrm{d}Q - P_n\\cdot Q_n\\\\\n    &= \\int_0^{Q_n} A\\mathrm{d}Q - A\\cdot Q_n\\\\\n    &= A\\cdot Q_n - A\\cdot Q_n\\\\\n    &= 0.\n\\end{align*}\n\\noindent For $PS_{before}$,\n\\begin{align*}\n    PS_{before} &= P_n\\cdot Q_n - \\int\\displaylimits_{Q\\leq Q_n} MPC\\left(Q\\right) \\mathrm{d}Q\\\\\n    &= P_n\\cdot Q_n - \\int_0^{Q_n} sQ\\mathrm{d}Q\\\\\n    &= A\\cdot Q_n - \\left[\\frac{s}{2}Q_n^2\\right]\\\\\n    &= b+c.\n\\end{align*}\n\\noindent Now by \"warm glow\", $w$, $MPB_n$ is shifted to $MPB_n+ w$ and by \"transation costs\", $t$, $MPC_n$ is also shifted to $MPC_n+t$. Then, after introducing Fair-Trade label, for $CS_{after}$,\n\\begin{align*}\n    CS_{after} &= \\int\\displaylimits_{Q\\leq Q_{ft}} MPB'\\left(Q\\right) \\mathrm{d}Q - P_{ft}\\cdot Q_{ft}\\\\\n    &= \\int_0^{Q_{ft}} A+w\\mathrm{d}Q - (A+w)\\cdot Q_{ft}\\\\\n    &= (A+w)\\cdot Q_{ft} - (a+w)\\cdot Q_{ft}\\\\\n    &= 0.\n\\end{align*}\n\\noindent For $PS_{after}$,\n\\begin{align*}\n    PS_{after} &= P_{ft}\\cdot Q_{ft} - \\int\\displaylimits_{Q\\leq Q_{ft}} MPC\\left(Q\\right) \\mathrm{d}Q\\\\\n    &= P_{ft}\\cdot Q_{ft} - \\int_0^{Q_{ft}} B+sQ\\mathrm{d}Q\\\\\n    &= (A+w)\\cdot Q_{ft} - \\left[BQ_{ft}+\\frac{s}{2}Q_{ft}^2\\right]\\\\\n    &= a+b.\n\\end{align*}\nTherefore, the changes of $CS$ and $PS$ are:\n\\begin{align*}\n    \\Delta CS &= CS_{after} -CS_{before}\\\\\n    &= 0 - 0\\\\\n    &= \\$0.\n\\end{align*}\n\\begin{align*}\n    \\Delta PS &= PS_{after} -PS_{before}\\\\\n    &= \\frac{(A+w-t)\\cdot Q_{ft}}{2} - \\frac{A\\cdot Q_n}{2}\\\\\n    &= \\frac{(A+w-t)^2}{4} - \\frac{A^2}{4}\\\\\n    &= \\frac{(2A+w-t)(A+w-t-A)}{4}\\\\\n    &= \\$\\frac{(2A+w-t)(w-t)}{4}\n\\end{align*}\nOverall, given $w=550$ and $t=150$, the change in the total surplus of Fair-Trade labelling is\n\\begin{align*}\n    \\Delta TS &= 0 + 200A+40000 = +\\$200A+40000.\n\\end{align*}\n\\end{soln}\n%Answer End\n\n%Q9\n\\clearpage\n\\subsection*{A-9. Growing Warm Glow}\n\\noindent (15 points) Now suppose that instead of being the same for each unit consumed, the “warm glow” consumers receive is $550 + \\frac{Q}{4}$. Continue to assume that consumers’ use value is $8000-3Q$, the Fair-Trade transaction cost is $\\$150$/tonne, and $MPC = 2Q$. Use a graph and math to calculate the change in CS and PS when the Fair-Trade label is implemented. Also, in your own words, interpret the warm glow function---e.g. is marginal warm glow increasing or decreasing with consumption?---and explain whether you think this is realistic. Eg., for a product such as bananas, do you think the warm glow received from the final unit consumed would be larger or smaller than the warm glow received from the first unit consumed?\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.5]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(17,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,8.1)  {$P_{ft}$};\n                \\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6.5,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node [below] at (16,0) {$\\frac{34200}{11}$};\n                \n                \\node at (2.5,9.5) {$a$};\n                \\node at (0.6,8.7)   {$b$};\n                \\node at (1.5,6.2) {$c$};\n                \\node at (4,7){$d$};\n                \\node at (5.1,6.2){$e$};\n                \\node at (4,5.4) {$f$};\n                \\node at (1,4) {$g$};\n                \\node at (2,3) {$h$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(16,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (15,3) {$MPB_n+w(Q)$};\n                \\draw [dashed] (6.1,8.1)--(6.1,0);\n                \\draw [dashed] (5,5)--(5,0);\n                \\draw [] (0,8.1)--(6.1,8.1);\n                \\draw [] (0,5)--(5,5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product with Fair Trade Increasing Warm Glow}\n            \\label{fig:A-9}\n        \\end{center}\n    \\end{figure}\n    \n\\noindent Before Fair-Trade label implemented, the demand and supply functions are\n$$\n\\begin{cases}\n    MPB &= 8000-3Q\\\\\n    MPC &= 2Q.\n\\end{cases}\n$$\nSo, the equilibrium is where $MPB(Q_n)=MPC(Q_n)$\n\\begin{align*}\n    8000-3Q_n &= 2Q_n\\\\\n    Q_n &= 1600\\\\\n    P_n &= 3200\n\\end{align*}\nOnce the Fair-Trade label is introduced, if warm glow is defined as $550+\\frac{Q}{4}$ and FT transaction cost as $\\$150$, there an new equilibrium exists where $MPB(Q_{ft})+w(Q_{ft}) = MPC(Q_{ft})+t$:\n\\begin{align*}\n    MPB(Q_{ft})+w(Q_{ft}) &= MPC(Q_{ft})+t\\\\\n    8000-3Q_{ft}+550+\\frac{Q_{ft}}{4} &= 2Q_{ft}+150\\\\\n    Q_{ft} &= \\frac{33600}{19} \\approx 1,768.42\\\\\n    P_{ft} &= \\frac{70050}{19} \\approx \\$3,686.84\n\\end{align*}\nThen, calculate the change in consumer surplus:\n\\begin{align*}\n    \\Delta CS &= (a+ b) - (b+c+f)\\\\\n    &= \\frac{1}{2}\\left(8550-\\frac{70050}{19}\\right)\\frac{33600}{19} - \\frac{1}{2}\\left(8000-3200\\right)1600\\\\\n    &=\\frac{166080000}{361} \\approx \\$460,055.40.\n\\end{align*}\nFor the change of producer surplus is:\n\\begin{align*}\n    \\Delta PS &= (c+d+g)-(g+h)\\\\\n    &= \\frac{1}{2}\\left(\\frac{70050}{19}-150\\right)\\frac{33600}{19} - \\frac{1}{2}\\left(3200-0\\right)1600\\\\\n    &= \\frac{204800000}{361} \\approx \\$567,313.02.\n\\end{align*}\n\n\\doublespacing\nIncreasing the warm glow function as the quantity goes up means that the interval $I \\in \\{Q | (0,\\frac{34200}{11})\\}$, the warm glow function $f$ is $f(a) \\geq f(b), \\forall a&gt;b \\in I$, or $f'(Q)&gt;0, \\forall Q \\in I$. In this case, the more quantity demanded, the more warm glow is added; or, as an additional unit of the agriculture product consumed, the dollar value of warm glow increases by $f'(Q) = \\$\\frac{1}{4}$. Overall, the slope of the demand becomes relatively flat compared to a constant warm glow.\n\n\n\\doublespacing\nThis is an unrealistic assumption. However, we believe that it depends on a market product.\n%The higher price elasticity of demand implies that consumers are more willing and able to seek out substitutes for the agriculture product after a price change.\\\\\nOne of the aims at supporting Fair Trade's products is for small-scaled farmers and creators.  For a small quantity of a product, \n%The warm glow received from the final unit consumed would be smaller than the warm glow received from the first unity consumed. \nconsumers are willing to pay a large amount of warm glow, while for a high volume of a product, it is likely to be a small amount of warm glow. It is unlikely that consumers will be willing to pay a lot of warm glow for large quantities of produced goods. Also, with increasing the value of the brand, like the Fair Trade label, by reducing the willingness to search for substitutes, the willingness to pay becomes insensitive as quantity demanded rises. Therefore, decreasing the warm glow function with respect to quantity is realistic assumption for small-quantity in the market.\n\n\\doublespacing\nAnother aim of Fair Trade is to support farmers and workers to get paid a better price or wage and support the associated communities. The most common products, such as coffee and cacao, are produced in large scale with typically exported from the developing counties to the developed countries. For a large-scaled quantity of a product, especially plantations, consumer are likely willing to pay a relative large amount of warm glow by endorsing and consumer more in order to support workers, who work in severe conditions.\nHence, in this case, increasing the warm glow function with respect to quantity is valid assumption for large-quantity in the market.\n\n\\doublespacing\nOverall, we believe that a different product has a different warm glow function; however, we might assume that overall warm glow is a decreasing function with concave up, instead of a constant rate increasing function. For a product of small-scaled production, warm glow function is decreasing, but for large-scaled quantity goods, warm glow function is increasing.\n\n\\begin{figure}[H]\n\\begin{minipage}[c]{0.5\\linewidth}\n\\begin{tikzpicture}[scale=0.4]\n% Axis\n\\draw [thick](0,0) -- (10,0);\n\\draw [thick](0,0) -- (0,8);\n\\node [above] at (-0.2,8) {$Warm Glow (\\$/tonne)$};\n\\node [right] at (10,-0.2) {$Q(tonne)$};\n\\draw [thick] (0,2)--(9,7) node[right]{$w(Q)$};\n\\end{tikzpicture}\n\\caption{Increasing Warm Glow Function}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[c]{0.5\\linewidth}\n\\begin{tikzpicture}[scale=0.4]\n% Axis\n\\draw [thick](0,0) -- (10,0);\n\\draw [thick](0,0) -- (0,8);\n\\node [above] at (-0.2,8) {$Warm Glow (\\$/tonne)$};\n\\node [right] at (10,-0.2) {$Q(tonne)$};\n%Curve\n\\draw [thick] (1,8) to [out=280,in=175] (9,1);\n\\node [right] at (9,1) {$w(Q)$};\n\\end{tikzpicture}\n\\caption{Decreasing Warm Glow with Concave up}\n\\end{minipage}\n\\end{figure}\n\n\\end{soln}\n%Answer End\n\n%\\clearpage\n\\subsection*{A-10. Price Elasticity and Markets}\n\\noindent (10 points) Comparing your results from questions 8 and 9, explain in your own words why the allocation of benefits across consumers and producers differs in the two scenarios. Also conjecture which scenario (that in question 8 or question 9) would be more accurate in the case of a Fair Trade product such as coffee; explain your answer.\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n    \\begin{minipage}[c]{0.5\\linewidth}\n        \\begin{tikzpicture}[scale=0.35]\n            \\draw[thick,&lt;-&gt;] (0,13) node[above]{$\\$/tonne$}--(0,0)--(13,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                %\\node [left] at (0,13) {$8550$};\n                %\\node [left] at (0,10) {$8000$};\n                %\\node [left] at (0,7.5)  {$P_{ft}$};\n                %\\node [left] at (0,5.7)  {$P_{ft}-t$};\n                %\\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,8) {$P_{ft}=A+w$};\n                \\node [left] at (0,5) {$P_n=A$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (5,0) {$Q_n$};\n                \\node [below] at (6.2,0) {$Q_{ft}$};\n                %\\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node at (2.5,6.5) {$a$};\n                \\node at (0.9,4.1) {$b$};\n                \\node at (2,3) {$c$};\n                \\node at (5.4,6.5){$d$};\n                \\node at (5.5,4.5){$E$};\n                \\node at (6,8.5){$E'$};\n                \n                \\draw (0,0)--(9,9)  node[right] {$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,8)--(9,8) node [right] {$MPB_n+w$};\n                \\draw (0,5)--(9,5) node [right] {$MPB_n$};\n                %\\node [above] at (10,1) {$MPB_n$};\n                %\\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (6,8)--(6,0);\n                \\draw [dashed] (5,5)--(5,0);\n                %\\draw [dashed] (0,5.5)--(5,5.5);\n        \\end{tikzpicture}\n    \\caption{Perfect Elastic Demand with FT}\n    \\label{fig:A-10}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}[c]{0.5\\linewidth}\n        \\begin{tikzpicture}[scale=0.35]\n            \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(17,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,8.1)  {$P_{ft}$};\n                \\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6.5,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\node at (2.5,9.5) {$a$};\n                \\node at (0.6,8.7)   {$b$};\n                \\node at (1.5,6.2) {$c$};\n                \\node at (4,7){$d$};\n                \\node at (5.1,6.2){$e$};\n                \\node at (4,5.4) {$f$};\n                \\node at (1,4) {$g$};\n                \\node at (2,3) {$h$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(16,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (15,3) {$MPB_n+w(Q)$};\n                \\draw [dashed] (6.1,8.1)--(6.1,0);\n                \\draw [dashed] (5,5)--(5,0);\n                \\draw [] (0,8.1)--(6.1,8.1);\n                \\draw [] (0,5)--(5,5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n        \\end{tikzpicture}\n    \\caption{FT with Increasing Warm Glow}\n    \\label{fig:A-10-1}\n    \\end{minipage}\n\\end{figure}\n% compare elasticity\n\\doublespacing\n\\noindent A main difference between questions 8 and 9 is the price elasticity of demand for the good. In question 8, the price elasticity of demand is infinite, its in question 9 is relatively inelastic, while $MPC$ is same for both questions.\nDue to the price elasticity, the allocation of benefits is different in the two scenarios.\n% price and quantity\nThe introduction of the Fair-Trade label increases the equilibrium prices due to the existence of warm glow and transaction costs; but the equilibrium quantity depends on the amount of warm glow compared to the amount of transaction costs. \n\n\\doublespacing\n% welfare change CS PS warm glow and transaction costs\nAs found in question 8, the equilibrium price is determined by the $A$ and $w$, and the change in consumer surplus is zero, while the change in producer surplus is the area of $a-c$. So, the total change in welfare under the perfect elastic demand is $a-c$. Therefore, the net benefit of warm glow capture is the area of $a$, and the loss of transaction costs is $c$.\nOn the other hand in question 9, the equilibrium price depends on the demand and supply functions as well as the values of $w$ and $t$. And the change in consumer surplus is the area of $a-(c+f)$, while the change in producer surplus is $c+d-h$. So, the total change in welfare under the perfect elastic demand is $a+d-(f+h)$. The net welfare gain of warm glow capture is the area of $a+d$, and the welfare loss due to transaction costs is $f+h$.\n\n\\doublespacing\n% incidence \nFarmers are more likely to bear the transaction costs of Fair Trade if demand is more elastic. Under the perfect price elastic for demand, all of the transaction costs are borne by the farmers, and the premium paid to the farmers are $A+w-t$. However if the elasticity is relatively less, consumers can bear the increased transaction costs. The amount of warm glow greater than the costs, $w&gt;t$, creates a positive premium.\n\n% % compare increasing warm glow\nQuestion 9 is more accurate in the the case of a Fair Trade product, such as coffee. The price elasticity of demand with Fair Trade production, especially for coffee, could be inelastic compared to an infinite elasticity, with less possibility of consumers' substitution to the other products. Hence, an increase in production costs erodes the consumer's warm glow, but the farmers bear less of the burden.\n\n\\end{soln}\n%Answer End\n\n%\\clearpage\n\\vspace{5em}\n\\subsection*{A-11. Significance of Introducing Warm Glow and Truth Disclosure.}\n\\noindent (10 points) In your own words, discuss why/whether it makes sense to include “warm glow” in measures of economic benefits from a market. \nAlso discuss whether economic value is destroyed when consumers are informed that, for products such as coffee, very little of the Fair Trade price premium actually passes through to farmers and farm workers. \n\n%Answer Start\n\\begin{soln}\n\\doublespacing\n% reason of measuring warm glow\nPerfect competitive market allows consumers to have complete or \"perfect\" information about the product being sold and the prices charged by each production any time. So under this conditions, the inclusion of economic benefit of warm glow does not make sense. \nHowever, when consumers believe information, even if based on a false information, that workers on the farm are in a weak position, such as not receiving a fair wage or working under severe conditions, they will continue to purchase the product at a fair price, relative large price, in order to improve the lives of producers and workers in developing communities in a vulnerable position and to achieve self-reliance. And paying with warm glow makes consumers happy, so this increased willingness-to-pay should be measured in the economic benefits.\n%The warm glow represents the selfish joy that comes from \"doing good\" by \"doing your part\" to help others. \nAlso, empirical evidence show that Fair Trade producers gain higher prices than conventional farmers for their products. At the same time, surveys show that the premium paid by consumers is less than the premium received to farmers.\nIntegration of the warm glow into an economic model provides an explanation for the rise in Fair Trade prices and premiums.\n\n% Eroding Demand\n\\begin{figure}[H]\n    \\begin{minipage}[c]{0.5\\linewidth}\n            \\begin{tikzpicture}[scale=0.3]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                % \\node [left] at (0,13) {$8550$};\n                % \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,7.7)  {$P_{ft}$};\n                %\\node [left] at (0,6.7)  {$P'$};\n                \\node [left] at (0,5.7)  {$P_{ft}-t$};\n                \\node [left] at (0,4.6) {$P_n$};\n                % \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.5,0) {$Q_n$};\n                \\node [below] at (6.5,0) {$Q_{ft}$};\n                % \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (12,4) {$MPB_n+w$};\n                \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                \\draw [dashed] (0,5)--(5,5)--(5,0);\n                \\draw [dashed] (0,5.5)--(5.5,5.5);\n                %\\draw [dashed] (0,6.5)--(6.5,6.5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Warm Glow $\\geq$ Transaction costs}\n            \\label{fig:A-11-1}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}[c]{0.5\\linewidth}\n            \\begin{tikzpicture}[scale=0.3]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                % \\node [left] at (0,13) {$8550$};\n                % \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,8.7)  {$P_{ft}$};\n                % \\node [left] at (0,6.7)  {$P'$};\n                \\node [left] at (0,4)  {$P_{ft}-t$};\n                \\node [left] at (0,5.5) {$P_n$};\n                %\\node [left] at (0,2) {$150$};\n                \\node [below] at (4,0) {$Q_{ft}$};\n                \\node [below] at (5.8,0) {$Q_n$};\n                % \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,4)--(9,13) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (12,4) {$MPB_n+w$};\n                \\draw [dashed] (0,5)--(5,5)--(5,0);\n                \\draw [dashed] (0,8.5)--(4.5,8.5)--(4.5,0);\n                \\draw [dashed] (0,4.3)--(4.5,4.3);\n                % \\draw [dashed] (0,6.5)--(6.5,6.5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Warm Glow $\\leq$ Transaction Costs}\n            \\label{fig:A-11-2}\n    \\end{minipage}\n\\end{figure}\n\n\\doublespacing\n%w is exogenous\nWarm glow is defined exogenously, with an default value of $w$, which is assumed to be greater than the transaction costs, $t$, due to Fair Trade certification.\n% w&gt;t -&gt; positive premium\nConsumers believe that the all amount of warm glow $w$ goes directly to farmers. However, in question 7, as long as $w&gt;t$, positive price premium, $P_{ft}-t-P_n$ in Figure \\ref{fig:A-11-1}, goes to farmers, where $w$ will be subtracted with $t$. \n% w&lt;t -&gt; negative premium\nWhen consumers are informed that price premium farmers actually gets is less than the warm glow, or that money is not used for an improvement of schools or hospitals, then it shrinks demand with new warm glow $w' \\approx P_{ft}-t-P_n$, or even zero.\nPrice premium become negative if new warm glow, $w'$ is less than the transaction costs per unit $t$. This demand shrink erodes the Fair Trade price premium, and very little or even negative the premium, $P_{ft}-t-P_n$ in Figure \\ref{fig:A-11-2}, is paid to farm workers. Eventually, the economic value of the Fair Trade certification is negative.\n\n\\end{soln}\n%Answer End\n\n\\clearpage\n\\section*{Part B: Stats, Mental Accounting, and Food Stamps}\n\\noindent Using Google Scholar (https://scholar.google.ca/) or similar academic search engine, find a peer-reviewed article that gives statistical evidence of whether recipients of food stamps treat those benefits as fungible with other income and/or whether the recipients engage in mental accounting. You are welcome to review more than one scholarly article when executing this part of the assignment, but you only need to provide the following information for a single article. \n%Q1\n\\subsection*{B-a. Introduction of Study}\n\\noindent (10 points) Bibliographic information (use MLA or APA style)\n%\\clearpage\n%Answer Start\n\\begin{soln}\n\\bibliographystyle{apacite}\n\\bibliography{fre460a5.bib}\n\\nocite{*}\n\\end{soln}\n%Answer End\n\n%Q2\n\\subsection*{B-b. Data Description}\n\\noindent (20 points, approximately 200 words) Description of the data used by the authors, e.g. geographic location, years covered, how the data was generated/collected\\footnote{E.g. scanner data, classroom experiment, field experiment, household survey}, and key variables of interest. \n\n%Answer Start\n\\begin{soln}\n\\doublespacing\n\\citet{hastings2018snap} study how the Supplemental Nutrition Assistance Program (SNAP) influences spending by households. The SNAP program provides eligible households with a monthly electronic benefit that can only be spent on grocery stores. In the paper, the authors track detailed transaction records including method of payment that allows authors to infer to SNAP participation from February 2006 to December 2012 for regular customers at large grocery stores in Rhode Island. The food consumption trend of nearly half a million regular customers of a large US grocery stores is analyzed and studied. The authors are interested in the change of food consumption in response to the access to the SNAP benefit, increase in SNAP benefits and when the benefits run out. Then the authors compare the consumption difference when people receive cash benefits versus SNAP benefit to determine whether people treat different sources of income differently (Cash benefit vs. SNAP benefit). Additionally, the authors also analyze the percentage of SNAP benefit spend on food versus the percentage of SNAP benefit spend on other items to determine the SNAP benefit’s fungibility-shifting the purchase of food to purchase of other items with SNAP benefit. \n\n\\end{soln}\n%Answer End\n%Q3\n\\clearpage\n\\subsection*{B-c. Findings}\n\\noindent (30 points, approximately 300 words) Describe the authors’ central findings, particularly as they relate to the perceived fungibility of benefits received in the form of food stamps and other income.\n\n%Answer Start\n\\begin{soln}\n\\doublespacing\nThe authors develop three approaches to analyze the causal effect between SNAP and household spending. The three approaches include food consumption trends when households cross the eligibility threshold for SNAP, when the benefit ends, and when the SNAP benefit is increased. In all three cases, the marginal propensity to consume SNAP-eligible food (MPCF) out of SNAP benefit is $0.5$ to $0.6$. In the other words, approximately $50$ to $60$ cents out of one additional dollar of SNAP benefits gets spent on food. Compared to people when they get cash benefits or cash income, they spend less than $10$ cents out of every dollar on food. The main finding of the study indicates that the MPCF out of SNAP is greater than the MPCF out of other income sources. In addition, the greater MPCF for SNAP compared to cash is attributed to people’s separate mental account when they treat SNAP. The structured qualitative interviews reveal that some households report that they have planned to spend SNAP differently from cash, and quantitative evidence suggests that household reduce shopping effort for SNAP-eligible product more than for SNAP-ineligible product. The finding of the study has an important implication on economic activity. As people spend quickly after the recipient of SNAP benefit, it has a positive effect on total economic activity. Most importantly, the greater MPCF for SNAP confirms the benefit of SNAP on spending in food in food retail stores, thus improving food consumption among low income households.\n\n\\end{soln}\n%Answer End\n\n%Q4\n\\clearpage\n\\subsection*{B-d. Discussion for BC Food Insecurity}\n\\noindent (30 points, approximately 300 words) Discuss whether you think British Columbia should commence a food stamp program to address food insecurity. Be sure to allocate at least half your answer to discussing the evidence reported in the research study discussed in parts (b) and (c) of this question, but you are also welcome to discuss relevant issues that are commonly ignored by economists---stigma, for example. As always, answers supported with empirical evidence, rather than simply opinion, are the most persuasive. Be sure to cite any additional sources used to support your answers and give full bibliographic information for each\\footnote{Bibliographic information for these additional sources does not count toward the word count (300 words) recommended for this question.}.\n\n%Answer Start\n\\begin{soln}\n\\doublespacing\nI think British Columbia should commence a food stamp program to address the issue of food insecurity. Referring to the main conclusion on the study mentioned in part b, data from large grocery stores in Rhode Island is retrieved and studied in order to draw a relationship between the receipt of SNAP benefits and household consumption trend. Three approaches are developed to draw causal inference, and the study finds that the MPCF out of SNAP benefit is $0.5$ to $0.6$ which is significantly larger than the MPCF for other income sources (e.g. cash). The SNAP program reflects its goal in increasing food purchases among low income households. Additionally, there is another study suggests a positive correlation between SNAP and improved nutritional outcomes and negative correlation between SNAP and health care costs. Adults in households with food insecurity are more likely to use health care. The result of the study shows that $71\\%$ very low food-secure households use health care, compared to only $13\\%$ marginal food-secure households use health care \\citep{hastings2018snap}. Food insecurity imposes a heavy economic burden on society, including reduced productivity and increased health care costs. SNAP is a security net to protect people who are vulnerable to food insecurity. Average of $\\$1.40$ per person per meal in 2017 form a security net for the health and well-being of people with low income, which eventually lifting millions out of poverty and hunger \\citep{hastings2018snap}. The cost of implementing food stamp program is small relative to the cost of not implementing the program. Not only the existed food stamp program accomplished its goal in improving food security, but the program is also feasible from the financial feasibility and economic efficiency perspectives. \n\n\\end{soln}\n%Answer End\n\n%Reference list\n\\clearpage\n\\begin{soln}\n\\singlespacing\n%\\bibliographystyle{apacite}\n\\bibliography{fre460a5.bib}\n\\nocite{*}\n\\end{soln}\n\\end{document}"
  },
  {
    "objectID": "about.html#hello-world",
    "href": "about.html#hello-world",
    "title": "About me",
    "section": "",
    "text": "I’m a recent graduate with a Master’s degree in Natural Resources and Environmental Studies from the University of Northern BC, where I worked under the supervision of Dr. Balbinder Deo. My research focused on estimating food loss and waste in restaurant operations through an in-depth case study—a topic that bridges my academic interests with real-world sustainability challenges.\nCurrently, I’m seeking opportunities to apply my research skills and passion for environmental studies in a professional setting. My academic journey has equipped me with expertise in quantitative analysis and a deep understanding of resource management, while my hands-on experience has given me practical insights into the food service industry."
  },
  {
    "objectID": "about.html#what-is-himagineer",
    "href": "about.html#what-is-himagineer",
    "title": "About me",
    "section": "",
    "text": "The word Himagineer blends the Japanese concept of “Hima” (暇), representing leisure or free time, with the English term “Imagineer,” signifying someone who creates and shares innovative ideas. For me, Himagineer encapsulates a personal philosophy: it’s during these moments of “Hima” – my leisure time – that I dedicate myself to “imagineering” pleasant information. This means engaging in activities where I can create, innovate, and share insights, turning personal pursuits into a source of shared knowledge and inspiration."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#create-project-in-rstudio",
    "href": "posts/create a Quarto blog/index.html#create-project-in-rstudio",
    "title": "Create a Quarto Blog",
    "section": "1. Create Project in RStudio",
    "text": "1. Create Project in RStudio\n\nCreate a new Quarto blog project in RStudio.\nName the directory with the desired URL (e.g., “yourusername.github.io” for a personal blog).\nEnsure “Create a git repository” and “Use renv with this project” are checked."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#set-up-github-repository",
    "href": "posts/create a Quarto blog/index.html#set-up-github-repository",
    "title": "Create a Quarto Blog",
    "section": "2. Set up GitHub Repository",
    "text": "2. Set up GitHub Repository\n\nCreate a new public repository on GitHub with the same name, “yourusername.github.io”, as the RStudio project directory."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#push-local-to-github",
    "href": "posts/create a Quarto blog/index.html#push-local-to-github",
    "title": "Create a Quarto Blog",
    "section": "3. Push Local to GitHub",
    "text": "3. Push Local to GitHub\n\nUse terminal to link the local project folder to the new GitHub repository.\nCommit and push the initial project files to the main branch on GitHub."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#create-gh-pages-branch",
    "href": "posts/create a Quarto blog/index.html#create-gh-pages-branch",
    "title": "Create a Quarto Blog",
    "section": "4. Create gh-pages Branch",
    "text": "4. Create gh-pages Branch\n\nOn GitHub, create a new branch named “gh-pages”."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#set-up-github-actions-workflow",
    "href": "posts/create a Quarto blog/index.html#set-up-github-actions-workflow",
    "title": "Create a Quarto Blog",
    "section": "5. Set up GitHub Actions Workflow",
    "text": "5. Set up GitHub Actions Workflow\n\nIn your RStudio project, create the folder structure: “.github/workflows/”.\nInside workflows, create a new text file named publish.yml.\nCopy the R EnV example code chunk from the Quarto documentation for publishing to GitHub Pages and paste it into publish.yml.\nSave, commit, and push these changes to the main branch on GitHub."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#configure-github-pages",
    "href": "posts/create a Quarto blog/index.html#configure-github-pages",
    "title": "Create a Quarto Blog",
    "section": "6. Configure GitHub Pages",
    "text": "6. Configure GitHub Pages\n\nOn GitHub, go to your repository’s Settings, then Pages.\nUnder “Branch”, select gh-pages and click Save."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#monitore-workflow-and-visit-site",
    "href": "posts/create a Quarto blog/index.html#monitore-workflow-and-visit-site",
    "title": "Create a Quarto Blog",
    "section": "7. Monitore Workflow and Visit Site",
    "text": "7. Monitore Workflow and Visit Site\n\nGo to the Actions tab on GitHub to monitor the running workflows.\nOnce all workflows have successfully completed, return to Settings and then Pages.\nClick “Visit site” to view your live Quarto website.\n\nAfter this setup, any future changes committed to the repository will automatically update the blog without local rendering."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html",
    "href": "posts/UNBC thesis in latex/thesis1.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "As a fresh master’s graduate, I’ve found that one of the most important skills isn’t just knowing how to run a regression, but how to ensure my entire research process is transparent and reproducible. This means others can easily access and replicate my work, from the raw data to the final presentation.\nThis workflow, inspired by the principles of reproducible research, helped me efficiently manage my data, analysis, and writing. This series will guide you through setting up and using a similar workflow for your own research projects.\nIn this first post, we’ll cover the foundational concepts of a reproducible research workflow and how to effectively manage your project files.\n\n\nA reproducible research workflow is a system for organizing your entire research project—from data gathering to final presentation—in a way that allows you, or others, to easily and exactly reproduce your results. The key idea is to automate and link every step of the process, ensuring that your final paper is a direct, verifiable product of your data and analysis.\n\n\n\nBefore we dive into organizing our folders, let’s establish two guiding principles that are the bedrock of this entire workflow, as emphasized in Christopher Gandrud’s “Reproducible Research with R and RStudio”.\n\n\nThe ideal is to store every component of your research in plain text formats.\n\nData: Use text-based formats like comma-separated values (.csv).\nAnalysis & Documents: This is where R scripts (.R), Quarto/R Markdown (.qmd/.Rmd), LaTeX (.tex), and BibTeX (.bib) files shine. They are all plain text.\n\nWhy is this so important? Text files are universal and future-proof. They can be opened by virtually any software on any computer. Proprietary file formats can become obsolete, locking you out of your own work. Text files are also easily tracked by version control systems like Git.\n\n\n\nJust because a computer can read your file doesn’t mean a person can understand it. The goal is to make your files clear to two very important people: your future self and your collaborators.\n\nCommenting Your Code: Liberally add comments to your R scripts. Explain the why behind your code, not just the what.\nClear Naming Conventions: Give your files and variables descriptive names. A consistent style, like the camelCase we use here (e.g., mainAnalysis.qmd), makes files easy to read and reference.\nLiterate Programming: This is the key. Tools like Quarto allow you to weave your code, its output, and your explanatory text together in a single document.\n\n\n\n\n\nA research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#what-is-a-reproducible-research-workflow",
    "href": "posts/UNBC thesis in latex/thesis1.html#what-is-a-reproducible-research-workflow",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A reproducible research workflow is a system for organizing your entire research project—from data gathering to final presentation—in a way that allows you, or others, to easily and exactly reproduce your results. The key idea is to automate and link every step of the process, ensuring that your final paper is a direct, verifiable product of your data and analysis."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#file-management-the-foundation-of-reproducibility",
    "href": "posts/UNBC thesis in latex/thesis1.html#file-management-the-foundation-of-reproducibility",
    "title": "Reproducible Research Reporting: A LaTeX and Overleaf Guide",
    "section": "",
    "text": "Now, let’s build on these principles with a well-organized file structure. A logical folder system is the bedrock of a reproducible research project. It makes it easier to find your files, understand their purpose, and automate your workflow. Here’s a recommended file structure:\nyour-project-name/\n|-- data/\n|   |-- raw_data/\n|   |   |-- raw_data_source_1.csv\n|   |   `-- raw_data_source_2.csv\n|   |-- processed_data/\n|   |   `-- combined_data.csv\n|   `-- data_cleaning.R\n|-- analysis/\n|   |-- modeling.R\n|   `-- generate_figures.R\n|-- results/\n|   |-- tables/\n|   |   `-- regression_table.tex\n|   `-- figures/\n|       `-- scatter_plot.png\n|-- presentation/\n|   |-- thesis.qmd\n|   `-- presentation.qmd\n`-- README.md\nLet’s break down the purpose of each directory:\n\ndata/: This directory contains all of your data and the scripts used to process it.\n\nraw_data/: Store your original, untouched data files here.\nprocessed_data/: This is where you’ll save your cleaned and combined data sets.\ndata_cleaning.R: An R script that takes the raw data, cleans it, and saves the processed data.\n\nanalysis/: This directory holds the R scripts for your data analysis.\nresults/: This is for the outputs of your analysis.\n\ntables/: LaTeX or other formatted tables generated from your analysis.\nfigures/: Plots and charts generated from your analysis.\n\npresentation/: Your Quarto or LaTeX files for your final documents.\nREADME.md: A file that provides an overview of your project, explains the file structure, and gives instructions on how to reproduce the research.\n\nIn the next post, we will dive into the practical aspects of data analysis and generating results using R and RMarkdown.\nI will generate the next blog post in the series. Would you like me to proceed? I can also create a more detailed file structure for you, or we can discuss how to adapt this workflow to your specific interests in inventory management and the food industry."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#core-principles-text-files-and-human-readability",
    "href": "posts/UNBC thesis in latex/thesis1.html#core-principles-text-files-and-human-readability",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we dive into organizing our folders, let’s establish two guiding principles that are the bedrock of this entire workflow, as emphasized in Christopher Gandrud’s “Reproducible Research with R and RStudio”.\n\n\nThe ideal is to store every component of your research in plain text formats.\n\nData: Use text-based formats like comma-separated values (.csv).\nAnalysis & Documents: This is where R scripts (.R), Quarto/R Markdown (.qmd/.Rmd), LaTeX (.tex), and BibTeX (.bib) files shine. They are all plain text.\n\nWhy is this so important? Text files are universal and future-proof. They can be opened by virtually any software on any computer. Proprietary file formats can become obsolete, locking you out of your own work. Text files are also easily tracked by version control systems like Git.\n\n\n\nJust because a computer can read your file doesn’t mean a person can understand it. The goal is to make your files clear to two very important people: your future self and your collaborators.\n\nCommenting Your Code: Liberally add comments to your R scripts. Explain the why behind your code, not just the what.\nClear Naming Conventions: Give your files and variables descriptive names. A consistent style, like the camelCase we use here (e.g., mainAnalysis.qmd), makes files easy to read and reference.\nLiterate Programming: This is the key. Tools like Quarto allow you to weave your code, its output, and your explanatory text together in a single document."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#structuring-research-as-an-argument-the-toulmin-model",
    "href": "posts/UNBC thesis in latex/thesis1.html#structuring-research-as-an-argument-the-toulmin-model",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#structuring-research-the-toulmin-model",
    "href": "posts/UNBC thesis in latex/thesis1.html#structuring-research-the-toulmin-model",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html",
    "href": "posts/UNBC thesis in latex/thesis0.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "As a fresh master’s graduate, I’ve found that one of the most important skills isn’t just knowing how to run a regression, but how to ensure my entire research process is transparent and reproducible. This means others can easily access and replicate my work, from the raw data to the final presentation.\nThis workflow, inspired by the principles of reproducible research, helped me efficiently manage my data, analysis, and writing. This series will guide you through setting up and using a similar workflow for your own research projects.\nIn this first post, we’ll cover the foundational concepts of a reproducible research workflow and how to effectively manage your project files.\n\n\nA reproducible research workflow is a system for organizing your entire research project—from data gathering to final presentation—in a way that allows you, or others, to easily and exactly reproduce your results. The key idea is to automate and link every step of the process, ensuring that your final paper is a direct, verifiable product of your data and analysis.\n\n\n\nBefore we dive into organizing our folders, let’s establish two guiding principles that are the bedrock of this entire workflow, as emphasized in Christopher Gandrud’s “Reproducible Research with R and RStudio”.\n\n\nThe ideal is to store every component of your research in plain text formats.\n\nData: Use text-based formats like comma-separated values (.csv).\nAnalysis & Documents: This is where R scripts (.R), Quarto/R Markdown (.qmd/.Rmd), LaTeX (.tex), and BibTeX (.bib) files shine. They are all plain text.\n\nWhy is this so important? Text files are universal and future-proof. They can be opened by virtually any software on any computer. Proprietary file formats can become obsolete, locking you out of your own work. Text files are also easily tracked by version control systems like Git.\n\n\n\nJust because a computer can read your file doesn’t mean a person can understand it. The goal is to make your files clear to two very important people: your future self and your collaborators.\n\nCommenting Your Code: Liberally add comments to your R scripts. Explain the why behind your code, not just the what.\nClear Naming Conventions: Give your files and variables descriptive names. A consistent style, like the camelCase we use here (e.g., mainAnalysis.qmd), makes files easy to read and reference.\nLiterate Programming: This is the key. Tools like Quarto allow you to weave your code, its output, and your explanatory text together in a single document.\n\n\n\n\n\nA research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- explns/\n|   |   `-- 00_explanatory.R\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html#what-is-a-reproducible-research-workflow",
    "href": "posts/UNBC thesis in latex/thesis0.html#what-is-a-reproducible-research-workflow",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A reproducible research workflow is a system for organizing your entire research project—from data gathering to final presentation—in a way that allows you, or others, to easily and exactly reproduce your results. The key idea is to automate and link every step of the process, ensuring that your final paper is a direct, verifiable product of your data and analysis."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html#core-principles-text-files-and-human-readability",
    "href": "posts/UNBC thesis in latex/thesis0.html#core-principles-text-files-and-human-readability",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we dive into organizing our folders, let’s establish two guiding principles that are the bedrock of this entire workflow, as emphasized in Christopher Gandrud’s “Reproducible Research with R and RStudio”.\n\n\nThe ideal is to store every component of your research in plain text formats.\n\nData: Use text-based formats like comma-separated values (.csv).\nAnalysis & Documents: This is where R scripts (.R), Quarto/R Markdown (.qmd/.Rmd), LaTeX (.tex), and BibTeX (.bib) files shine. They are all plain text.\n\nWhy is this so important? Text files are universal and future-proof. They can be opened by virtually any software on any computer. Proprietary file formats can become obsolete, locking you out of your own work. Text files are also easily tracked by version control systems like Git.\n\n\n\nJust because a computer can read your file doesn’t mean a person can understand it. The goal is to make your files clear to two very important people: your future self and your collaborators.\n\nCommenting Your Code: Liberally add comments to your R scripts. Explain the why behind your code, not just the what.\nClear Naming Conventions: Give your files and variables descriptive names. A consistent style, like the camelCase we use here (e.g., mainAnalysis.qmd), makes files easy to read and reference.\nLiterate Programming: This is the key. Tools like Quarto allow you to weave your code, its output, and your explanatory text together in a single document."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html#structuring-research-the-toulmin-model",
    "href": "posts/UNBC thesis in latex/thesis0.html#structuring-research-the-toulmin-model",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- explns/\n|   |   `-- 00_explanatory.R\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "thesis1.html",
    "href": "thesis1.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Welcome back to our series on building a reproducible research workflow.\nIn Part 1, we established a philosophical and structural foundation for our project using the Toulmin model.\nWe set up a directory structure to house our Evidence, Warrant, and Claim.\nNow, we’ll focus on the first and most critical step of the research process: the Evidence.\nSpecifically, how do we gather and store our data and references in a way that is robust, transparent, and reproducible?\nThis post will cover the “how-to” for populating our 1_evidence/ directory.\n\n\nThe 1_evidence/rawData/ directory is the most important folder in your entire project.\nIt should be treated as a write-once, read-many-times archive.\nThe Golden Rule: Never, ever, ever manually edit your raw data files. Do not open rawDataSource1.csv in Excel to “quickly fix” a typo or delete a column.\nWhy is this so critical?\n\nTraceability: Any change to the data must be documented and scripted. If you manually edit the raw file, you lose the record of that change forever.\nIntegrity: It guarantees that you can always return to the original state of your data if you make a mistake in your data processing scripts.\nReproducibility: Anyone seeking to reproduce your work needs access to the exact same starting point you had.\n\nAll data cleaning, filtering, and transformation should happen programmatically in the scripts we will build later in the 2_warrantAndAnalysis/ directory.\n\n\n\nThe most reproducible way to get data is to script its collection. This creates a perfect, runnable record of where your evidence came from.\n\n\nSometimes, your data might come from a non-digital source, like a historical text or your own experimental observations. Instead of putting it in Excel first, you can enter it directly into R using a data.frame(). This is the most transparent way to digitize data.\n```{r createDataManually}\n# Creating a small dataset of hypothetical inventory data\ninventoryData &lt;- data.frame(\n  productId = c(\"A1\", \"A2\", \"B1\", \"B2\"),\n  category = c(\"Fruit\", \"Fruit\", \"Dairy\", \"Dairy\"),\n  stockLevel = c(150, 200, 80, 120),\n  storageTemp = c(4, 4, 2, 2)\n)\n\n# Save this manually created raw data for the record\n# Note: This is one of the few times we write to the evidence folder,\n# and only to create a reproducible starting point.\nwrite.csv(inventoryData, \"../1_evidence/rawData/manualInventory.csv\", row.names = FALSE)\n```\n\n\n\nIf data is hosted on a website in a plain-text format (like .csv), you can download it directly. This avoids manual “download and drop” and ensures you’re getting the data from the original source every time.\n```{r downloadFromWeb}\n# URL of a raw CSV file on the web\nurl &lt;- \"http://www.some-data-repository.com/food-data.csv\"\n\n# Use rio::import( ) to read the data directly into R\n# The rio package is great at handling various file types from a URL\nlibrary(rio)\nwebData &lt;- import(url, format = \"csv\")\n```\n\n\n\nGitHub is a common place to find data.\nTo download a file, it is similar to Method 2, make sure you use the “Raw” file URL.\n```{r downloadFromGitHub}\n# URL for the \"Raw\" version of a CSV file on GitHub\ngithubUrl &lt;- \"https://raw.githubusercontent.com/someuser/somerepo/main/data.csv\"\n\ngithubData &lt;- import(githubUrl, format = \"csv\" )\n```\n\n\n\nFor larger, more structured projects, data is often stored in a relational database (like PostgreSQL, MySQL, or SQL Server). R can connect directly to these databases and run SQL queries. This is an extremely powerful and reproducible method.\nTo do this, you’ll need two key packages:\n\nDBI: The standard database interface for R.\nA specific package for your database type (e.g., RPostgres for PostgreSQL, RMariaDB for MySQL/MariaDB).\n\nThe process involves three steps: connect, query, and disconnect.\n``{r downloadFromSQL, eval=false} # NOTE: This chunk is not evaluated (eval=false`) because it requires a live database. # 1. Load the necessary libraries library(DBI) library(RPostgres) # Or the package for your specific database"
  },
  {
    "objectID": "thesis1.html#the-sanctity-of-raw-data",
    "href": "thesis1.html#the-sanctity-of-raw-data",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "The 1_evidence/rawData/ directory is the most important folder in your entire project.\nIt should be treated as a write-once, read-many-times archive.\nThe Golden Rule: Never, ever, ever manually edit your raw data files. Do not open rawDataSource1.csv in Excel to “quickly fix” a typo or delete a column.\nWhy is this so critical?\n\nTraceability: Any change to the data must be documented and scripted. If you manually edit the raw file, you lose the record of that change forever.\nIntegrity: It guarantees that you can always return to the original state of your data if you make a mistake in your data processing scripts.\nReproducibility: Anyone seeking to reproduce your work needs access to the exact same starting point you had.\n\nAll data cleaning, filtering, and transformation should happen programmatically in the scripts we will build later in the 2_warrantAndAnalysis/ directory."
  },
  {
    "objectID": "thesis1.html#gathering-data-with-r-scripts",
    "href": "thesis1.html#gathering-data-with-r-scripts",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "The most reproducible way to gather data is to script the process. If you are sourcing data from the web, instead of manually downloading a file and placing it in your rawData folder, you can write an R script to do it for you.\nLet’s say you need data from the World Bank. You can use the WDI package in R to download it directly. A simple script might look like this:\n# Filename: downloadWorldBankData.R \n# Purpose: Downloads raw data on fertilizer consumption and saves it. \n# Load the necessary library \nlibrary(WDI) \n\n# Download the data for specific years \nfertilizerData &lt;- WDI(indicator = \"AG.CON.FERT.ZS\", start = 2010, end = 2020 ) \n\n# Save the raw data to the evidence folder \nwrite.csv(fertilizerData, \"../1_evidence/rawData/worldBankFertilizer.csv\", row.names = FALSE )\nThis script not only downloads the data but also serves as perfect documentation of its origin and the exact parameters of your download."
  },
  {
    "objectID": "thesis1.html#storing-your-literature-the-power-of-bibtex",
    "href": "thesis1.html#storing-your-literature-the-power-of-bibtex",
    "title": "Reproducible Research Reporting",
    "section": "Storing Your Literature: The Power of BibTeX",
    "text": "Storing Your Literature: The Power of BibTeX\nJust as your raw data is empirical evidence, your references are scholarly evidence. The best way to manage them is with a BibTeX file (.bib).\nA BibTeX file is a plain-text database of all your citations. Each entry has a unique key and contains all the necessary metadata for a reference.\nWhy use BibTeX?\n\nSingle Source of Truth: All your references are in one place. No more copying and pasting citations between documents.\nConsistency: It ensures every citation in your paper and every entry in your bibliography is formatted perfectly and consistently according to your chosen style.\nPortability: It’s a plain text file, making it easy to share, version control, and use across different projects and documents.\n\n\nBibTeX Examples\nLet’s look at a few common entry types you might have in your 1_evidence/literature.bib file.\n1. A Book: The @book type is for a complete book. Note that the pages field is generally not used for a whole book, but for a specific chapter (see next example).\n@book{Gandrud2020,\n  author    = {Christopher Gandrud},\n  title     = {Reproducible Research with R and RStudio},\n  edition   = {Third},\n  year      = {2020},\n  publisher = {CRC Press}\n}\n2. A Journal Article: The @article type is for a peer-reviewed journal article. It requires journal and volume fields.\n@article{Donoho2010,\n  author  = {David L. Donoho},\n  title   = {An invitation to reproducible computational research},\n  journal = {Biostatistics},\n  year    = {2010},\n  volume  = {11},\n  number  = {3},\n  pages   = {385--388}\n}\n3. A Chapter in an Edited Book: The @incollection type is perfect for citing a single chapter from a book with multiple authors and an editor. This is where the pages field is essential.\n@incollection{Buckheit1995,\n  author    = {Jonathan B. Buckheit and David L. Donoho},\n  title     = {Wavelab and Reproducible Research},\n  booktitle = {Wavelets and Statistics},\n  editor    = {Anestis Antoniadis},\n  year      = {1995},\n  publisher = {Springer},\n  address   = {New York},\n  pages     = {55--81}\n}\nThe unique key (e.g., Gandrud2020, Donoho2010) is what you’ll use to cite the work in your documents.\nBy the end of this stage, your 1_evidence/ directory should be populated with your raw, untouched data files and a comprehensive literature.bib file. You now have a solid, verifiable foundation of evidence.\nAfter these steps, you have a clean, well-documented, and fully-reproducible analytical dataset, ready for the statistical modeling we will cover in the next part.\nIn our next post, we will move on to Part 3: The Warrant - Processing Data and Weaving the Analysis, where we’ll start working in the 2_warrantAndAnalysis/ directory. We’ll write our first R scripts to read in the raw data, clean it, and prepare it for the main analysis."
  },
  {
    "objectID": "thesis1.html#gathering-data-scripting-the-collection",
    "href": "thesis1.html#gathering-data-scripting-the-collection",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "The most reproducible way to get data is to script its collection. This creates a perfect, runnable record of where your evidence came from.\n\n\nSometimes, your data might come from a non-digital source, like a historical text or your own experimental observations. Instead of putting it in Excel first, you can enter it directly into R using a data.frame(). This is the most transparent way to digitize data.\n```{r createDataManually}\n# Creating a small dataset of hypothetical inventory data\ninventoryData &lt;- data.frame(\n  productId = c(\"A1\", \"A2\", \"B1\", \"B2\"),\n  category = c(\"Fruit\", \"Fruit\", \"Dairy\", \"Dairy\"),\n  stockLevel = c(150, 200, 80, 120),\n  storageTemp = c(4, 4, 2, 2)\n)\n\n# Save this manually created raw data for the record\n# Note: This is one of the few times we write to the evidence folder,\n# and only to create a reproducible starting point.\nwrite.csv(inventoryData, \"../1_evidence/rawData/manualInventory.csv\", row.names = FALSE)\n```\n\n\n\nIf data is hosted on a website in a plain-text format (like .csv), you can download it directly. This avoids manual “download and drop” and ensures you’re getting the data from the original source every time.\n```{r downloadFromWeb}\n# URL of a raw CSV file on the web\nurl &lt;- \"http://www.some-data-repository.com/food-data.csv\"\n\n# Use rio::import( ) to read the data directly into R\n# The rio package is great at handling various file types from a URL\nlibrary(rio)\nwebData &lt;- import(url, format = \"csv\")\n```\n\n\n\nGitHub is a common place to find data.\nTo download a file, it is similar to Method 2, make sure you use the “Raw” file URL.\n```{r downloadFromGitHub}\n# URL for the \"Raw\" version of a CSV file on GitHub\ngithubUrl &lt;- \"https://raw.githubusercontent.com/someuser/somerepo/main/data.csv\"\n\ngithubData &lt;- import(githubUrl, format = \"csv\" )\n```\n\n\n\nFor larger, more structured projects, data is often stored in a relational database (like PostgreSQL, MySQL, or SQL Server). R can connect directly to these databases and run SQL queries. This is an extremely powerful and reproducible method.\nTo do this, you’ll need two key packages:\n\nDBI: The standard database interface for R.\nA specific package for your database type (e.g., RPostgres for PostgreSQL, RMariaDB for MySQL/MariaDB).\n\nThe process involves three steps: connect, query, and disconnect.\n``{r downloadFromSQL, eval=false} # NOTE: This chunk is not evaluated (eval=false`) because it requires a live database. # 1. Load the necessary libraries library(DBI) library(RPostgres) # Or the package for your specific database"
  },
  {
    "objectID": "thesis1.html#data-manipulation-forging-the-analytical-dataset",
    "href": "thesis1.html#data-manipulation-forging-the-analytical-dataset",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Raw data is rarely ready for analysis.\nThe following steps are fundamental for preparing your data.\nWe’ll use the dplyr package, a core part of the tidyverse, for its powerful and readable syntax.\n```{r dataManipulation}\n# Load the tidyverse suite of packages\nlibrary(tidyverse)\n\n# Let's assume we've loaded our 'inventoryData' and 'webData'\n# For this example, let's create a second data frame to merge\nsalesData &lt;- data.frame(\n  productId = c(\"A1\", \"B1\", \"A2\", \"B2\"),\n  unitsSold = c(50, 20, 75, 40)\n)\n```\n\n\n\nBinding: If you have two data frames with the exact same columns and you just want to stack them, use bind_rows().\nMerging: More commonly, you’ll want to join two datasets based on a common key (like productId). This is a merge or join.\n\n```{r mergingData}\n# Merge inventoryData with salesData by the 'productId' column\nfullData &lt;- left_join(inventoryData, salesData, by = \"productId\")\n```\n\n\n\n\nOrdering: Ensure your data is in a logical order, for example, by category and then product ID.\nSubsetting: Select only the rows and columns you need for a specific analysis.\n\n```{r orderingSubsetting}\n# Order the data\nfullData &lt;- fullData %&gt;%\n  arrange(category, productId)\n\n# Subset to keep only the 'Fruit' category and specific columns\nfruitData &lt;- fullData %&gt;%\n  filter(category == \"Fruit\") %&gt;%\n  select(productId, stockLevel, unitsSold)\n```\n\n\n\nYou often need to modify existing variables or create new ones.\n```{r recodingCreating}\n# Recoding a string variable: Let's make category names uppercase\nfullData &lt;- fullData %&gt;%\n  mutate(category = toupper(category))\n\n# Creating a new variable: Calculate the sell-through rate\nfullData &lt;- fullData %&gt;%\n  mutate(sellThroughRate = unitsSold / (stockLevel + unitsSold))\n```\n\n\n\nWhen you merge datasets, you might end up with duplicate columns (e.g., country.x, country.y).\nIt’s crucial to resolve this.\nDecide which column to keep and rename it, dropping the others.\n```{r handleDuplicates}\n# Let's pretend we had a 'country.x' and 'country.y'\n# We decide 'country.x' is the correct one and rename it\n\n# hypotheticalData &lt;- hypotheticalData %&gt;%\n#   rename(country = country.x) %&gt;%\n#   select(-country.y) # The minus sign drops the column\n```"
  },
  {
    "objectID": "thesis2.html",
    "href": "thesis2.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Welcome back. In Part 1, we established our project’s structure, and in Part 2, we scripted the gathering of our raw evidence.\nNow, we will construct the Warrant—the intellectual and computational engine of our research. This is where we transform our raw evidence into analytical results. Our entire process will be orchestrated from a single, central document: mainAnalysis.qmd, located in the 2_warrantAndAnalysis folder. This document will tell the complete story of our analysis, from initial exploration to final model output, making it the perfect summary to share with supervisors and collaborators.\n\n\nOur workflow is designed for clarity and modularity. The mainAnalysis.qmd file acts as the conductor, calling on specialized R scripts to perform specific tasks.\n\nLoad Evidence: Read the raw data from the 1_evidence directory.\nSource Explanatory Analysis: Call the 00_explanatory.R script to generate and load initial data objects.\nSource Research Models: Call the .R scripts from the models/ directory to run the core analyses and create model objects.\nSave Results: Use the objects created by the scripts to generate and save the final tables and figures into the results/ directory.\n\nLet’s walk through how this is implemented in our mainAnalysis.qmd file.\n\n\nWe begin with a setup chunk to load packages and a data chunk to load our raw data.\n```{{r setup, include=false}}\n# Load all necessary packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(knitr)\nlibrary(rio)\n# Set global options\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\n```\n### Data Loading\n```{{r loadData}}\n# The path goes up one directory (‘..’) then into the evidence folder.\ninventoryData &lt;- import(“../1_evidence/rawData/manualInventory.csv”)\n```\n\n\n\nWe execute our exploratory script. Its job is to perform initial calculations and create exploratory objects, which are then available for us to use in the main document.\n### Explanatory Analysis To begin, we explore the basic characteristics of our data by running the script in `explns/00_explanatory.R`.\n```{{r runExplanatoryAnalysis}}\n# This script creates objects like ‘summary_stats’\nsource(“explns/00_explanatory.R”)\n```\nNow we can display the `summary_stats` object that was just created.\n```{{r showExplanatoryStats, echo=true}}\n# Use kable for a clean summary table right here in our analysis\ndocument kable(summary_stats, caption = “Initial Data Summary”)\n```\n\n\n\nNext, we execute our core statistical models. The key is that the R scripts in the models/ folder should only create the model objects; they should not save any output themselves.\nExample: models/01_linearRegression.R\n# models/01_linearRegression.R\n# This script’s ONLY job is to run the model and produce the ‘modelFit’ object.\n# It assumes ‘inventoryData’ is already loaded in the environment.\nmodelFit &lt;- lm(stockLevel ~ storageTemp + category, data = inventoryData)\nWe call this from mainAnalysis.qmd to create the modelFit object in our environment.\n### Running the Main Regression Model\nNow, we run the linear model specified in our external script. This will create a model object called `modelFit` in our R environment, which we can then use to generate results.\n```{{r runLinearModel}}\nsource(“models/01_linearRegression.R”)\n```\n\n\n\nWith the modelFit object created, we can now generate our final outputs from within mainAnalysis.qmd. The key here is that we don’t just display them—we save them as files to the results/ directory. This decouples our analysis from our final presentation, allowing us to simply \\input or \\includegraphics in our LaTeX documents later.\n\n\nWe’ll use the broom package to clean our model output and knitr::kable() with the latex format to create a publication-ready table.\n```{{r saveTable}}\n# Tidy the model output into a clean data frame\nmodelSummary &lt;- tidy(modelFit)\n\n# Use kable to format as a LaTeX table and save_kable to write it to a file\n# The booktabs = TRUE option creates a more professional-looking table\nkable(modelSummary, format = “latex”, booktabs = TRUE,\ncaption = “Regression of Stock Levels on Storage Temperature and Category.”,\ncol.names = c(“Term”, “Estimate”, “Std. Error”, “Statistic”, “P-value”), digits = 3) %&gt;%\nsave_kable(file = “results/tables/regressionTable.tex”)\n```\nThis code creates a file named regressionTable.tex in the results/tables/ folder. It’s a self-contained piece of LaTeX code, ready to be included in our paper.\n\n\n\nSimilarly, we use ggplot2 to create our plot and the ggsave() function to save it as a high-quality image file.\n```{{r saveFigure}}\n# Create the plot object\nstockPlot &lt;- ggplot(inventoryData, aes(x = category, y = stockLevel, fill = category)) +\ngeom_boxplot() +\nlabs(title = “Stock Levels by Product Category”, x = “Category”, y = “Stock Level”) +\ntheme_minimal()\n# Save the plot to the figures directory\n# Saving as .pdf is great for LaTeX; .png is better for websites.\nggsave( “results/figures/stockByCategory.pdf”, plot = stockPlot, width = 7, height = 5 )\n```\nThis creates a file named stockByCategory.pdf in our results/figures/ folder.\nBy the end of this process, running the single mainAnalysis.qmd file has verifiably transformed your raw evidence into a complete set of polished, presentation-ready results, all neatly organized in their respective folders.\n\n\n\n\nIt is important to highlight that the tools shown above (knitr::kable and ggplot2::ggsave) are just one way to accomplish our goal. The R ecosystem is rich with packages designed for creating high-quality tables and figures. The key is not the specific package, but the workflow: use code to generate and save your results as files.\nHere are some excellent alternatives you might explore:\n\nFor Tables:\n\nxtable: One of the oldest and most powerful packages for creating LaTeX and HTML tables. It offers a huge amount of control over the final output.\ntexreg: Fantastic for creating regression tables from multiple models, side-by-side, in a format common in academic journals.\nmodelsummary: A modern and extremely flexible package that can output results to a huge variety of formats (LaTeX, HTML, Word, Markdown) and is very easy to use.\n\nFor Figures:\n\nBase R Graphics: R’s built-in plot() function is powerful and fast. To save a base R plot, you would wrap the code like this: pdf(\"myplot.pdf\"); plot(x,y); dev.off().\ngoogleVis: Creates interactive HTML-based charts and maps using the Google Charts API. These are excellent for web-based presentations (website.qmd) but cannot be embedded in static PDF documents.\n\n\nFeel free to experiment and find the packages that you prefer. As long as you can save their output to a file in your results directory, they will fit perfectly into this reproducible workflow.\nIn Part 4: The Claim - Presenting Your Research, we’ll see how to write the paper.tex and slides.tex files that effortlessly pull in these saved tables and figures to construct a polished, professional, and fully reproducible final document."
  },
  {
    "objectID": "thesis2.html#the-mainanalysis.qmd-your-projects-master-narrative",
    "href": "thesis2.html#the-mainanalysis.qmd-your-projects-master-narrative",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "The mainAnalysis.qmd file is not just a script; it’s a “literate programming” document. Think of it as a lab notebook that tells the complete story of your research, from loading the raw data to generating the final figures. It weaves your narrative (the “why”) with your code (the “how”) and its output.\nThis approach is incredibly powerful because it forces you to document your decisions as you make them, creating a transparent and readable record of your entire analytical process."
  },
  {
    "objectID": "thesis2.html#step-1-the-setup-chunk",
    "href": "thesis2.html#step-1-the-setup-chunk",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Every mainAnalysis.qmd file should begin with a setup chunk. This is where you prepare your R environment. This chunk is essential for reproducibility but is usually hidden from the final output.\n```{{r setup, include=false}} # 1. Load all necessary packages for the analysis library(tidyverse) # For data manipulation (dplyr, ggplot2) library(broom) # For tidying model outputs library(knitr) # For creating tables"
  },
  {
    "objectID": "thesis2.html#the-analysis-flow-a-narrative-driven-by-scripts",
    "href": "thesis2.html#the-analysis-flow-a-narrative-driven-by-scripts",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Our workflow is designed for clarity and modularity. The mainAnalysis.qmd file acts as the conductor, calling on specialized R scripts to perform specific tasks.\n\nLoad Evidence: Read the raw data from the 1_evidence directory.\nSource Explanatory Analysis: Call the 00_explanatory.R script to generate and load initial data objects.\nSource Research Models: Call the .R scripts from the models/ directory to run the core analyses and create model objects.\nSave Results: Use the objects created by the scripts to generate and save the final tables and figures into the results/ directory.\n\nLet’s walk through how this is implemented in our mainAnalysis.qmd file.\n\n\nWe begin with a setup chunk to load packages and a data chunk to load our raw data.\n```{{r setup, include=false}}\n# Load all necessary packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(knitr)\nlibrary(rio)\n# Set global options\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\n```\n### Data Loading\n```{{r loadData}}\n# The path goes up one directory (‘..’) then into the evidence folder.\ninventoryData &lt;- import(“../1_evidence/rawData/manualInventory.csv”)\n```\n\n\n\nWe execute our exploratory script. Its job is to perform initial calculations and create exploratory objects, which are then available for us to use in the main document.\n### Explanatory Analysis To begin, we explore the basic characteristics of our data by running the script in `explns/00_explanatory.R`.\n```{{r runExplanatoryAnalysis}}\n# This script creates objects like ‘summary_stats’\nsource(“explns/00_explanatory.R”)\n```\nNow we can display the `summary_stats` object that was just created.\n```{{r showExplanatoryStats, echo=true}}\n# Use kable for a clean summary table right here in our analysis\ndocument kable(summary_stats, caption = “Initial Data Summary”)\n```\n\n\n\nNext, we execute our core statistical models. The key is that the R scripts in the models/ folder should only create the model objects; they should not save any output themselves.\nExample: models/01_linearRegression.R\n# models/01_linearRegression.R\n# This script’s ONLY job is to run the model and produce the ‘modelFit’ object.\n# It assumes ‘inventoryData’ is already loaded in the environment.\nmodelFit &lt;- lm(stockLevel ~ storageTemp + category, data = inventoryData)\nWe call this from mainAnalysis.qmd to create the modelFit object in our environment.\n### Running the Main Regression Model\nNow, we run the linear model specified in our external script. This will create a model object called `modelFit` in our R environment, which we can then use to generate results.\n```{{r runLinearModel}}\nsource(“models/01_linearRegression.R”)\n```\n\n\n\nWith the modelFit object created, we can now generate our final outputs from within mainAnalysis.qmd. The key here is that we don’t just display them—we save them as files to the results/ directory. This decouples our analysis from our final presentation, allowing us to simply \\input or \\includegraphics in our LaTeX documents later.\n\n\nWe’ll use the broom package to clean our model output and knitr::kable() with the latex format to create a publication-ready table.\n```{{r saveTable}}\n# Tidy the model output into a clean data frame\nmodelSummary &lt;- tidy(modelFit)\n\n# Use kable to format as a LaTeX table and save_kable to write it to a file\n# The booktabs = TRUE option creates a more professional-looking table\nkable(modelSummary, format = “latex”, booktabs = TRUE,\ncaption = “Regression of Stock Levels on Storage Temperature and Category.”,\ncol.names = c(“Term”, “Estimate”, “Std. Error”, “Statistic”, “P-value”), digits = 3) %&gt;%\nsave_kable(file = “results/tables/regressionTable.tex”)\n```\nThis code creates a file named regressionTable.tex in the results/tables/ folder. It’s a self-contained piece of LaTeX code, ready to be included in our paper.\n\n\n\nSimilarly, we use ggplot2 to create our plot and the ggsave() function to save it as a high-quality image file.\n```{{r saveFigure}}\n# Create the plot object\nstockPlot &lt;- ggplot(inventoryData, aes(x = category, y = stockLevel, fill = category)) +\ngeom_boxplot() +\nlabs(title = “Stock Levels by Product Category”, x = “Category”, y = “Stock Level”) +\ntheme_minimal()\n# Save the plot to the figures directory\n# Saving as .pdf is great for LaTeX; .png is better for websites.\nggsave( “results/figures/stockByCategory.pdf”, plot = stockPlot, width = 7, height = 5 )\n```\nThis creates a file named stockByCategory.pdf in our results/figures/ folder.\nBy the end of this process, running the single mainAnalysis.qmd file has verifiably transformed your raw evidence into a complete set of polished, presentation-ready results, all neatly organized in their respective folders.\n\n\n\n\nIt is important to highlight that the tools shown above (knitr::kable and ggplot2::ggsave) are just one way to accomplish our goal. The R ecosystem is rich with packages designed for creating high-quality tables and figures. The key is not the specific package, but the workflow: use code to generate and save your results as files.\nHere are some excellent alternatives you might explore:\n\nFor Tables:\n\nxtable: One of the oldest and most powerful packages for creating LaTeX and HTML tables. It offers a huge amount of control over the final output.\ntexreg: Fantastic for creating regression tables from multiple models, side-by-side, in a format common in academic journals.\nmodelsummary: A modern and extremely flexible package that can output results to a huge variety of formats (LaTeX, HTML, Word, Markdown) and is very easy to use.\n\nFor Figures:\n\nBase R Graphics: R’s built-in plot() function is powerful and fast. To save a base R plot, you would wrap the code like this: pdf(\"myplot.pdf\"); plot(x,y); dev.off().\ngoogleVis: Creates interactive HTML-based charts and maps using the Google Charts API. These are excellent for web-based presentations (website.qmd) but cannot be embedded in static PDF documents.\n\n\nFeel free to experiment and find the packages that you prefer. As long as you can save their output to a file in your results directory, they will fit perfectly into this reproducible workflow.\nIn Part 4: The Claim - Presenting Your Research, we’ll see how to write the paper.tex and slides.tex files that effortlessly pull in these saved tables and figures to construct a polished, professional, and fully reproducible final document."
  },
  {
    "objectID": "thesis3.html",
    "href": "thesis3.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Welcome to the final post in our series on building a reproducible research workflow. So far, we have:\n\nEstablished a project structure based on the Toulmin model (Part 1).\nGathered our Evidence (raw data and references) into the 1_evidence directory (Part 2).\nBuilt our Warrant by scripting the analysis and saving the outputs to the 2_warrantAndAnalysis/results directory (Part 3).\n\nNow, we will construct the Claim—the polished, final documents where we present our findings. This happens in the 3_claimAndPresentation directory. The beauty of our workflow is that this final step is now incredibly simple and robust. Our presentation files don’t contain any analysis; they are lightweight documents that simply include the results we’ve already generated.\n\n\nBefore we build our final documents, it’s important to discuss the roles of the different authoring tools we’ve encountered.\n\nLaTeX (.tex): This is the gold standard for well-organized, professional, and beautifully typeset documents. Its power lies in its structure and control, making it ideal for a final, polished paper or a formal slide deck. We use it here for our paper.tex and slides.tex files.\nQuarto/R Markdown (.qmd/.Rmd): These tools are best for tentative, formatted analysis documents. Our mainAnalysis.qmd is a perfect example. It’s a working document where we weave code and narrative to tell the story of our analysis for ourselves and our close collaborators. It’s a dynamic notebook, not a final paper.\nR Sweave (.Rnw): This is the original tool for integrating R code directly into LaTeX documents. While powerful, Sweave compiles the R code and the LaTeX document at the same time. On my machine, this process can be slow, especially for large documents. For a short, two-to-three-page document, it can be efficient. However, for a full paper, I prefer our current workflow: separating the R analysis (.qmd) from the final LaTeX writing (.tex) for better speed and modularity.\n\n\n\n\nBefore we import our results, let’s understand how a LaTeX document is structured. Unlike a word processor, LaTeX is a markup language; you write plain text commands that tell the compiler how to format the document.\nA LaTeX document has two main parts:\n\nThe Preamble: This is everything from the very first line until the \\begin{document} command. Here, you define the document type (\\documentclass), load packages (\\usepackage), and set global document properties like the title and author.\nThe Body: This is everything between \\begin{document} and \\end{document}. It contains the actual content of your paper.\n\n\n\nHere are some of the most common commands you’ll use when writing your paper:\n\nHeadings: Use \\section{Section Name}, \\subsection{Subsection Name}, and \\subsubsection{Sub-subsection Name} to structure your paper. LaTeX handles the numbering and formatting automatically.\nParagraphs and Spacing: A blank line in your .tex file creates a new paragraph. LaTeX manages the indentation itself. To force a line break without a new paragraph, use \\\\.\nHorizontal Lines: To create a full-width horizontal line, for example to separate major sections, use the \\hrulefill command.\nText Formatting:\n\nItalics: \\emph{italicized text}\nBold: \\textbf{bold text}\n\nMath Equations: This is where LaTeX shines.\n\nInline math: \\( E = mc^2 \\) will place the equation within a sentence.\nDisplayed math: \\[ E = mc^2 \\] will give the equation its own centered line.\n\nLists:\n\nBulleted list: Use the itemize environment: \\begin{itemize} \\item First item. \\end{itemize}.\nNumbered list: Use the enumerate environment: \\begin{enumerate} \\item First item. \\end{enumerate}.\n\nFootnotes and Cross-References:\n\nCreate a footnote with \\footnote{This is the footnote text.}.\nTo reference a figure, table, or section, first label it with \\label{uniqueLabel}. Then, refer to it anywhere in the text with \\ref{uniqueLabel}. LaTeX will automatically insert the correct number.\n\n\n\n\n\nManaging citations is a core strength of LaTeX. The process involves three parts:\n\nThe .bib file: Your literature.bib file in the 1_evidence folder, which contains all your reference data.\nThe \\bibliography command: At the end of your document (before \\end{document}), you tell LaTeX which .bib file to use: \\bibliography{../1_evidence/literature.bib}.\nThe \\bibliographystyle command: Placed before the \\bibliography command, this sets the citation style (e.g., \\bibliographystyle{apa}).\n\nTo actually cite a source in your text, you use a cite command. When using an author-year style with the natbib package (which we included in our preamble with \\usepackage[authoryear]{natbib}), you have several powerful options:\n\n\\citep{key} (Parenthetical Citation): This is the most common command. It wraps the entire author-year reference in parentheses.\n\nExample: ...as shown in previous work \\citep{Gandrud2020}.\nOutput: …as shown in previous work (Gandrud, 2020).\n\n\\cite{key} or \\citet{key} (Textual Citation): This command is used when the author’s name is part of the sentence. It keeps the author name in the main text and only puts the year in parentheses. \\citet is the more robust command from natbib.\n\nExample: According to \\citet{Gandrud2020}, this workflow is...\nOutput: According to Gandrud (2020), this workflow is…\n\nCiting Multiple Sources: You can include multiple keys in a single command, separated by commas.\n\nExample: ...as shown in previous work \\citep{Gandrud2020, Donoho2010}.\nOutput: …as shown in previous work (Gandrud, 2020; Donoho, 2010).\n\nAdding Page Numbers: To add a page number or other note inside the citation, use square brackets.\n\nExample: ...this specific point is made in \\citet[p. 45]{Gandrud2020}.\nOutput: …this specific point is made in Gandrud (2020, p. 45).\n\n\n\n\n\n\n\n\nFor those who prefer to stay within the RStudio environment, compiling a .tex file into a PDF requires a LaTeX distribution. Traditionally, this meant installing large, separate programs like MiKTeX (Windows) or MacTeX (macOS). However, there is a much simpler, R-native solution: the tinytex package.\nWhat is tinytex? Created by Yihui Xie (the same author of knitr), tinytex is an R package that installs a lightweight, self-contained, and portable LaTeX distribution.\nWhy use it?\n\nSimplicity: You can install it directly from the R console: install.packages(\"tinytex\"); tinytex::install_tinytex().\nAutomatic Package Installation: This is its killer feature. If your LaTeX document requires a package (e.g., \\usepackage{booktabs}) that isn’t in the base tinytex installation, tinytex will automatically find and install it for you during compilation. This saves an enormous amount of time troubleshooting obscure LaTeX errors.\nSelf-Contained: It lives within your R library paths and doesn’t interfere with any other LaTeX installations you might have.\n\nWith tinytex installed, you can write your paper.tex file in RStudio and simply click the “Compile PDF” button to generate your final document. It’s a seamless, integrated experience.\n\n\n\nAs discussed in the previous post, we will use GitHub to store our project and Overleaf as our collaborative LaTeX editor. By linking your Overleaf project to your GitHub repository, you ensure your writing environment always has access to your latest results.\nWhile we can write LaTeX on our local machine, a more powerful modern approach is to use Overleaf, a collaborative, cloud-based LaTeX editor. It allows multiple co-authors to work on a document simultaneously and focuses purely on writing.\nBut how do we get our results from our local machine into our Overleaf project? The bridge is GitHub.\nBy keeping our entire project in a GitHub repository, we create a “single source of truth” in the cloud. Overleaf can then link directly to this repository, allowing us to pull in our results seamlessly.\n\n\n\nAfter you run your mainAnalysis.qmd file locally, the results/ directory is updated with the latest tables and figures. The next step is to commit and push these changes to your GitHub repository.\nUsing the command line (or a Git client), you would run:\nBash\n# Stage the new results\ngit add 2_warrantAndAnalysis/results/\n\n# Commit the changes with a descriptive message\ngit commit -m \"Update regression table and stock level figure\"\n\n# Push the changes to your main branch on GitHub\ngit push origin main\nYour latest results are now securely stored and versioned on GitHub.\n\n\n\nOverleaf has a fantastic feature to link a project directly to a GitHub repository.\n\nIn Overleaf, create a new project.\nIn the project menu, select “Import from GitHub”.\nAuthorize Overleaf to access your GitHub account and select your research project repository.\n\nOverleaf will now clone your entire project. You can work on your paper.tex file directly in Overleaf’s editor. To include your results, you simply need to provide the correct file path from the root of the project.\n\n\n\nOnce your Overleaf project is synced with GitHub, you can pull in the results.\n\n\nTo include a table, we use the \\input{} command. This command acts as if you are “pasting” the entire content of the specified file right into that spot. Since we saved our table as a complete, self-contained .tex file, this is incredibly clean.\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n% Create a table float environment for the caption and label\n\\begin{table}[h!]\n    \\centering\n    % Input the table we saved in Part 3.\n    % The path is relative to the root of our project folder.\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\nThe table environment provides a caption and a referenceable label. The \\input command does the heavy lifting of bringing in the table itself.\n\n\n\nTo include a figure, we use the \\includegraphics{} command inside a figure environment. The figure environment is crucial as it provides the caption and label, just like for a table.\nlatex\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n% Create a figure float environment\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure we saved in Part 3.\n    % We use the .pdf for high-quality printing.\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\nThe [width=0.8\\textwidth] option scales the image to be 80% of the page’s text width, ensuring it fits nicely.\nHere is a simplified paper.tex file as it would look inside your Overleaf project:\n\\documentclass{article}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage[authoryear]{natbib}\n\n\\title{Analysis of Inventory Stock Levels}\n\\author{A. Researcher}\n\\date{\\today}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\\maketitle\n\n\\section{Introduction}\nThis paper investigates the factors affecting inventory stock levels...\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure from the synced GitHub folder\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\n\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n\\begin{table}[h!]\n    \\centering\n    % Input the table from the synced GitHub folder\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\n\n% --- BIBLIOGRAPHY ---\n\\bibliographystyle{apa}\n% Point to the BibTeX file in the synced GitHub folder\n\\bibliography{1_evidence/literature.bib}\n\n\\end{document}\nWhen you need to update your results, the workflow is simple:\n\nRun mainAnalysis.qmd on your local machine.\nPush the updated results/ folder to GitHub.\nIn Overleaf, click the “Git” menu and “Pull” the latest changes from GitHub.\nRecompile your paper.tex.\n\nYour paper is now updated with the latest results, with zero manual copy-pasting.\n\n\n\n\n\nCreating a conference presentation with LaTeX Beamer follows the exact same logic. The document structure is different (using \\begin{frame} environments), but the principle of including results is identical.\n\\documentclass{beamer}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\n\\title{Inventory Stock Level Analysis}\n\\author{A. Researcher}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\n\\begin{frame}\n    \\titlepage\n\\end{frame}\n\n\\begin{frame}{Regression Results}\n    % Input the same table file\n    \\input{../2_warrantAndAnalysis/results/tables/regressionTable.tex}\n\\end{frame}\n\n\\end{document}\nYou can use the very same result files across multiple presentation formats without any extra work.\n\n\n\nFor a blog post or project website, we can use Quarto. The syntax is different, but the concept is the same: refer to the files in the results directory.\n---\ntitle: \"A Blog Post on My Inventory Research\" \\\nauthor: \"A. Researcher\" format: html \n---\n\n\\## Key Findings Our research explored the factors affecting inventory stock levels. The main regression results are summarized below. \\`\\`\\`{r, results='asis'} \\# Read and display the raw LaTeX from the results file \\# Quarto is smart enough to render it correctly to HTML knitr::asis_output(readLines(\"../2_warrantAndAnalysis/results/tables/regressionTable.tex\")) \\`\\`\\` We also found a key difference between product categories, as illustrated in this plot: \\![Stock Levels by Category\\](../2_warrantAndAnalysis/results/figures/stockByCategory.png) \\\nNote that for a website, it’s often better to save and use the .png version of the figure, as it’s more web-friendly.\n\n\n\nThis four-part series has taken us through a complete, reproducible research workflow. By separating our project into Evidence, Warrant, and Claim, and by using modern tools like R, Quarto, GitHub, and Overleaf, we have created a system that is not only transparent and reproducible but also highly efficient and collaborative.\nThis workflow requires discipline, but the payoff in clarity, accuracy, and peace of mind is immeasurable. Happy researching"
  },
  {
    "objectID": "thesis3.html#a-note-on-authoring-tools-choosing-the-right-tool-for-the-job",
    "href": "thesis3.html#a-note-on-authoring-tools-choosing-the-right-tool-for-the-job",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we build our final documents, it’s important to discuss the roles of the different authoring tools we’ve encountered.\n\nLaTeX (.tex): This is the gold standard for well-organized, professional, and beautifully typeset documents. Its power lies in its structure and control, making it ideal for a final, polished paper or a formal slide deck. We use it here for our paper.tex and slides.tex files.\nQuarto/R Markdown (.qmd/.Rmd): These tools are best for tentative, formatted analysis documents. Our mainAnalysis.qmd is a perfect example. It’s a working document where we weave code and narrative to tell the story of our analysis for ourselves and our close collaborators. It’s a dynamic notebook, not a final paper.\nR Sweave (.Rnw): This is the original tool for integrating R code directly into LaTeX documents. While powerful, Sweave compiles the R code and the LaTeX document at the same time. On my machine, this process can be slow, especially for large documents. For a short, two-to-three-page document, it can be efficient. However, for a full paper, I prefer our current workflow: separating the R analysis (.qmd) from the final LaTeX writing (.tex) for better speed and modularity."
  },
  {
    "objectID": "thesis3.html#the-modern-workflow-github-overleaf",
    "href": "thesis3.html#the-modern-workflow-github-overleaf",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "As discussed in the previous post, we will use GitHub to store our project and Overleaf as our collaborative LaTeX editor. By linking your Overleaf project to your GitHub repository, you ensure your writing environment always has access to your latest results.\nWhile we can write LaTeX on our local machine, a more powerful modern approach is to use Overleaf, a collaborative, cloud-based LaTeX editor. It allows multiple co-authors to work on a document simultaneously and focuses purely on writing.\nBut how do we get our results from our local machine into our Overleaf project? The bridge is GitHub.\nBy keeping our entire project in a GitHub repository, we create a “single source of truth” in the cloud. Overleaf can then link directly to this repository, allowing us to pull in our results seamlessly.\n\n\nAfter you run your mainAnalysis.qmd file locally, the results/ directory is updated with the latest tables and figures. The next step is to commit and push these changes to your GitHub repository.\nUsing the command line (or a Git client), you would run:\nBash\n# Stage the new results\ngit add 2_warrantAndAnalysis/results/\n\n# Commit the changes with a descriptive message\ngit commit -m \"Update regression table and stock level figure\"\n\n# Push the changes to your main branch on GitHub\ngit push origin main\nYour latest results are now securely stored and versioned on GitHub.\n\n\n\nOverleaf has a fantastic feature to link a project directly to a GitHub repository.\n\nIn Overleaf, create a new project.\nIn the project menu, select “Import from GitHub”.\nAuthorize Overleaf to access your GitHub account and select your research project repository.\n\nOverleaf will now clone your entire project. You can work on your paper.tex file directly in Overleaf’s editor. To include your results, you simply need to provide the correct file path from the root of the project.\n\n\n\nOnce your Overleaf project is synced with GitHub, you can pull in the results.\n\n\nTo include a table, we use the \\input{} command. This command acts as if you are “pasting” the entire content of the specified file right into that spot. Since we saved our table as a complete, self-contained .tex file, this is incredibly clean.\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n% Create a table float environment for the caption and label\n\\begin{table}[h!]\n    \\centering\n    % Input the table we saved in Part 3.\n    % The path is relative to the root of our project folder.\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\nThe table environment provides a caption and a referenceable label. The \\input command does the heavy lifting of bringing in the table itself.\n\n\n\nTo include a figure, we use the \\includegraphics{} command inside a figure environment. The figure environment is crucial as it provides the caption and label, just like for a table.\nlatex\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n% Create a figure float environment\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure we saved in Part 3.\n    % We use the .pdf for high-quality printing.\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\nThe [width=0.8\\textwidth] option scales the image to be 80% of the page’s text width, ensuring it fits nicely.\nHere is a simplified paper.tex file as it would look inside your Overleaf project:\n\\documentclass{article}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage[authoryear]{natbib}\n\n\\title{Analysis of Inventory Stock Levels}\n\\author{A. Researcher}\n\\date{\\today}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\\maketitle\n\n\\section{Introduction}\nThis paper investigates the factors affecting inventory stock levels...\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure from the synced GitHub folder\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\n\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n\\begin{table}[h!]\n    \\centering\n    % Input the table from the synced GitHub folder\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\n\n% --- BIBLIOGRAPHY ---\n\\bibliographystyle{apa}\n% Point to the BibTeX file in the synced GitHub folder\n\\bibliography{1_evidence/literature.bib}\n\n\\end{document}\nWhen you need to update your results, the workflow is simple:\n\nRun mainAnalysis.qmd on your local machine.\nPush the updated results/ folder to GitHub.\nIn Overleaf, click the “Git” menu and “Pull” the latest changes from GitHub.\nRecompile your paper.tex.\n\nYour paper is now updated with the latest results, with zero manual copy-pasting."
  },
  {
    "objectID": "thesis3.html#building-the-slides-slides.tex",
    "href": "thesis3.html#building-the-slides-slides.tex",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Creating a conference presentation with LaTeX Beamer follows the exact same logic. The document structure is different (using \\begin{frame} environments), but the principle of including results is identical.\n\\documentclass{beamer}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\n\\title{Inventory Stock Level Analysis}\n\\author{A. Researcher}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\n\\begin{frame}\n    \\titlepage\n\\end{frame}\n\n\\begin{frame}{Regression Results}\n    % Input the same table file\n    \\input{../2_warrantAndAnalysis/results/tables/regressionTable.tex}\n\\end{frame}\n\n\\end{document}\nYou can use the very same result files across multiple presentation formats without any extra work."
  },
  {
    "objectID": "thesis3.html#building-the-website-website.qmd",
    "href": "thesis3.html#building-the-website-website.qmd",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "For a blog post or project website, we can use Quarto. The syntax is different, but the concept is the same: refer to the files in the results directory.\n---\ntitle: \"A Blog Post on My Inventory Research\" \\\nauthor: \"A. Researcher\" format: html \n---\n\n\\## Key Findings Our research explored the factors affecting inventory stock levels. The main regression results are summarized below. \\`\\`\\`{r, results='asis'} \\# Read and display the raw LaTeX from the results file \\# Quarto is smart enough to render it correctly to HTML knitr::asis_output(readLines(\"../2_warrantAndAnalysis/results/tables/regressionTable.tex\")) \\`\\`\\` We also found a key difference between product categories, as illustrated in this plot: \\![Stock Levels by Category\\](../2_warrantAndAnalysis/results/figures/stockByCategory.png) \\\nNote that for a website, it’s often better to save and use the .png version of the figure, as it’s more web-friendly."
  },
  {
    "objectID": "thesis3.html#conclusion-a-coherent-whole",
    "href": "thesis3.html#conclusion-a-coherent-whole",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "This four-part series has taken us through a complete, reproducible research workflow. By separating our project into Evidence, Warrant, and Claim, and by using modern tools like R, Quarto, GitHub, and Overleaf, we have created a system that is not only transparent and reproducible but also highly efficient and collaborative.\nThis workflow requires discipline, but the payoff in clarity, accuracy, and peace of mind is immeasurable. Happy researching"
  },
  {
    "objectID": "thesis3.html#latex-basics-the-structure-of-a-document",
    "href": "thesis3.html#latex-basics-the-structure-of-a-document",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we import our results, let’s understand how a LaTeX document is structured. Unlike a word processor, LaTeX is a markup language; you write plain text commands that tell the compiler how to format the document.\nA LaTeX document has two main parts:\n\nThe Preamble: This is everything from the very first line until the \\begin{document} command. Here, you define the document type (\\documentclass), load packages (\\usepackage), and set global document properties like the title and author.\nThe Body: This is everything between \\begin{document} and \\end{document}. It contains the actual content of your paper.\n\n\n\nHere are some of the most common commands you’ll use when writing your paper:\n\nHeadings: Use \\section{Section Name}, \\subsection{Subsection Name}, and \\subsubsection{Sub-subsection Name} to structure your paper. LaTeX handles the numbering and formatting automatically.\nParagraphs and Spacing: A blank line in your .tex file creates a new paragraph. LaTeX manages the indentation itself. To force a line break without a new paragraph, use \\\\.\nHorizontal Lines: To create a full-width horizontal line, for example to separate major sections, use the \\hrulefill command.\nText Formatting:\n\nItalics: \\emph{italicized text}\nBold: \\textbf{bold text}\n\nMath Equations: This is where LaTeX shines.\n\nInline math: \\( E = mc^2 \\) will place the equation within a sentence.\nDisplayed math: \\[ E = mc^2 \\] will give the equation its own centered line.\n\nLists:\n\nBulleted list: Use the itemize environment: \\begin{itemize} \\item First item. \\end{itemize}.\nNumbered list: Use the enumerate environment: \\begin{enumerate} \\item First item. \\end{enumerate}.\n\nFootnotes and Cross-References:\n\nCreate a footnote with \\footnote{This is the footnote text.}.\nTo reference a figure, table, or section, first label it with \\label{uniqueLabel}. Then, refer to it anywhere in the text with \\ref{uniqueLabel}. LaTeX will automatically insert the correct number.\n\n\n\n\n\nManaging citations is a core strength of LaTeX. The process involves three parts:\n\nThe .bib file: Your literature.bib file in the 1_evidence folder, which contains all your reference data.\nThe \\bibliography command: At the end of your document (before \\end{document}), you tell LaTeX which .bib file to use: \\bibliography{../1_evidence/literature.bib}.\nThe \\bibliographystyle command: Placed before the \\bibliography command, this sets the citation style (e.g., \\bibliographystyle{apa}).\n\nTo actually cite a source in your text, you use a cite command. When using an author-year style with the natbib package (which we included in our preamble with \\usepackage[authoryear]{natbib}), you have several powerful options:\n\n\\citep{key} (Parenthetical Citation): This is the most common command. It wraps the entire author-year reference in parentheses.\n\nExample: ...as shown in previous work \\citep{Gandrud2020}.\nOutput: …as shown in previous work (Gandrud, 2020).\n\n\\cite{key} or \\citet{key} (Textual Citation): This command is used when the author’s name is part of the sentence. It keeps the author name in the main text and only puts the year in parentheses. \\citet is the more robust command from natbib.\n\nExample: According to \\citet{Gandrud2020}, this workflow is...\nOutput: According to Gandrud (2020), this workflow is…\n\nCiting Multiple Sources: You can include multiple keys in a single command, separated by commas.\n\nExample: ...as shown in previous work \\citep{Gandrud2020, Donoho2010}.\nOutput: …as shown in previous work (Gandrud, 2020; Donoho, 2010).\n\nAdding Page Numbers: To add a page number or other note inside the citation, use square brackets.\n\nExample: ...this specific point is made in \\citet[p. 45]{Gandrud2020}.\nOutput: …this specific point is made in Gandrud (2020, p. 45)."
  },
  {
    "objectID": "thesis3.html#compiling-your-document-two-powerful-approaches",
    "href": "thesis3.html#compiling-your-document-two-powerful-approaches",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "For those who prefer to stay within the RStudio environment, compiling a .tex file into a PDF requires a LaTeX distribution. Traditionally, this meant installing large, separate programs like MiKTeX (Windows) or MacTeX (macOS). However, there is a much simpler, R-native solution: the tinytex package.\nWhat is tinytex? Created by Yihui Xie (the same author of knitr), tinytex is an R package that installs a lightweight, self-contained, and portable LaTeX distribution.\nWhy use it?\n\nSimplicity: You can install it directly from the R console: install.packages(\"tinytex\"); tinytex::install_tinytex().\nAutomatic Package Installation: This is its killer feature. If your LaTeX document requires a package (e.g., \\usepackage{booktabs}) that isn’t in the base tinytex installation, tinytex will automatically find and install it for you during compilation. This saves an enormous amount of time troubleshooting obscure LaTeX errors.\nSelf-Contained: It lives within your R library paths and doesn’t interfere with any other LaTeX installations you might have.\n\nWith tinytex installed, you can write your paper.tex file in RStudio and simply click the “Compile PDF” button to generate your final document. It’s a seamless, integrated experience.\n\n\n\nAs discussed in the previous post, we will use GitHub to store our project and Overleaf as our collaborative LaTeX editor. By linking your Overleaf project to your GitHub repository, you ensure your writing environment always has access to your latest results.\nWhile we can write LaTeX on our local machine, a more powerful modern approach is to use Overleaf, a collaborative, cloud-based LaTeX editor. It allows multiple co-authors to work on a document simultaneously and focuses purely on writing.\nBut how do we get our results from our local machine into our Overleaf project? The bridge is GitHub.\nBy keeping our entire project in a GitHub repository, we create a “single source of truth” in the cloud. Overleaf can then link directly to this repository, allowing us to pull in our results seamlessly.\n\n\n\nAfter you run your mainAnalysis.qmd file locally, the results/ directory is updated with the latest tables and figures. The next step is to commit and push these changes to your GitHub repository.\nUsing the command line (or a Git client), you would run:\nBash\n# Stage the new results\ngit add 2_warrantAndAnalysis/results/\n\n# Commit the changes with a descriptive message\ngit commit -m \"Update regression table and stock level figure\"\n\n# Push the changes to your main branch on GitHub\ngit push origin main\nYour latest results are now securely stored and versioned on GitHub.\n\n\n\nOverleaf has a fantastic feature to link a project directly to a GitHub repository.\n\nIn Overleaf, create a new project.\nIn the project menu, select “Import from GitHub”.\nAuthorize Overleaf to access your GitHub account and select your research project repository.\n\nOverleaf will now clone your entire project. You can work on your paper.tex file directly in Overleaf’s editor. To include your results, you simply need to provide the correct file path from the root of the project.\n\n\n\nOnce your Overleaf project is synced with GitHub, you can pull in the results.\n\n\nTo include a table, we use the \\input{} command. This command acts as if you are “pasting” the entire content of the specified file right into that spot. Since we saved our table as a complete, self-contained .tex file, this is incredibly clean.\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n% Create a table float environment for the caption and label\n\\begin{table}[h!]\n    \\centering\n    % Input the table we saved in Part 3.\n    % The path is relative to the root of our project folder.\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\nThe table environment provides a caption and a referenceable label. The \\input command does the heavy lifting of bringing in the table itself.\n\n\n\nTo include a figure, we use the \\includegraphics{} command inside a figure environment. The figure environment is crucial as it provides the caption and label, just like for a table.\nlatex\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n% Create a figure float environment\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure we saved in Part 3.\n    % We use the .pdf for high-quality printing.\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\nThe [width=0.8\\textwidth] option scales the image to be 80% of the page’s text width, ensuring it fits nicely.\nHere is a simplified paper.tex file as it would look inside your Overleaf project:\n\\documentclass{article}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage[authoryear]{natbib}\n\n\\title{Analysis of Inventory Stock Levels}\n\\author{A. Researcher}\n\\date{\\today}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\\maketitle\n\n\\section{Introduction}\nThis paper investigates the factors affecting inventory stock levels...\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure from the synced GitHub folder\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\n\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n\\begin{table}[h!]\n    \\centering\n    % Input the table from the synced GitHub folder\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\n\n% --- BIBLIOGRAPHY ---\n\\bibliographystyle{apa}\n% Point to the BibTeX file in the synced GitHub folder\n\\bibliography{1_evidence/literature.bib}\n\n\\end{document}\nWhen you need to update your results, the workflow is simple:\n\nRun mainAnalysis.qmd on your local machine.\nPush the updated results/ folder to GitHub.\nIn Overleaf, click the “Git” menu and “Pull” the latest changes from GitHub.\nRecompile your paper.tex.\n\nYour paper is now updated with the latest results, with zero manual copy-pasting."
  },
  {
    "objectID": "Black holes 250824.html",
    "href": "Black holes 250824.html",
    "title": "A New Cosmic Theory: Black Holes and Dark Energy",
    "section": "",
    "text": "Black Holes: The Surprising Source of Dark Energy?\nI recently stumbled upon something that completely blew me away: a new theory suggesting that black holes might be the source of dark energy. For years, scientists have been grappling with the mystery of dark energy—the force that’s causing the universe’s expansion to accelerate. Now, a groundbreaking new idea proposes an elegant solution to this cosmic puzzle.\n\n\n\nPrimary Citation\nThe research behind this theory is detailed in a paper titled “Positive Neutrino Masses with DESI DR2 via Matter Conversion to Dark Energy,” which was published in Physical Review Letters.\nThe full citation is: S. P. Ahlen et al., “Positive Neutrino Masses with DESI DR2 via Matter Conversion to Dark Energy,” PHYSICAL REVIEW LETTERS 135, 081003 (2025).\n\n\n\nA Tale of Two Theories\nThe old theory, known as the Lambda CDM model, is what we’ve been using to understand the universe for decades. It’s built on the assumption that dark energy is a constant, unchanging force. But this model has run into some problems. Recent data from projects like the Dark Energy Spectroscopic Instrument (DESI) have shown inconsistencies, particularly a puzzling discrepancy in the measurement of the universe’s expansion rate, the Hubble constant.\nThis is where the new Cosmological Coupled Black Hole Theory comes in. It proposes a dynamic relationship where black holes are “cosmologically coupled” to the universe’s expansion. Essentially, as the universe expands, black holes grow in mass, and this growth converts some of the matter they consume into dark energy. It’s a cosmic feedback loop: black holes fuel the very expansion that helps them grow.\n\n\n\nWhat This Theory Can Tell Us\nThe implications of this new theory are profound. A study that tested both the standard and the new black hole models against DESI data found something remarkable. The old model produced a physically impossible result (a negative mass for neutrinos), while the new black hole theory produced a positive mass—a result that aligns with our current understanding of physics. This single finding gives this new theory incredible weight.\nIt suggests that our universe might be part of a grand, repeating cosmic cycle.\n\n\n\nThe Conclusion: A New Beginning\nThis revolutionary theory offers a fresh perspective on the universe’s evolution. Imagine a universe that starts with a Big Bang, expands with the help of black holes, and eventually becomes so vast and empty that a new Big Bang is triggered. It’s a cyclical, self-sustaining process that brings a beautiful new coherence to the mysteries of dark energy and the universe’s ultimate fate. It shows how even the most destructive objects in the universe might be the very creators of its future."
  }
]