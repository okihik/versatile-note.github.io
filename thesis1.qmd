---
title: "Reproducible Research Reporting"
subtitle: "Part 2: Data Gathering and Storage"
date: "2025-08-19"
categories: [Latex]
---

# **Part 2: Data Gathering and Storage: The Evidence Base**

Welcome back to our series on building a reproducible research workflow.\
In [Part 1](), we established a philosophical and structural foundation for our project using the Toulmin model.\
We set up a directory structure to house our **Evidence**, **Warrant**, and **Claim**.

Now, we'll focus on the first and most critical step of the research process: the **Evidence**.\
Specifically, how do we gather and store our data and references in a way that is robust, transparent, and reproducible?\
This post will cover the "how-to" for populating our `1_evidence/` directory.

## **The Sanctity of Raw Data**

The `1_evidence/rawData/` directory is the most important folder in your entire project.\
It should be treated as a write-once, read-many-times archive.

**The Golden Rule:** Never, ever, *ever* manually edit your raw data files. Do not open `rawDataSource1.csv` in Excel to "quickly fix" a typo or delete a column.

Why is this so critical?

-   **Traceability:** Any change to the data must be documented and scripted. If you manually edit the raw file, you lose the record of that change forever.

-   **Integrity:** It guarantees that you can always return to the original state of your data if you make a mistake in your data processing scripts.

-   **Reproducibility:** Anyone seeking to reproduce your work needs access to the exact same starting point you had.

All data cleaning, filtering, and transformation should happen programmatically in the scripts we will build later in the `2_warrantAndAnalysis/` directory.

## **Gathering Data: Scripting the Collection**

The most reproducible way to get data is to script its collection. This creates a perfect, runnable record of where your evidence came from.

### **Method 1: Creating Data "By Hand" in RStudio**

Sometimes, your data might come from a non-digital source, like a historical text or your own experimental observations. Instead of putting it in Excel first, you can enter it directly into R using a `data.frame()`. This is the most transparent way to digitize data.

```{{r createDataManually}}
# Creating a small dataset of hypothetical inventory data
inventoryData <- data.frame(
  productId = c("A1", "A2", "B1", "B2"),
  category = c("Fruit", "Fruit", "Dairy", "Dairy"),
  stockLevel = c(150, 200, 80, 120),
  storageTemp = c(4, 4, 2, 2)
)

# Save this manually created raw data for the record
# Note: This is one of the few times we write to the evidence folder,
# and only to create a reproducible starting point.
write.csv(inventoryData, "../1_evidence/rawData/manualInventory.csv", row.names = FALSE)
```

### **Method 2: Sourcing from the Web (HTTP)**

If data is hosted on a website in a plain-text format (like `.csv`), you can download it directly. This avoids manual "download and drop" and ensures you're getting the data from the original source every time.

```{{r downloadFromWeb}}
# URL of a raw CSV file on the web
url <- "http://www.some-data-repository.com/food-data.csv"

# Use rio::import( ) to read the data directly into R
# The rio package is great at handling various file types from a URL
library(rio)
webData <- import(url, format = "csv")
```

### **Method 3: Sourcing from GitHub**

GitHub is a common place to find data.\
To download a file, it is similar to Method 2, make sure you use the "Raw" file URL.

```{{r downloadFromGitHub}}
# URL for the "Raw" version of a CSV file on GitHub
githubUrl <- "https://raw.githubusercontent.com/someuser/somerepo/main/data.csv"

githubData <- import(githubUrl, format = "csv" )
```

### **Method 4: Sourcing from a SQL Server**

For larger, more structured projects, data is often stored in a relational database (like PostgreSQL, MySQL, or SQL Server). R can connect directly to these databases and run SQL queries. This is an extremely powerful and reproducible method.

To do this, you'll need two key packages:

1.  `DBI`: The standard database interface for R.

2.  A specific package for your database type (e.g., `RPostgres` for PostgreSQL, `RMariaDB` for MySQL/MariaDB).

The process involves three steps: connect, query, and disconnect.

```{{r downloadFromSQL, eval=false}}
# NOTE: This chunk is not evaluated (`eval=false`) because it requires a live database.
# 1. Load the necessary libraries
library(DBI)
library(RPostgres) # Or the package for your specific database

# 2. Establish the connection
# NEVER hard-code credentials in a script. Use environment variables or a secure config file.
con <- dbConnect(RPostgres::Postgres(),
                 dbname = "productionDB",
                 host = "db.your-university.edu",
                 port = 5432,
                 user = Sys.getenv("DB_USER"),
                 password = Sys.getenv("DB_PASSWORD"))

# 3. Write and execute your SQL query
sqlQuery <- "SELECT productId, saleDate, unitsSold FROM sales WHERE saleDate >= '2022-01-01';"
salesDataFromDB <- dbGetQuery(con, sqlQuery)

# 4. Always close the connection
dbDisconnect(con)
```

This method is highly reproducible because the SQL query is a precise, text-based record of the exact data you requested.

## **Data Manipulation: Forging the Analytical Dataset**

Raw data is rarely ready for analysis.\
The following steps are fundamental for preparing your data.\
We'll use the `dplyr` package, a core part of the `tidyverse`, for its powerful and readable syntax.

```{{r dataManipulation}}
# Load the tidyverse suite of packages
library(tidyverse)

# Let's assume we've loaded our 'inventoryData' and 'webData'
# For this example, let's create a second data frame to merge
salesData <- data.frame(
  productId = c("A1", "B1", "A2", "B2"),
  unitsSold = c(50, 20, 75, 40)
)
```

### **Binding and Merging DataFrames**

-   **Binding:** If you have two data frames with the exact same columns and you just want to stack them, use `bind_rows()`.

-   **Merging:** More commonly, you'll want to join two datasets based on a common key (like `productId`). This is a **merge** or **join**.

```{{r mergingData}}
# Merge inventoryData with salesData by the 'productId' column
fullData <- left_join(inventoryData, salesData, by = "productId")
```

### **Ordering and Subsetting**

-   **Ordering:** Ensure your data is in a logical order, for example, by category and then product ID.

-   **Subsetting:** Select only the rows and columns you need for a specific analysis.

```{{r orderingSubsetting}}
# Order the data
fullData <- fullData %>%
  arrange(category, productId)

# Subset to keep only the 'Fruit' category and specific columns
fruitData <- fullData %>%
  filter(category == "Fruit") %>%
  select(productId, stockLevel, unitsSold)
```

### **Recoding Variables and Creating New Ones**

You often need to modify existing variables or create new ones.

```{{r recodingCreating}}
# Recoding a string variable: Let's make category names uppercase
fullData <- fullData %>%
  mutate(category = toupper(category))

# Creating a new variable: Calculate the sell-through rate
fullData <- fullData %>%
  mutate(sellThroughRate = unitsSold / (stockLevel + unitsSold))
```

### **Dealing with Duplicate Columns**

When you merge datasets, you might end up with duplicate columns (e.g., `country.x`, `country.y`).\
It's crucial to resolve this.\
Decide which column to keep and rename it, dropping the others.

```{{r handleDuplicates}}
# Let's pretend we had a 'country.x' and 'country.y'
# We decide 'country.x' is the correct one and rename it

# hypotheticalData <- hypotheticalData %>%
#   rename(country = country.x) %>%
#   select(-country.y) # The minus sign drops the column
```

## **Storing Your Literature: The Power of BibTeX**

Just as your raw data is empirical evidence, your references are scholarly evidence. The best way to manage them is with a BibTeX file (`.bib`).

A BibTeX file is a plain-text database of all your citations. Each entry has a unique key and contains all the necessary metadata for a reference.

**Why use BibTeX?**

-   **Single Source of Truth:** All your references are in one place. No more copying and pasting citations between documents.

-   **Consistency:** It ensures every citation in your paper and every entry in your bibliography is formatted perfectly and consistently according to your chosen style.

-   **Portability:** It's a plain text file, making it easy to share, version control, and use across different projects and documents.

### **BibTeX Examples**

Let's look at a few common entry types you might have in your `1_evidence/literature.bib` file.

**1. A Book:** The `@book` type is for a complete book. Note that the `pages` field is generally not used for a whole book, but for a specific chapter (see next example).

```         
@book{Gandrud2020,
  author    = {Christopher Gandrud},
  title     = {Reproducible Research with R and RStudio},
  edition   = {Third},
  year      = {2020},
  publisher = {CRC Press}
}
```

**2. A Journal Article:** The `@article` type is for a peer-reviewed journal article. It requires `journal` and `volume` fields.

```         
@article{Donoho2010,
  author  = {David L. Donoho},
  title   = {An invitation to reproducible computational research},
  journal = {Biostatistics},
  year    = {2010},
  volume  = {11},
  number  = {3},
  pages   = {385--388}
}
```

**3. A Chapter in an Edited Book:** The `@incollection` type is perfect for citing a single chapter from a book with multiple authors and an editor. This is where the `pages` field is essential.

```         
@incollection{Buckheit1995,
  author    = {Jonathan B. Buckheit and David L. Donoho},
  title     = {Wavelab and Reproducible Research},
  booktitle = {Wavelets and Statistics},
  editor    = {Anestis Antoniadis},
  year      = {1995},
  publisher = {Springer},
  address   = {New York},
  pages     = {55--81}
}
```

The unique key (e.g., `Gandrud2020`, `Donoho2010`) is what you'll use to cite the work in your documents.

By the end of this stage, your `1_evidence/` directory should be populated with your raw, untouched data files and a comprehensive `literature.bib` file. You now have a solid, verifiable foundation of evidence.

After these steps, you have a clean, well-documented, and fully-reproducible analytical dataset, ready for the statistical modeling we will cover in the next part.

In our next post, we will move on to **Part 3: The Warrant - Processing Data and Weaving the Analysis**, where we'll start working in the `2_warrantAndAnalysis/` directory. We'll write our first R scripts to read in the raw data, clean it, and prepare it for the main analysis.
