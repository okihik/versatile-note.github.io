[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Himagineer",
    "section": "",
    "text": "Generational Accounting and OLG Models: An Introduction\n\n\nPart 1 of a series on converting ‘Matlabによるマクロ経済モデル入門’ to R\n\n\n\nR\n\nEconomics\n\nOLG\n\nGenerational Accounting\n\n\n\n\n\n\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Inventory Calculation: A Dynamic Approach\n\n\n\n\n\n\nInventory Management\n\n\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Make-to-Stock Production: Producing Only What Sells\n\n\n\n\n\n\nInventory Management\n\n\n\n\n\n\n\n\n\nAug 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Make-to-Order Production: Mastering the Flow of Time\n\n\n\n\n\n\nInventory Management\n\n\n\n\n\n\n\n\n\nAug 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Production Management: Shifting from Centralized Control to Demand-Driven Autonomy\n\n\n\n\n\n\nInventory Management\n\n\n\n\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Inventory Theories: An Introduction\n\n\n\n\n\n\nInventory Management\n\n\n\n\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Inventory Theories: Unpacking Inventory Management Challenges\n\n\n\n\n\n\nInventory Management\n\n\n\n\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research Reporting\n\n\nPart 4: The Claim - Presenting Your Research\n\n\n\nLatex\n\n\n\n\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA New Cosmic Theory: Black Holes and Dark Energy\n\n\n\n\n\n\nPhysics\n\n\n\n\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research Reporting\n\n\nPart 3: The Warrant - Weaving the Analysis with Quarto\n\n\n\nLatex\n\n\n\n\n\n\n\n\n\nAug 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research Reporting\n\n\nPart 2: Data Gathering and Storage\n\n\n\nLatex\n\n\n\n\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research Reporting\n\n\nPart 1: Concept and File Management\n\n\n\nLatex\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrafting Academic Reports: A LaTeX and Overleaf Guide\n\n\npart 2\n\n\n\nReporting\n\n\n\n\n\n\n\n\n\nJul 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrafting Academic Reports: A LaTeX and Overleaf Guide\n\n\npart 1\n\n\n\nReporting\n\n\n\n\n\n\n\n\n\nJul 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Quarto Blog\n\n\n\n\n\n\nQuarto Blog\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Black holes 250824.html",
    "href": "posts/Black holes 250824.html",
    "title": "A New Cosmic Theory: Black Holes and Dark Energy",
    "section": "",
    "text": "Black Holes: The Surprising Source of Dark Energy?\nI recently stumbled upon something that completely blew me away: a new theory suggesting that black holes might be the source of dark energy. For years, scientists have been grappling with the mystery of dark energy—the force that’s causing the universe’s expansion to accelerate. Now, a groundbreaking new idea proposes an elegant solution to this cosmic puzzle.\n\n\n\nPrimary Citation\nThe research behind this theory is detailed in a paper titled “Positive Neutrino Masses with DESI DR2 via Matter Conversion to Dark Energy,” which was published in Physical Review Letters.\nThe full citation is: S. P. Ahlen et al., “Positive Neutrino Masses with DESI DR2 via Matter Conversion to Dark Energy,” PHYSICAL REVIEW LETTERS 135, 081003 (2025).\n\n\n\nA Tale of Two Theories\nThe old theory, known as the Lambda CDM model, is what we’ve been using to understand the universe for decades. It’s built on the assumption that dark energy is a constant, unchanging force. But this model has run into some problems. Recent data from projects like the Dark Energy Spectroscopic Instrument (DESI) have shown inconsistencies, particularly a puzzling discrepancy in the measurement of the universe’s expansion rate, the Hubble constant.\nThis is where the new Cosmological Coupled Black Hole Theory comes in. It proposes a dynamic relationship where black holes are “cosmologically coupled” to the universe’s expansion. Essentially, as the universe expands, black holes grow in mass, and this growth converts some of the matter they consume into dark energy. It’s a cosmic feedback loop: black holes fuel the very expansion that helps them grow.\n\n\n\nWhat This Theory Can Tell Us\nThe implications of this new theory are profound. A study that tested both the standard and the new black hole models against DESI data found something remarkable. The old model produced a physically impossible result (a negative mass for neutrinos), while the new black hole theory produced a positive mass—a result that aligns with our current understanding of physics. This single finding gives this new theory incredible weight.\nIt suggests that our universe might be part of a grand, repeating cosmic cycle.\n\n\n\nThe Conclusion: A New Beginning\nThis revolutionary theory offers a fresh perspective on the universe’s evolution. Imagine a universe that starts with a Big Bang, expands with the help of black holes, and eventually becomes so vast and empty that a new Big Bang is triggered. It’s a cyclical, self-sustaining process that brings a beautiful new coherence to the mysteries of dark energy and the universe’s ultimate fate. It shows how even the most destructive objects in the universe might be the very creators of its future."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis2.html",
    "href": "posts/UNBC thesis in latex/thesis2.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Welcome back. In Part 1, we established our project’s structure, and in Part 2, we scripted the gathering of our raw evidence.\nNow, we will construct the Warrant—the intellectual and computational engine of our research. This is where we transform our raw evidence into analytical results. Our entire process will be orchestrated from a single, central document: mainAnalysis.qmd, located in the 2_warrantAndAnalysis folder. This document will tell the complete story of our analysis, from initial exploration to final model output, making it the perfect summary to share with supervisors and collaborators.\n\n\nOur workflow is designed for clarity and modularity. The mainAnalysis.qmd file acts as the conductor, calling on specialized R scripts to perform specific tasks.\n\nLoad Evidence: Read the raw data from the 1_evidence directory.\nSource Explanatory Analysis: Call the 00_explanatory.R script to generate and load initial data objects.\nSource Research Models: Call the .R scripts from the models/ directory to run the core analyses and create model objects.\nSave Results: Use the objects created by the scripts to generate and save the final tables and figures into the results/ directory.\n\nLet’s walk through how this is implemented in our mainAnalysis.qmd file.\n\n\nWe begin with a setup chunk to load packages and a data chunk to load our raw data.\n```{{r setup, include=false}}\n# Load all necessary packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(knitr)\nlibrary(rio)\n# Set global options\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\n```\n### Data Loading\n```{{r loadData}}\n# The path goes up one directory (‘..’) then into the evidence folder.\ninventoryData &lt;- import(“../1_evidence/rawData/manualInventory.csv”)\n```\n\n\n\nWe execute our exploratory script. Its job is to perform initial calculations and create exploratory objects, which are then available for us to use in the main document.\n### Explanatory Analysis To begin, we explore the basic characteristics of our data by running the script in `explns/00_explanatory.R`.\n```{{r runExplanatoryAnalysis}}\n# This script creates objects like ‘summary_stats’\nsource(“explns/00_explanatory.R”)\n```\nNow we can display the `summary_stats` object that was just created.\n```{{r showExplanatoryStats, echo=true}}\n# Use kable for a clean summary table right here in our analysis\ndocument kable(summary_stats, caption = “Initial Data Summary”)\n```\n\n\n\nNext, we execute our core statistical models. The key is that the R scripts in the models/ folder should only create the model objects; they should not save any output themselves.\nExample: models/01_linearRegression.R\n# models/01_linearRegression.R\n# This script’s ONLY job is to run the model and produce the ‘modelFit’ object.\n# It assumes ‘inventoryData’ is already loaded in the environment.\nmodelFit &lt;- lm(stockLevel ~ storageTemp + category, data = inventoryData)\nWe call this from mainAnalysis.qmd to create the modelFit object in our environment.\n### Running the Main Regression Model\nNow, we run the linear model specified in our external script. This will create a model object called `modelFit` in our R environment, which we can then use to generate results.\n```{{r runLinearModel}}\nsource(“models/01_linearRegression.R”)\n```\n\n\n\nWith the modelFit object created, we can now generate our final outputs from within mainAnalysis.qmd. The key here is that we don’t just display them—we save them as files to the results/ directory. This decouples our analysis from our final presentation, allowing us to simply \\input or \\includegraphics in our LaTeX documents later.\n\n\nWe’ll use the broom package to clean our model output and knitr::kable() with the latex format to create a publication-ready table.\n```{{r saveTable}}\n# Tidy the model output into a clean data frame\nmodelSummary &lt;- tidy(modelFit)\n\n# Use kable to format as a LaTeX table and save_kable to write it to a file\n# The booktabs = TRUE option creates a more professional-looking table\nkable(modelSummary, format = “latex”, booktabs = TRUE,\ncaption = “Regression of Stock Levels on Storage Temperature and Category.”,\ncol.names = c(“Term”, “Estimate”, “Std. Error”, “Statistic”, “P-value”), digits = 3) %&gt;%\nsave_kable(file = “results/tables/regressionTable.tex”)\n```\nThis code creates a file named regressionTable.tex in the results/tables/ folder. It’s a self-contained piece of LaTeX code, ready to be included in our paper.\n\n\n\nSimilarly, we use ggplot2 to create our plot and the ggsave() function to save it as a high-quality image file.\n```{{r saveFigure}}\n# Create the plot object\nstockPlot &lt;- ggplot(inventoryData, aes(x = category, y = stockLevel, fill = category)) +\ngeom_boxplot() +\nlabs(title = “Stock Levels by Product Category”, x = “Category”, y = “Stock Level”) +\ntheme_minimal()\n# Save the plot to the figures directory\n# Saving as .pdf is great for LaTeX; .png is better for websites.\nggsave( “results/figures/stockByCategory.pdf”, plot = stockPlot, width = 7, height = 5 )\n```\nThis creates a file named stockByCategory.pdf in our results/figures/ folder.\nBy the end of this process, running the single mainAnalysis.qmd file has verifiably transformed your raw evidence into a complete set of polished, presentation-ready results, all neatly organized in their respective folders.\n\n\n\n\nIt is important to highlight that the tools shown above (knitr::kable and ggplot2::ggsave) are just one way to accomplish our goal. The R ecosystem is rich with packages designed for creating high-quality tables and figures. The key is not the specific package, but the workflow: use code to generate and save your results as files.\nHere are some excellent alternatives you might explore:\n\nFor Tables:\n\nxtable: One of the oldest and most powerful packages for creating LaTeX and HTML tables. It offers a huge amount of control over the final output.\ntexreg: Fantastic for creating regression tables from multiple models, side-by-side, in a format common in academic journals.\nmodelsummary: A modern and extremely flexible package that can output results to a huge variety of formats (LaTeX, HTML, Word, Markdown) and is very easy to use.\n\nFor Figures:\n\nBase R Graphics: R’s built-in plot() function is powerful and fast. To save a base R plot, you would wrap the code like this: pdf(\"myplot.pdf\"); plot(x,y); dev.off().\ngoogleVis: Creates interactive HTML-based charts and maps using the Google Charts API. These are excellent for web-based presentations (website.qmd) but cannot be embedded in static PDF documents.\n\n\nFeel free to experiment and find the packages that you prefer. As long as you can save their output to a file in your results directory, they will fit perfectly into this reproducible workflow.\nIn Part 4: The Claim - Presenting Your Research, we’ll see how to write the paper.tex and slides.tex files that effortlessly pull in these saved tables and figures to construct a polished, professional, and fully reproducible final document."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis2.html#the-analysis-flow-a-narrative-driven-by-scripts",
    "href": "posts/UNBC thesis in latex/thesis2.html#the-analysis-flow-a-narrative-driven-by-scripts",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Our workflow is designed for clarity and modularity. The mainAnalysis.qmd file acts as the conductor, calling on specialized R scripts to perform specific tasks.\n\nLoad Evidence: Read the raw data from the 1_evidence directory.\nSource Explanatory Analysis: Call the 00_explanatory.R script to generate and load initial data objects.\nSource Research Models: Call the .R scripts from the models/ directory to run the core analyses and create model objects.\nSave Results: Use the objects created by the scripts to generate and save the final tables and figures into the results/ directory.\n\nLet’s walk through how this is implemented in our mainAnalysis.qmd file.\n\n\nWe begin with a setup chunk to load packages and a data chunk to load our raw data.\n```{{r setup, include=false}}\n# Load all necessary packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(knitr)\nlibrary(rio)\n# Set global options\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\n```\n### Data Loading\n```{{r loadData}}\n# The path goes up one directory (‘..’) then into the evidence folder.\ninventoryData &lt;- import(“../1_evidence/rawData/manualInventory.csv”)\n```\n\n\n\nWe execute our exploratory script. Its job is to perform initial calculations and create exploratory objects, which are then available for us to use in the main document.\n### Explanatory Analysis To begin, we explore the basic characteristics of our data by running the script in `explns/00_explanatory.R`.\n```{{r runExplanatoryAnalysis}}\n# This script creates objects like ‘summary_stats’\nsource(“explns/00_explanatory.R”)\n```\nNow we can display the `summary_stats` object that was just created.\n```{{r showExplanatoryStats, echo=true}}\n# Use kable for a clean summary table right here in our analysis\ndocument kable(summary_stats, caption = “Initial Data Summary”)\n```\n\n\n\nNext, we execute our core statistical models. The key is that the R scripts in the models/ folder should only create the model objects; they should not save any output themselves.\nExample: models/01_linearRegression.R\n# models/01_linearRegression.R\n# This script’s ONLY job is to run the model and produce the ‘modelFit’ object.\n# It assumes ‘inventoryData’ is already loaded in the environment.\nmodelFit &lt;- lm(stockLevel ~ storageTemp + category, data = inventoryData)\nWe call this from mainAnalysis.qmd to create the modelFit object in our environment.\n### Running the Main Regression Model\nNow, we run the linear model specified in our external script. This will create a model object called `modelFit` in our R environment, which we can then use to generate results.\n```{{r runLinearModel}}\nsource(“models/01_linearRegression.R”)\n```\n\n\n\nWith the modelFit object created, we can now generate our final outputs from within mainAnalysis.qmd. The key here is that we don’t just display them—we save them as files to the results/ directory. This decouples our analysis from our final presentation, allowing us to simply \\input or \\includegraphics in our LaTeX documents later.\n\n\nWe’ll use the broom package to clean our model output and knitr::kable() with the latex format to create a publication-ready table.\n```{{r saveTable}}\n# Tidy the model output into a clean data frame\nmodelSummary &lt;- tidy(modelFit)\n\n# Use kable to format as a LaTeX table and save_kable to write it to a file\n# The booktabs = TRUE option creates a more professional-looking table\nkable(modelSummary, format = “latex”, booktabs = TRUE,\ncaption = “Regression of Stock Levels on Storage Temperature and Category.”,\ncol.names = c(“Term”, “Estimate”, “Std. Error”, “Statistic”, “P-value”), digits = 3) %&gt;%\nsave_kable(file = “results/tables/regressionTable.tex”)\n```\nThis code creates a file named regressionTable.tex in the results/tables/ folder. It’s a self-contained piece of LaTeX code, ready to be included in our paper.\n\n\n\nSimilarly, we use ggplot2 to create our plot and the ggsave() function to save it as a high-quality image file.\n```{{r saveFigure}}\n# Create the plot object\nstockPlot &lt;- ggplot(inventoryData, aes(x = category, y = stockLevel, fill = category)) +\ngeom_boxplot() +\nlabs(title = “Stock Levels by Product Category”, x = “Category”, y = “Stock Level”) +\ntheme_minimal()\n# Save the plot to the figures directory\n# Saving as .pdf is great for LaTeX; .png is better for websites.\nggsave( “results/figures/stockByCategory.pdf”, plot = stockPlot, width = 7, height = 5 )\n```\nThis creates a file named stockByCategory.pdf in our results/figures/ folder.\nBy the end of this process, running the single mainAnalysis.qmd file has verifiably transformed your raw evidence into a complete set of polished, presentation-ready results, all neatly organized in their respective folders.\n\n\n\n\nIt is important to highlight that the tools shown above (knitr::kable and ggplot2::ggsave) are just one way to accomplish our goal. The R ecosystem is rich with packages designed for creating high-quality tables and figures. The key is not the specific package, but the workflow: use code to generate and save your results as files.\nHere are some excellent alternatives you might explore:\n\nFor Tables:\n\nxtable: One of the oldest and most powerful packages for creating LaTeX and HTML tables. It offers a huge amount of control over the final output.\ntexreg: Fantastic for creating regression tables from multiple models, side-by-side, in a format common in academic journals.\nmodelsummary: A modern and extremely flexible package that can output results to a huge variety of formats (LaTeX, HTML, Word, Markdown) and is very easy to use.\n\nFor Figures:\n\nBase R Graphics: R’s built-in plot() function is powerful and fast. To save a base R plot, you would wrap the code like this: pdf(\"myplot.pdf\"); plot(x,y); dev.off().\ngoogleVis: Creates interactive HTML-based charts and maps using the Google Charts API. These are excellent for web-based presentations (website.qmd) but cannot be embedded in static PDF documents.\n\n\nFeel free to experiment and find the packages that you prefer. As long as you can save their output to a file in your results directory, they will fit perfectly into this reproducible workflow.\nIn Part 4: The Claim - Presenting Your Research, we’ll see how to write the paper.tex and slides.tex files that effortlessly pull in these saved tables and figures to construct a polished, professional, and fully reproducible final document."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html",
    "href": "posts/UNBC thesis in latex/thesis3.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Welcome to the final post in our series on building a reproducible research workflow. So far, we have:\n\nEstablished a project structure based on the Toulmin model (Part 1).\nGathered our Evidence (raw data and references) into the 1_evidence directory (Part 2).\nBuilt our Warrant by scripting the analysis and saving the outputs to the 2_warrantAndAnalysis/results directory (Part 3).\n\nNow, we will construct the Claim—the polished, final documents where we present our findings. This happens in the 3_claimAndPresentation directory. The beauty of our workflow is that this final step is now incredibly simple and robust. Our presentation files don’t contain any analysis; they are lightweight documents that simply include the results we’ve already generated.\n\n\nBefore we build our final documents, it’s important to discuss the roles of the different authoring tools we’ve encountered.\n\nLaTeX (.tex): This is the gold standard for well-organized, professional, and beautifully typeset documents. Its power lies in its structure and control, making it ideal for a final, polished paper or a formal slide deck. We use it here for our paper.tex and slides.tex files.\nQuarto/R Markdown (.qmd/.Rmd): These tools are best for tentative, formatted analysis documents. Our mainAnalysis.qmd is a perfect example. It’s a working document where we weave code and narrative to tell the story of our analysis for ourselves and our close collaborators. It’s a dynamic notebook, not a final paper.\nR Sweave (.Rnw): This is the original tool for integrating R code directly into LaTeX documents. While powerful, Sweave compiles the R code and the LaTeX document at the same time. On my machine, this process can be slow, especially for large documents. For a short, two-to-three-page document, it can be efficient. However, for a full paper, I prefer our current workflow: separating the R analysis (.qmd) from the final LaTeX writing (.tex) for better speed and modularity.\n\n\n\n\nBefore we import our results, let’s understand how a LaTeX document is structured. Unlike a word processor, LaTeX is a markup language; you write plain text commands that tell the compiler how to format the document.\nA LaTeX document has two main parts:\n\nThe Preamble: This is everything from the very first line until the \\begin{document} command. Here, you define the document type (\\documentclass), load packages (\\usepackage), and set global document properties like the title and author.\nThe Body: This is everything between \\begin{document} and \\end{document}. It contains the actual content of your paper.\n\n\n\nHere are some of the most common commands you’ll use when writing your paper:\n\nHeadings: Use \\section{Section Name}, \\subsection{Subsection Name}, and \\subsubsection{Sub-subsection Name} to structure your paper. LaTeX handles the numbering and formatting automatically.\nParagraphs and Spacing: A blank line in your .tex file creates a new paragraph. LaTeX manages the indentation itself. To force a line break without a new paragraph, use \\\\.\nHorizontal Lines: To create a full-width horizontal line, for example to separate major sections, use the \\hrulefill command.\nText Formatting:\n\nItalics: \\emph{italicized text}\nBold: \\textbf{bold text}\n\nMath Equations: This is where LaTeX shines.\n\nInline math: \\( E = mc^2 \\) will place the equation within a sentence.\nDisplayed math: \\[ E = mc^2 \\] will give the equation its own centered line.\n\nLists:\n\nBulleted list: Use the itemize environment: \\begin{itemize} \\item First item. \\end{itemize}.\nNumbered list: Use the enumerate environment: \\begin{enumerate} \\item First item. \\end{enumerate}.\n\nFootnotes and Cross-References:\n\nCreate a footnote with \\footnote{This is the footnote text.}.\nTo reference a figure, table, or section, first label it with \\label{uniqueLabel}. Then, refer to it anywhere in the text with \\ref{uniqueLabel}. LaTeX will automatically insert the correct number.\n\n\n\n\n\nManaging citations is a core strength of LaTeX. The process involves three parts:\n\nThe .bib file: Your literature.bib file in the 1_evidence folder, which contains all your reference data.\nThe \\bibliography command: At the end of your document (before \\end{document}), you tell LaTeX which .bib file to use: \\bibliography{../1_evidence/literature.bib}.\nThe \\bibliographystyle command: Placed before the \\bibliography command, this sets the citation style (e.g., \\bibliographystyle{apa}).\n\nTo actually cite a source in your text, you use a cite command. When using an author-year style with the natbib package (which we included in our preamble with \\usepackage[authoryear]{natbib}), you have several powerful options:\n\n\\citep{key} (Parenthetical Citation): This is the most common command. It wraps the entire author-year reference in parentheses.\n\nExample: ...as shown in previous work \\citep{Gandrud2020}.\nOutput: …as shown in previous work (Gandrud, 2020).\n\n\\cite{key} or \\citet{key} (Textual Citation): This command is used when the author’s name is part of the sentence. It keeps the author name in the main text and only puts the year in parentheses. \\citet is the more robust command from natbib.\n\nExample: According to \\citet{Gandrud2020}, this workflow is...\nOutput: According to Gandrud (2020), this workflow is…\n\nCiting Multiple Sources: You can include multiple keys in a single command, separated by commas.\n\nExample: ...as shown in previous work \\citep{Gandrud2020, Donoho2010}.\nOutput: …as shown in previous work (Gandrud, 2020; Donoho, 2010).\n\nAdding Page Numbers: To add a page number or other note inside the citation, use square brackets.\n\nExample: ...this specific point is made in \\citet[p. 45]{Gandrud2020}.\nOutput: …this specific point is made in Gandrud (2020, p. 45).\n\n\n\n\n\n\n\n\nFor those who prefer to stay within the RStudio environment, compiling a .tex file into a PDF requires a LaTeX distribution. Traditionally, this meant installing large, separate programs like MiKTeX (Windows) or MacTeX (macOS). However, there is a much simpler, R-native solution: the tinytex package.\nWhat is tinytex? Created by Yihui Xie (the same author of knitr), tinytex is an R package that installs a lightweight, self-contained, and portable LaTeX distribution.\nWhy use it?\n\nSimplicity: You can install it directly from the R console: install.packages(\"tinytex\"); tinytex::install_tinytex().\nAutomatic Package Installation: This is its killer feature. If your LaTeX document requires a package (e.g., \\usepackage{booktabs}) that isn’t in the base tinytex installation, tinytex will automatically find and install it for you during compilation. This saves an enormous amount of time troubleshooting obscure LaTeX errors.\nSelf-Contained: It lives within your R library paths and doesn’t interfere with any other LaTeX installations you might have.\n\nWith tinytex installed, you can write your paper.tex file in RStudio and simply click the “Compile PDF” button to generate your final document. It’s a seamless, integrated experience.\n\n\n\nAs discussed in the previous post, we will use GitHub to store our project and Overleaf as our collaborative LaTeX editor. By linking your Overleaf project to your GitHub repository, you ensure your writing environment always has access to your latest results.\nWhile we can write LaTeX on our local machine, a more powerful modern approach is to use Overleaf, a collaborative, cloud-based LaTeX editor. It allows multiple co-authors to work on a document simultaneously and focuses purely on writing.\nBut how do we get our results from our local machine into our Overleaf project? The bridge is GitHub.\nBy keeping our entire project in a GitHub repository, we create a “single source of truth” in the cloud. Overleaf can then link directly to this repository, allowing us to pull in our results seamlessly.\n\n\n\nAfter you run your mainAnalysis.qmd file locally, the results/ directory is updated with the latest tables and figures. The next step is to commit and push these changes to your GitHub repository.\nUsing the command line (or a Git client), you would run:\nBash\n# Stage the new results\ngit add 2_warrantAndAnalysis/results/\n\n# Commit the changes with a descriptive message\ngit commit -m \"Update regression table and stock level figure\"\n\n# Push the changes to your main branch on GitHub\ngit push origin main\nYour latest results are now securely stored and versioned on GitHub.\n\n\n\nOverleaf has a fantastic feature to link a project directly to a GitHub repository.\n\nIn Overleaf, create a new project.\nIn the project menu, select “Import from GitHub”.\nAuthorize Overleaf to access your GitHub account and select your research project repository.\n\nOverleaf will now clone your entire project. You can work on your paper.tex file directly in Overleaf’s editor. To include your results, you simply need to provide the correct file path from the root of the project.\n\n\n\nOnce your Overleaf project is synced with GitHub, you can pull in the results.\n\n\nTo include a table, we use the \\input{} command. This command acts as if you are “pasting” the entire content of the specified file right into that spot. Since we saved our table as a complete, self-contained .tex file, this is incredibly clean.\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n% Create a table float environment for the caption and label\n\\begin{table}[h!]\n    \\centering\n    % Input the table we saved in Part 3.\n    % The path is relative to the root of our project folder.\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\nThe table environment provides a caption and a referenceable label. The \\input command does the heavy lifting of bringing in the table itself.\n\n\n\nTo include a figure, we use the \\includegraphics{} command inside a figure environment. The figure environment is crucial as it provides the caption and label, just like for a table.\nlatex\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n% Create a figure float environment\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure we saved in Part 3.\n    % We use the .pdf for high-quality printing.\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\nThe [width=0.8\\textwidth] option scales the image to be 80% of the page’s text width, ensuring it fits nicely.\nHere is a simplified paper.tex file as it would look inside your Overleaf project:\n\\documentclass{article}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage[authoryear]{natbib}\n\n\\title{Analysis of Inventory Stock Levels}\n\\author{A. Researcher}\n\\date{\\today}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\\maketitle\n\n\\section{Introduction}\nThis paper investigates the factors affecting inventory stock levels...\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure from the synced GitHub folder\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\n\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n\\begin{table}[h!]\n    \\centering\n    % Input the table from the synced GitHub folder\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\n\n% --- BIBLIOGRAPHY ---\n\\bibliographystyle{apa}\n% Point to the BibTeX file in the synced GitHub folder\n\\bibliography{1_evidence/literature.bib}\n\n\\end{document}\nWhen you need to update your results, the workflow is simple:\n\nRun mainAnalysis.qmd on your local machine.\nPush the updated results/ folder to GitHub.\nIn Overleaf, click the “Git” menu and “Pull” the latest changes from GitHub.\nRecompile your paper.tex.\n\nYour paper is now updated with the latest results, with zero manual copy-pasting.\n\n\n\n\n\nCreating a conference presentation with LaTeX Beamer follows the exact same logic. The document structure is different (using \\begin{frame} environments), but the principle of including results is identical.\n\\documentclass{beamer}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\n\\title{Inventory Stock Level Analysis}\n\\author{A. Researcher}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\n\\begin{frame}\n    \\titlepage\n\\end{frame}\n\n\\begin{frame}{Regression Results}\n    % Input the same table file\n    \\input{../2_warrantAndAnalysis/results/tables/regressionTable.tex}\n\\end{frame}\n\n\\end{document}\nYou can use the very same result files across multiple presentation formats without any extra work.\n\n\n\nFor a blog post or project website, we can use Quarto. The syntax is different, but the concept is the same: refer to the files in the results directory.\n---\ntitle: \"A Blog Post on My Inventory Research\" \\\nauthor: \"A. Researcher\" format: html \n---\n\n\\## Key Findings Our research explored the factors affecting inventory stock levels. The main regression results are summarized below. \\`\\`\\`{r, results='asis'} \\# Read and display the raw LaTeX from the results file \\# Quarto is smart enough to render it correctly to HTML knitr::asis_output(readLines(\"../2_warrantAndAnalysis/results/tables/regressionTable.tex\")) \\`\\`\\` We also found a key difference between product categories, as illustrated in this plot: \\![Stock Levels by Category\\](../2_warrantAndAnalysis/results/figures/stockByCategory.png) \\\nNote that for a website, it’s often better to save and use the .png version of the figure, as it’s more web-friendly.\n\n\n\nThis four-part series has taken us through a complete, reproducible research workflow. By separating our project into Evidence, Warrant, and Claim, and by using modern tools like R, Quarto, GitHub, and Overleaf, we have created a system that is not only transparent and reproducible but also highly efficient and collaborative.\nThis workflow requires discipline, but the payoff in clarity, accuracy, and peace of mind is immeasurable. Happy researching"
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html#a-note-on-authoring-tools-choosing-the-right-tool-for-the-job",
    "href": "posts/UNBC thesis in latex/thesis3.html#a-note-on-authoring-tools-choosing-the-right-tool-for-the-job",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we build our final documents, it’s important to discuss the roles of the different authoring tools we’ve encountered.\n\nLaTeX (.tex): This is the gold standard for well-organized, professional, and beautifully typeset documents. Its power lies in its structure and control, making it ideal for a final, polished paper or a formal slide deck. We use it here for our paper.tex and slides.tex files.\nQuarto/R Markdown (.qmd/.Rmd): These tools are best for tentative, formatted analysis documents. Our mainAnalysis.qmd is a perfect example. It’s a working document where we weave code and narrative to tell the story of our analysis for ourselves and our close collaborators. It’s a dynamic notebook, not a final paper.\nR Sweave (.Rnw): This is the original tool for integrating R code directly into LaTeX documents. While powerful, Sweave compiles the R code and the LaTeX document at the same time. On my machine, this process can be slow, especially for large documents. For a short, two-to-three-page document, it can be efficient. However, for a full paper, I prefer our current workflow: separating the R analysis (.qmd) from the final LaTeX writing (.tex) for better speed and modularity."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html#latex-basics-the-structure-of-a-document",
    "href": "posts/UNBC thesis in latex/thesis3.html#latex-basics-the-structure-of-a-document",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we import our results, let’s understand how a LaTeX document is structured. Unlike a word processor, LaTeX is a markup language; you write plain text commands that tell the compiler how to format the document.\nA LaTeX document has two main parts:\n\nThe Preamble: This is everything from the very first line until the \\begin{document} command. Here, you define the document type (\\documentclass), load packages (\\usepackage), and set global document properties like the title and author.\nThe Body: This is everything between \\begin{document} and \\end{document}. It contains the actual content of your paper.\n\n\n\nHere are some of the most common commands you’ll use when writing your paper:\n\nHeadings: Use \\section{Section Name}, \\subsection{Subsection Name}, and \\subsubsection{Sub-subsection Name} to structure your paper. LaTeX handles the numbering and formatting automatically.\nParagraphs and Spacing: A blank line in your .tex file creates a new paragraph. LaTeX manages the indentation itself. To force a line break without a new paragraph, use \\\\.\nHorizontal Lines: To create a full-width horizontal line, for example to separate major sections, use the \\hrulefill command.\nText Formatting:\n\nItalics: \\emph{italicized text}\nBold: \\textbf{bold text}\n\nMath Equations: This is where LaTeX shines.\n\nInline math: \\( E = mc^2 \\) will place the equation within a sentence.\nDisplayed math: \\[ E = mc^2 \\] will give the equation its own centered line.\n\nLists:\n\nBulleted list: Use the itemize environment: \\begin{itemize} \\item First item. \\end{itemize}.\nNumbered list: Use the enumerate environment: \\begin{enumerate} \\item First item. \\end{enumerate}.\n\nFootnotes and Cross-References:\n\nCreate a footnote with \\footnote{This is the footnote text.}.\nTo reference a figure, table, or section, first label it with \\label{uniqueLabel}. Then, refer to it anywhere in the text with \\ref{uniqueLabel}. LaTeX will automatically insert the correct number.\n\n\n\n\n\nManaging citations is a core strength of LaTeX. The process involves three parts:\n\nThe .bib file: Your literature.bib file in the 1_evidence folder, which contains all your reference data.\nThe \\bibliography command: At the end of your document (before \\end{document}), you tell LaTeX which .bib file to use: \\bibliography{../1_evidence/literature.bib}.\nThe \\bibliographystyle command: Placed before the \\bibliography command, this sets the citation style (e.g., \\bibliographystyle{apa}).\n\nTo actually cite a source in your text, you use a cite command. When using an author-year style with the natbib package (which we included in our preamble with \\usepackage[authoryear]{natbib}), you have several powerful options:\n\n\\citep{key} (Parenthetical Citation): This is the most common command. It wraps the entire author-year reference in parentheses.\n\nExample: ...as shown in previous work \\citep{Gandrud2020}.\nOutput: …as shown in previous work (Gandrud, 2020).\n\n\\cite{key} or \\citet{key} (Textual Citation): This command is used when the author’s name is part of the sentence. It keeps the author name in the main text and only puts the year in parentheses. \\citet is the more robust command from natbib.\n\nExample: According to \\citet{Gandrud2020}, this workflow is...\nOutput: According to Gandrud (2020), this workflow is…\n\nCiting Multiple Sources: You can include multiple keys in a single command, separated by commas.\n\nExample: ...as shown in previous work \\citep{Gandrud2020, Donoho2010}.\nOutput: …as shown in previous work (Gandrud, 2020; Donoho, 2010).\n\nAdding Page Numbers: To add a page number or other note inside the citation, use square brackets.\n\nExample: ...this specific point is made in \\citet[p. 45]{Gandrud2020}.\nOutput: …this specific point is made in Gandrud (2020, p. 45)."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html#compiling-your-document-two-powerful-approaches",
    "href": "posts/UNBC thesis in latex/thesis3.html#compiling-your-document-two-powerful-approaches",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "For those who prefer to stay within the RStudio environment, compiling a .tex file into a PDF requires a LaTeX distribution. Traditionally, this meant installing large, separate programs like MiKTeX (Windows) or MacTeX (macOS). However, there is a much simpler, R-native solution: the tinytex package.\nWhat is tinytex? Created by Yihui Xie (the same author of knitr), tinytex is an R package that installs a lightweight, self-contained, and portable LaTeX distribution.\nWhy use it?\n\nSimplicity: You can install it directly from the R console: install.packages(\"tinytex\"); tinytex::install_tinytex().\nAutomatic Package Installation: This is its killer feature. If your LaTeX document requires a package (e.g., \\usepackage{booktabs}) that isn’t in the base tinytex installation, tinytex will automatically find and install it for you during compilation. This saves an enormous amount of time troubleshooting obscure LaTeX errors.\nSelf-Contained: It lives within your R library paths and doesn’t interfere with any other LaTeX installations you might have.\n\nWith tinytex installed, you can write your paper.tex file in RStudio and simply click the “Compile PDF” button to generate your final document. It’s a seamless, integrated experience.\n\n\n\nAs discussed in the previous post, we will use GitHub to store our project and Overleaf as our collaborative LaTeX editor. By linking your Overleaf project to your GitHub repository, you ensure your writing environment always has access to your latest results.\nWhile we can write LaTeX on our local machine, a more powerful modern approach is to use Overleaf, a collaborative, cloud-based LaTeX editor. It allows multiple co-authors to work on a document simultaneously and focuses purely on writing.\nBut how do we get our results from our local machine into our Overleaf project? The bridge is GitHub.\nBy keeping our entire project in a GitHub repository, we create a “single source of truth” in the cloud. Overleaf can then link directly to this repository, allowing us to pull in our results seamlessly.\n\n\n\nAfter you run your mainAnalysis.qmd file locally, the results/ directory is updated with the latest tables and figures. The next step is to commit and push these changes to your GitHub repository.\nUsing the command line (or a Git client), you would run:\nBash\n# Stage the new results\ngit add 2_warrantAndAnalysis/results/\n\n# Commit the changes with a descriptive message\ngit commit -m \"Update regression table and stock level figure\"\n\n# Push the changes to your main branch on GitHub\ngit push origin main\nYour latest results are now securely stored and versioned on GitHub.\n\n\n\nOverleaf has a fantastic feature to link a project directly to a GitHub repository.\n\nIn Overleaf, create a new project.\nIn the project menu, select “Import from GitHub”.\nAuthorize Overleaf to access your GitHub account and select your research project repository.\n\nOverleaf will now clone your entire project. You can work on your paper.tex file directly in Overleaf’s editor. To include your results, you simply need to provide the correct file path from the root of the project.\n\n\n\nOnce your Overleaf project is synced with GitHub, you can pull in the results.\n\n\nTo include a table, we use the \\input{} command. This command acts as if you are “pasting” the entire content of the specified file right into that spot. Since we saved our table as a complete, self-contained .tex file, this is incredibly clean.\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n% Create a table float environment for the caption and label\n\\begin{table}[h!]\n    \\centering\n    % Input the table we saved in Part 3.\n    % The path is relative to the root of our project folder.\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\nThe table environment provides a caption and a referenceable label. The \\input command does the heavy lifting of bringing in the table itself.\n\n\n\nTo include a figure, we use the \\includegraphics{} command inside a figure environment. The figure environment is crucial as it provides the caption and label, just like for a table.\nlatex\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n% Create a figure float environment\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure we saved in Part 3.\n    % We use the .pdf for high-quality printing.\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\nThe [width=0.8\\textwidth] option scales the image to be 80% of the page’s text width, ensuring it fits nicely.\nHere is a simplified paper.tex file as it would look inside your Overleaf project:\n\\documentclass{article}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\\usepackage[authoryear]{natbib}\n\n\\title{Analysis of Inventory Stock Levels}\n\\author{A. Researcher}\n\\date{\\today}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\\maketitle\n\n\\section{Introduction}\nThis paper investigates the factors affecting inventory stock levels...\nAs shown in Figure \\ref{fig:stockPlot}, we find a significant difference between categories.\n\n\\begin{figure}[h!]\n    \\centering\n    % Include the figure from the synced GitHub folder\n    \\includegraphics[width=0.8\\textwidth]{2_warrantAndAnalysis/results/figures/stockByCategory.pdf}\n    \\caption{Stock Levels by Product Category.}\n    \\label{fig:stockPlot}\n\\end{figure}\n\n\\section{Results}\nOur primary regression analysis, detailed in Table \\ref{tab:reg}, reveals...\n\n\\begin{table}[h!]\n    \\centering\n    % Input the table from the synced GitHub folder\n    \\input{2_warrantAndAnalysis/results/tables/regressionTable.tex}\n    \\label{tab:reg}\n\\end{table}\n\n% --- BIBLIOGRAPHY ---\n\\bibliographystyle{apa}\n% Point to the BibTeX file in the synced GitHub folder\n\\bibliography{1_evidence/literature.bib}\n\n\\end{document}\nWhen you need to update your results, the workflow is simple:\n\nRun mainAnalysis.qmd on your local machine.\nPush the updated results/ folder to GitHub.\nIn Overleaf, click the “Git” menu and “Pull” the latest changes from GitHub.\nRecompile your paper.tex.\n\nYour paper is now updated with the latest results, with zero manual copy-pasting."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html#building-the-slides-slides.tex",
    "href": "posts/UNBC thesis in latex/thesis3.html#building-the-slides-slides.tex",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Creating a conference presentation with LaTeX Beamer follows the exact same logic. The document structure is different (using \\begin{frame} environments), but the principle of including results is identical.\n\\documentclass{beamer}\n\n% --- PREAMBLE ---\n\\usepackage{graphicx}\n\\usepackage{booktabs}\n\n\\title{Inventory Stock Level Analysis}\n\\author{A. Researcher}\n\n% --- DOCUMENT START ---\n\\begin{document}\n\n\\begin{frame}\n    \\titlepage\n\\end{frame}\n\n\\begin{frame}{Regression Results}\n    % Input the same table file\n    \\input{../2_warrantAndAnalysis/results/tables/regressionTable.tex}\n\\end{frame}\n\n\\end{document}\nYou can use the very same result files across multiple presentation formats without any extra work."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html#building-the-website-website.qmd",
    "href": "posts/UNBC thesis in latex/thesis3.html#building-the-website-website.qmd",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "For a blog post or project website, we can use Quarto. The syntax is different, but the concept is the same: refer to the files in the results directory.\n---\ntitle: \"A Blog Post on My Inventory Research\" \\\nauthor: \"A. Researcher\" format: html \n---\n\n\\## Key Findings Our research explored the factors affecting inventory stock levels. The main regression results are summarized below. \\`\\`\\`{r, results='asis'} \\# Read and display the raw LaTeX from the results file \\# Quarto is smart enough to render it correctly to HTML knitr::asis_output(readLines(\"../2_warrantAndAnalysis/results/tables/regressionTable.tex\")) \\`\\`\\` We also found a key difference between product categories, as illustrated in this plot: \\![Stock Levels by Category\\](../2_warrantAndAnalysis/results/figures/stockByCategory.png) \\\nNote that for a website, it’s often better to save and use the .png version of the figure, as it’s more web-friendly."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis3.html#conclusion-a-coherent-whole",
    "href": "posts/UNBC thesis in latex/thesis3.html#conclusion-a-coherent-whole",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "This four-part series has taken us through a complete, reproducible research workflow. By separating our project into Evidence, Warrant, and Claim, and by using modern tools like R, Quarto, GitHub, and Overleaf, we have created a system that is not only transparent and reproducible but also highly efficient and collaborative.\nThis workflow requires discipline, but the payoff in clarity, accuracy, and peace of mind is immeasurable. Happy researching"
  },
  {
    "objectID": "posts/generationalAccountingModel/gam0.html",
    "href": "posts/generationalAccountingModel/gam0.html",
    "title": "Generational Accounting and OLG Models: An Introduction",
    "section": "",
    "text": "In recent decades, many developed nations, including Japan, have faced unprecedented demographic shifts: declining birth rates and rapidly aging populations. These are not just abstract statistics; they have profound and concrete implications for the economy. How will a smaller workforce support a larger retired population? What will happen to savings rates, economic growth, and the sustainability of public finances and social security systems?\nTo analyze these long-term, complex issues, economists need specialized tools. A simple snapshot of the economy today is insufficient. We need models that can project the consequences of our current policies and demographic trends far into the future. This is where Overlapping Generations (OLG) models and the concept of Generational Accounting become essential.\nThis post is the first in a series where we will explore these topics by translating the classic textbook, “Matlabによるマクロ経済モデル入門” (Introduction to Macroeconomic Models with Matlab) by Kazumasa Oguro and Manabu Shimasawa, from its original MATLAB into R. Today, we’ll focus on the core concepts: what are these models and why are they so powerful?"
  },
  {
    "objectID": "posts/generationalAccountingModel/gam0.html#the-challenge-understanding-long-term-economic-change",
    "href": "posts/generationalAccountingModel/gam0.html#the-challenge-understanding-long-term-economic-change",
    "title": "Generational Accounting and OLG Models: An Introduction",
    "section": "",
    "text": "In recent decades, many developed nations, including Japan, have faced unprecedented demographic shifts: declining birth rates and rapidly aging populations. These are not just abstract statistics; they have profound and concrete implications for the economy. How will a smaller workforce support a larger retired population? What will happen to savings rates, economic growth, and the sustainability of public finances and social security systems?\nTo analyze these long-term, complex issues, economists need specialized tools. A simple snapshot of the economy today is insufficient. We need models that can project the consequences of our current policies and demographic trends far into the future. This is where Overlapping Generations (OLG) models and the concept of Generational Accounting become essential.\nThis post is the first in a series where we will explore these topics by translating the classic textbook, “Matlabによるマクロ経済モデル入門” (Introduction to Macroeconomic Models with Matlab) by Kazumasa Oguro and Manabu Shimasawa, from its original MATLAB into R. Today, we’ll focus on the core concepts: what are these models and why are they so powerful?"
  },
  {
    "objectID": "posts/generationalAccountingModel/gam0.html#what-is-an-overlapping-generations-olg-model",
    "href": "posts/generationalAccountingModel/gam0.html#what-is-an-overlapping-generations-olg-model",
    "title": "Generational Accounting and OLG Models: An Introduction",
    "section": "What is an Overlapping Generations (OLG) Model?",
    "text": "What is an Overlapping Generations (OLG) Model?\nAt its heart, an OLG model is a simulation of an economy populated by realistic, mortal individuals. Unlike simpler models that assume an infinitely-lived “representative agent,” the OLG framework acknowledges a fundamental truth: at any given time, society is a mix of different generations living side-by-side.\nAs the book explains, this “overlapping” of generations is the model’s key feature. It allows us to simulate the economic lifecycle:\n\nYouth (Working Life): Individuals enter the economy, supply labor, earn wages, and make decisions about how much to consume and how much to save for the future.\nOld Age (Retirement): Individuals retire from the workforce and live off the savings they accumulated during their working years, supplemented by pensions and other transfers.\n\nBy modeling the economy as a collection of these lifecycle agents, we can analyze how macroeconomic variables emerge from individual decisions. For example, a country’s national savings rate is simply the sum of the positive savings of the young working generations minus the negative savings (or “dissaving”) of the retired generations.\nThis structure makes OLG models uniquely suited to study issues where timing and age are critical, such as:\n\nPension System Reform: How does changing from a pay-as-you-go to a funded pension system affect different cohorts?\nGovernment Debt: Who ultimately bears the burden of public debt—current taxpayers or future ones?\nDemographic Change: What happens to national savings and capital accumulation when the proportion of retirees to workers increases?"
  },
  {
    "objectID": "posts/generationalAccountingModel/gam0.html#generational-accounting-who-pays-the-bill",
    "href": "posts/generationalAccountingModel/gam0.html#generational-accounting-who-pays-the-bill",
    "title": "Generational Accounting and OLG Models: An Introduction",
    "section": "Generational Accounting: Who Pays the Bill?",
    "text": "Generational Accounting: Who Pays the Bill?\nThe OLG framework provides the engine for a powerful analytical concept known as Generational Accounting. The goal is to calculate, under a given set of policies, the total net amount that different generations are expected to pay to the government over their entire lifetimes.\nA Generational Account is an estimate of the present value of all future taxes a person from a specific generation will pay, minus the present value of all the government benefits (like social security and healthcare) they will receive.\n\nIf the net amount is positive, that generation is a net contributor to the government.\nIf the net amount is negative, that generation is a net beneficiary.\n\nBy comparing these accounts across generations, we can quantify the fairness of fiscal policy. For instance, if policies are enacted that benefit today’s older generations (e.g., maintaining high pension benefits without raising taxes), Generational Accounting will show that the burden is shifted onto younger and future generations, who will face higher taxes or lower benefits over their lifetimes.\nThe book highlights that this is the crucial advantage of the OLG approach over simpler methods. It captures the forward-looking nature of economic behavior. If the government announces a future tax increase, rational individuals will adjust their saving and consumption behavior today, and the model can capture this dynamic effect."
  },
  {
    "objectID": "posts/generationalAccountingModel/gam0.html#our-goal-from-matlab-to-an-interactive-r-model",
    "href": "posts/generationalAccountingModel/gam0.html#our-goal-from-matlab-to-an-interactive-r-model",
    "title": "Generational Accounting and OLG Models: An Introduction",
    "section": "Our Goal: From MATLAB to an Interactive R Model",
    "text": "Our Goal: From MATLAB to an Interactive R Model\nThe textbook by Oguro and Shimasawa provides a comprehensive guide to building and simulating these models in MATLAB. Our goal in this series is to replicate this work in R, making these powerful tools accessible to a wider audience in the data science and R communities.\nIn the upcoming posts, we will dive into the code, translating the MATLAB scripts chapter by chapter. We will cover:\n\nSetting up the model parameters in R.\nCalculating the economy’s long-run “steady state.”\nSimulating the transition path from one equilibrium to another.\nVisualizing the results for key economic variables.\n\nUltimately, we will build an interactive Shiny application to explore policy scenarios in real-time.\nStay tuned for the next post, where we will begin our coding journey by setting up the basic one-country model in R."
  },
  {
    "objectID": "posts/academic reporting in tex/Academic Report1.html",
    "href": "posts/academic reporting in tex/Academic Report1.html",
    "title": "Crafting Academic Reports: A LaTeX and Overleaf Guide",
    "section": "",
    "text": "The Challenge of Academic Reporting\nThe academic world demands precision, clarity, and professionalism, and nowhere is this more evident than in the reports and assignments we submit. You’ve poured hours into your research, meticulously crunching numbers, dissecting texts, and forming insightful conclusions. But what happens when it’s time to present that hard work? Many students find themselves wrestling with word processors, battling formatting issues, misaligned equations, and stubborn bibliographies. Often, these technical struggles can detract from the quality of your valuable content.\n\n\nIntroducing LaTeX: The Gold Standard for Professional Documents\nWhat if there was a better way? A tool that handles complex typesetting with grace, ensures consistent formatting across your entire document, and lets you focus on what truly matters: your ideas. That tool is LaTeX. LaTeX isn’t just another word processor; it’s a document preparation system renowned in academia and science for its ability to produce stunning, professional-quality documents with unparalleled precision, especially when it comes to mathematical equations, complex figures, and intricate citations. It’s the gold standard for anyone serious about academic writing.\n\n\nOverleaf: Making LaTeX Accessible\nBut we get it—the idea of learning a new system, especially one that involves coding, can seem daunting. That’s where Overleaf comes in. Think of Overleaf as your collaborative, cloud-based hub for LaTeX. It strips away the installation hassles and steep learning curves, providing an intuitive online environment where you can write, compile, and share your LaTeX documents in real-time. It’s like Google Docs, but purpose-built for the power of LaTeX.\n\n\nWhat You’ll Learn in This Guide\nIn this guide, we’ll demystify the process of creating academic reports using LaTeX on Overleaf. We’ll explore why these tools are a game-changer for students and researchers alike. I’ll also walk you through a practical example based on an actual university assignment, providing concrete steps you can follow. By the end, you’ll be equipped to elevate your academic writing, producing reports that not only look professional but are also a joy to create."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div2_inventory.html",
    "href": "posts/DynamicInventoryManagement/div2_inventory.html",
    "title": "Dynamic Inventory Theories: Unpacking Inventory Management Challenges",
    "section": "",
    "text": "In our previous post, we introduced Dynamic Production Management (DPM) as a paradigm shift from traditional production planning to a demand-centric approach. Building upon that foundation, this post delves deeper into the intricacies of inventory management, specifically addressing why conventional methods often fall short in today’s dynamic business environment. We will explore the challenges posed by classical inventory theory and introduce the concept of “inventory flow management” as a more adaptive solution. Furthermore, we will formulate a basic Inventory Dynamic Model with relevant equations.\n\n\nSince the mid-19th to mid-20th century, characterized by mass production and a seller’s market, the primary focus was on efficiently increasing production volume and ensuring a smooth supply of materials. It was within this context that classical inventory management mechanisms were shaped. The fundamental principle was simple: reorder when stock levels drop to a certain point, much like how we manage everyday household items. This is the essence of the reorder point system.\nHowever, this approach quickly encounters problems with materials that have long lead times (e.g., 3-6 months or more). Applying a reorder point system to such items can lead to excessive inventory or, conversely, frequent stockouts. To address this, methods like periodic replenishment were devised, where materials are ordered periodically based on future needs. While ideally linked to production plans, the unreliability of long-term production forecasts often meant that material procurement had to rely on independent forecasts.\nClassical inventory theory, born from the management of factory material warehouses, is still prevalent today. However, the question arises: can this classical theory be applied to the diverse forms of inventory that exist now, including finished goods warehouses, regional distribution centers, wholesalers, retailers, in-process inventory within factories, and even materials at external suppliers? The conclusion, as presented by tocken.com, is a resounding “No.” The core issues lie in two critical aspects: asymmetry and the ambiguity of optimal inventory.\n\n\n\nConsider a typical inventory management scenario where entity A manages its inventory, fulfilling customer orders and replenishing stock by ordering from entity B. Both the ordering process from A to B and the customer order process to A involve recording order times and quantities. This commercial transaction data is meticulously managed. However, when this data is brought into the inventory management framework, a critical asymmetry emerges.\nInventory management fundamentally involves adjusting “input” (replenishment) to match “output” (demand). Therefore, both input and output must be measured with the same yardstick. Demand, being largely uncontrollable, must be accepted as it is, making it crucial to understand its magnitude, including its variability. Yet, classical inventory theory often overlooks this crucial aspect.\nFor instance, when forecasting demand for inventory management, say for a two-month lead time, do we separately forecast the number of orders and the quantity per order? Classical inventory theory typically does not. It often provides a single demand forecast (e.g., XXX units for two months) without breaking down the number of orders or the quantity per order. While we meticulously consider order intervals and quantities when placing orders, our view of demand tends to be broad and undifferentiated. This disparity in how “output” and “input” are perceived—using different lenses for logistics—is what constitutes asymmetry.\nThis asymmetry has significant implications. Consider two scenarios where the average demand over a period is 200 units: one with an average of 40 orders of 5 units each, and another with 10 orders of 20 units each. While the average total demand is the same, the variability will differ significantly. Intuitively, receiving many small orders is preferable to a few large ones for minimizing inventory. Therefore, to accurately capture demand variability, it is essential to consider the number of orders and the quantity per order separately.\nSupply chains are interconnected networks of inventory management units. If the “output” and “input” are viewed from different perspectives, or if there is a lack of symmetry between them, the connections between these inventory management units become disjointed. From a physics perspective, symmetry is crucial for universality. Just as physical laws exhibit symmetry, suggesting their universal applicability, a lack of symmetry in inventory replenishment mechanisms implies a narrow scope for classical inventory theory.\n\n\n\nLet’s trace the flow of inventory through time: order received → shipment → inventory reduction → waiting for order → order placed → outstanding order → delivery → warehousing. Each event triggers a change in inventory status, and the quantity in each status fluctuates. It’s important to note that beyond physical stock in the warehouse, “inventory in transit” (waiting for order, outstanding order) often exists and can sometimes exceed physical stock. The quantity in each state increases or decreases with every event.\nHowever, classical inventory theory struggles to define precisely what constitutes “optimal inventory.” While some explanations might include outstanding orders in the definition of optimal inventory, most often link it solely to physical stock. Regardless, the quantity in each state fluctuates, and even when combined, these quantities exhibit variability. In inventory management, “optimal inventory” is a crucial management criterion. Yet, in practice, it remains ambiguous which inventory state should be used for comparison.\n\n\n\nThe recent surge in Digital Transformation (DX) initiatives highlights a growing concern: the increasing divergence between classical inventory theory and modern business realities. The need for DX in inventory management is urgent. The problems of “asymmetry” and “ambiguity of optimal inventory” inherent in classical theory are becoming more pronounced. The traditional approach, which assumes a stable environment and focuses on optimizing static inventory levels, is ill-equipped to handle the dynamic, interconnected, and data-rich landscape of today’s supply chains.\nDX in inventory management is not merely about digitizing existing processes; it’s about fundamentally transforming how inventory is perceived and managed. It necessitates a shift towards real-time data utilization, predictive analytics, and agile decision-making. The goal is to move beyond simply counting stock to understanding the flow and behaviour of inventory across the entire supply chain, enabling proactive adjustments rather than reactive responses.\n\n\n\nTo overcome the limitations of classical inventory theory introduces the concept of an Inventory Dynamic Model. This model emphasizes understanding inventory not as static quantities but as dynamic flows within a system. It recognizes that inventory is constantly in motion, transitioning through various states from raw materials to finished goods, and that these flows are influenced by both internal processes and external demand.\nWe can conceptualize the inventory dynamic model using a set of differential equations that describe the change in inventory levels over time. Let \\(I(t)\\) be the inventory level at time \\(t\\). The rate of change of inventory can be expressed as the difference between the inflow (replenishment rate) and the outflow (demand rate).\nLet: * \\(R(t)\\) = Replenishment rate (inflow) at time \\(t\\) * \\(D(t)\\) = Demand rate (outflow) at time \\(t\\)\nThe fundamental equation for the inventory dynamic model is:\n\\[ \\frac{dI(t)}{dt} = R(t) - D(t) \\]\nThis equation states that the rate of change of inventory over time is equal to the replenishment rate minus the demand rate. This is a continuous model, representing inventory as a dynamic that flows in and out of a system.\nTo further elaborate, we can consider the components of replenishment and demand. Replenishment can be influenced by factors such as lead time (\\(L\\)), which is the time delay between placing an order and receiving it. Demand can be influenced by various market factors and can be stochastic.\nConsider a simplified scenario where replenishment orders are placed based on a reorder point (\\(S\\)) and a fixed order quantity (\\(Q\\)). The replenishment rate \\(R(t)\\) would then be a function of the orders placed at an earlier time \\(t-L\\). The demand rate \\(D(t)\\) would be a function of customer orders.\nFor a more detailed model, we can consider different stages of inventory, such as raw materials, work-in-process (WIP), and finished goods. Let \\(I_{raw}(t)\\), \\(I_{wip}(t)\\), and \\(I_{fg}(t)\\) be the inventory levels for raw materials, WIP, and finished goods, respectively.\nLet: * \\(P_{raw}(t)\\) = Procurement rate of raw materials at time \\(t\\) * \\(C_{raw}(t)\\) = Consumption rate of raw materials for production at time \\(t\\) * \\(P_{wip}(t)\\) = Production rate into WIP at time \\(t\\) * \\(C_{wip}(t)\\) = Consumption rate from WIP for finished goods production at time \\(t\\) * \\(P_{fg}(t)\\) = Production rate of finished goods at time \\(t\\) * \\(D_{fg}(t)\\) = Demand rate for finished goods at time \\(t\\)\nThen, the system can be described by a set of coupled differential equations:\n\\[ \\frac{dI_{raw}(t)}{dt} = P_{raw}(t) - C_{raw}(t) \\]\n\\[ \\frac{dI_{wip}(t)}{dt} = P_{wip}(t) - C_{wip}(t) \\]\n\\[ \\frac{dI_{fg}(t)}{dt} = P_{fg}(t) - D_{fg}(t) \\]\nThese equations highlight the interconnectedness of different inventory stages and the continuous flow of materials through the supply chain. The “asymmetry” discussed earlier can be addressed by ensuring that the measurement and forecasting of \\(P(t)\\) and \\(D(t)\\) are consistent and account for the variability in both order frequency and quantity.\nThe “ambiguity of optimal inventory” can be resolved by considering the optimal levels for each stage of inventory (\\(I_{raw}\\), \\(I_{wip}\\), \\(I_{fg}\\)) and their combined effect on the overall system performance, rather than focusing solely on physical stock. The goal is to optimize the flow and minimize bottlenecks, ensuring that inventory is available where and when needed, without excessive holding costs.\nBy adopting an inventory dynamic perspective, businesses can gain a more accurate and holistic understanding of their inventory dynamics. This enables them to:\n\nImprove Responsiveness: By understanding real-time flow, businesses can react more quickly to demand changes and disruptions.\nReduce Variability: A symmetrical view of input and output helps in managing and reducing the inherent variability in inventory levels.\nOptimize Resource Utilization: By accurately tracking inventory in all states, businesses can avoid overstocking or understocking, leading to better utilization of capital and space.\nEnhance Decision-Making: With a clearer picture of inventory flow, decision-makers can make more informed choices regarding procurement, production, and distribution.\n\n\n\n\nThe challenges posed by classical inventory theory—namely, asymmetry and the ambiguity of optimal inventory—underscore the urgent need for a more dynamic approach. The Inventory Dynamic Model offers a promising solution by shifting the focus from static stock levels to the continuous flow of inventory throughout the supply chain. This aligns perfectly with the principles of Dynamic Production Management and the broader trend of Digital Transformation.\nIn future posts, we will delve deeper into the practical applications and advanced concepts of inventory flow management, exploring how businesses can implement these theories to achieve greater efficiency, responsiveness, and resilience in their operations."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div2_inventory.html#bridging-the-gap-from-static-inventory-to-dynamic-flow",
    "href": "posts/DynamicInventoryManagement/div2_inventory.html#bridging-the-gap-from-static-inventory-to-dynamic-flow",
    "title": "Dynamic Inventory Theories: Unpacking Inventory Management Challenges",
    "section": "",
    "text": "In our previous post, we introduced Dynamic Production Management (DPM) as a paradigm shift from traditional production planning to a demand-centric approach. Building upon that foundation, this post delves deeper into the intricacies of inventory management, specifically addressing why conventional methods often fall short in today’s dynamic business environment. We will explore the challenges posed by classical inventory theory and introduce the concept of “inventory flow management” as a more adaptive solution. Furthermore, we will formulate a basic Inventory Dynamic Model with relevant equations.\n\n\nSince the mid-19th to mid-20th century, characterized by mass production and a seller’s market, the primary focus was on efficiently increasing production volume and ensuring a smooth supply of materials. It was within this context that classical inventory management mechanisms were shaped. The fundamental principle was simple: reorder when stock levels drop to a certain point, much like how we manage everyday household items. This is the essence of the reorder point system.\nHowever, this approach quickly encounters problems with materials that have long lead times (e.g., 3-6 months or more). Applying a reorder point system to such items can lead to excessive inventory or, conversely, frequent stockouts. To address this, methods like periodic replenishment were devised, where materials are ordered periodically based on future needs. While ideally linked to production plans, the unreliability of long-term production forecasts often meant that material procurement had to rely on independent forecasts.\nClassical inventory theory, born from the management of factory material warehouses, is still prevalent today. However, the question arises: can this classical theory be applied to the diverse forms of inventory that exist now, including finished goods warehouses, regional distribution centers, wholesalers, retailers, in-process inventory within factories, and even materials at external suppliers? The conclusion, as presented by tocken.com, is a resounding “No.” The core issues lie in two critical aspects: asymmetry and the ambiguity of optimal inventory.\n\n\n\nConsider a typical inventory management scenario where entity A manages its inventory, fulfilling customer orders and replenishing stock by ordering from entity B. Both the ordering process from A to B and the customer order process to A involve recording order times and quantities. This commercial transaction data is meticulously managed. However, when this data is brought into the inventory management framework, a critical asymmetry emerges.\nInventory management fundamentally involves adjusting “input” (replenishment) to match “output” (demand). Therefore, both input and output must be measured with the same yardstick. Demand, being largely uncontrollable, must be accepted as it is, making it crucial to understand its magnitude, including its variability. Yet, classical inventory theory often overlooks this crucial aspect.\nFor instance, when forecasting demand for inventory management, say for a two-month lead time, do we separately forecast the number of orders and the quantity per order? Classical inventory theory typically does not. It often provides a single demand forecast (e.g., XXX units for two months) without breaking down the number of orders or the quantity per order. While we meticulously consider order intervals and quantities when placing orders, our view of demand tends to be broad and undifferentiated. This disparity in how “output” and “input” are perceived—using different lenses for logistics—is what constitutes asymmetry.\nThis asymmetry has significant implications. Consider two scenarios where the average demand over a period is 200 units: one with an average of 40 orders of 5 units each, and another with 10 orders of 20 units each. While the average total demand is the same, the variability will differ significantly. Intuitively, receiving many small orders is preferable to a few large ones for minimizing inventory. Therefore, to accurately capture demand variability, it is essential to consider the number of orders and the quantity per order separately.\nSupply chains are interconnected networks of inventory management units. If the “output” and “input” are viewed from different perspectives, or if there is a lack of symmetry between them, the connections between these inventory management units become disjointed. From a physics perspective, symmetry is crucial for universality. Just as physical laws exhibit symmetry, suggesting their universal applicability, a lack of symmetry in inventory replenishment mechanisms implies a narrow scope for classical inventory theory.\n\n\n\nLet’s trace the flow of inventory through time: order received → shipment → inventory reduction → waiting for order → order placed → outstanding order → delivery → warehousing. Each event triggers a change in inventory status, and the quantity in each status fluctuates. It’s important to note that beyond physical stock in the warehouse, “inventory in transit” (waiting for order, outstanding order) often exists and can sometimes exceed physical stock. The quantity in each state increases or decreases with every event.\nHowever, classical inventory theory struggles to define precisely what constitutes “optimal inventory.” While some explanations might include outstanding orders in the definition of optimal inventory, most often link it solely to physical stock. Regardless, the quantity in each state fluctuates, and even when combined, these quantities exhibit variability. In inventory management, “optimal inventory” is a crucial management criterion. Yet, in practice, it remains ambiguous which inventory state should be used for comparison.\n\n\n\nThe recent surge in Digital Transformation (DX) initiatives highlights a growing concern: the increasing divergence between classical inventory theory and modern business realities. The need for DX in inventory management is urgent. The problems of “asymmetry” and “ambiguity of optimal inventory” inherent in classical theory are becoming more pronounced. The traditional approach, which assumes a stable environment and focuses on optimizing static inventory levels, is ill-equipped to handle the dynamic, interconnected, and data-rich landscape of today’s supply chains.\nDX in inventory management is not merely about digitizing existing processes; it’s about fundamentally transforming how inventory is perceived and managed. It necessitates a shift towards real-time data utilization, predictive analytics, and agile decision-making. The goal is to move beyond simply counting stock to understanding the flow and behaviour of inventory across the entire supply chain, enabling proactive adjustments rather than reactive responses.\n\n\n\nTo overcome the limitations of classical inventory theory introduces the concept of an Inventory Dynamic Model. This model emphasizes understanding inventory not as static quantities but as dynamic flows within a system. It recognizes that inventory is constantly in motion, transitioning through various states from raw materials to finished goods, and that these flows are influenced by both internal processes and external demand.\nWe can conceptualize the inventory dynamic model using a set of differential equations that describe the change in inventory levels over time. Let \\(I(t)\\) be the inventory level at time \\(t\\). The rate of change of inventory can be expressed as the difference between the inflow (replenishment rate) and the outflow (demand rate).\nLet: * \\(R(t)\\) = Replenishment rate (inflow) at time \\(t\\) * \\(D(t)\\) = Demand rate (outflow) at time \\(t\\)\nThe fundamental equation for the inventory dynamic model is:\n\\[ \\frac{dI(t)}{dt} = R(t) - D(t) \\]\nThis equation states that the rate of change of inventory over time is equal to the replenishment rate minus the demand rate. This is a continuous model, representing inventory as a dynamic that flows in and out of a system.\nTo further elaborate, we can consider the components of replenishment and demand. Replenishment can be influenced by factors such as lead time (\\(L\\)), which is the time delay between placing an order and receiving it. Demand can be influenced by various market factors and can be stochastic.\nConsider a simplified scenario where replenishment orders are placed based on a reorder point (\\(S\\)) and a fixed order quantity (\\(Q\\)). The replenishment rate \\(R(t)\\) would then be a function of the orders placed at an earlier time \\(t-L\\). The demand rate \\(D(t)\\) would be a function of customer orders.\nFor a more detailed model, we can consider different stages of inventory, such as raw materials, work-in-process (WIP), and finished goods. Let \\(I_{raw}(t)\\), \\(I_{wip}(t)\\), and \\(I_{fg}(t)\\) be the inventory levels for raw materials, WIP, and finished goods, respectively.\nLet: * \\(P_{raw}(t)\\) = Procurement rate of raw materials at time \\(t\\) * \\(C_{raw}(t)\\) = Consumption rate of raw materials for production at time \\(t\\) * \\(P_{wip}(t)\\) = Production rate into WIP at time \\(t\\) * \\(C_{wip}(t)\\) = Consumption rate from WIP for finished goods production at time \\(t\\) * \\(P_{fg}(t)\\) = Production rate of finished goods at time \\(t\\) * \\(D_{fg}(t)\\) = Demand rate for finished goods at time \\(t\\)\nThen, the system can be described by a set of coupled differential equations:\n\\[ \\frac{dI_{raw}(t)}{dt} = P_{raw}(t) - C_{raw}(t) \\]\n\\[ \\frac{dI_{wip}(t)}{dt} = P_{wip}(t) - C_{wip}(t) \\]\n\\[ \\frac{dI_{fg}(t)}{dt} = P_{fg}(t) - D_{fg}(t) \\]\nThese equations highlight the interconnectedness of different inventory stages and the continuous flow of materials through the supply chain. The “asymmetry” discussed earlier can be addressed by ensuring that the measurement and forecasting of \\(P(t)\\) and \\(D(t)\\) are consistent and account for the variability in both order frequency and quantity.\nThe “ambiguity of optimal inventory” can be resolved by considering the optimal levels for each stage of inventory (\\(I_{raw}\\), \\(I_{wip}\\), \\(I_{fg}\\)) and their combined effect on the overall system performance, rather than focusing solely on physical stock. The goal is to optimize the flow and minimize bottlenecks, ensuring that inventory is available where and when needed, without excessive holding costs.\nBy adopting an inventory dynamic perspective, businesses can gain a more accurate and holistic understanding of their inventory dynamics. This enables them to:\n\nImprove Responsiveness: By understanding real-time flow, businesses can react more quickly to demand changes and disruptions.\nReduce Variability: A symmetrical view of input and output helps in managing and reducing the inherent variability in inventory levels.\nOptimize Resource Utilization: By accurately tracking inventory in all states, businesses can avoid overstocking or understocking, leading to better utilization of capital and space.\nEnhance Decision-Making: With a clearer picture of inventory flow, decision-makers can make more informed choices regarding procurement, production, and distribution.\n\n\n\n\nThe challenges posed by classical inventory theory—namely, asymmetry and the ambiguity of optimal inventory—underscore the urgent need for a more dynamic approach. The Inventory Dynamic Model offers a promising solution by shifting the focus from static stock levels to the continuous flow of inventory throughout the supply chain. This aligns perfectly with the principles of Dynamic Production Management and the broader trend of Digital Transformation.\nIn future posts, we will delve deeper into the practical applications and advanced concepts of inventory flow management, exploring how businesses can implement these theories to achieve greater efficiency, responsiveness, and resilience in their operations."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div5_makeToOrder.html",
    "href": "posts/DynamicInventoryManagement/div5_makeToOrder.html",
    "title": "Dynamic Make-to-Order Production: Mastering the Flow of Time",
    "section": "",
    "text": "In our series on dynamic production management, we have explored the distinctions between Make-to-Order (MTO) and Make-to-Stock (MTS) production, and the challenges associated with traditional production planning. This post delves deeper into Make-to-Order (MTO) production, focusing on the critical aspect of time management and how a dynamic approach can overcome the inherent problems.\n\n\nWhile MTO production, where products are made only after a customer order is confirmed, might seem ideal (no unsold inventory, no complaints about stockouts), it presents its own set of significant challenges. A common issue is the ambiguity surrounding the definition of a “confirmed order.” This can lead to “premature starts” where production begins based on tentative orders, unconfirmed specifications, or unclear delivery dates. These premature starts are often driven by the pressure to meet tight deadlines, as delaying production until all details are finalized could mean missing the customer’s requested delivery date.\nThe low on-time delivery rate is a persistent problem in MTO production. Is this due to poor time management? To understand why time management is so difficult in MTO, we need to examine the underlying issues.\n\n\nSeveral time elements are at play in MTO production:\n\nCustomer’s Desired Delivery Date: The date by which the customer expects the product to be completed and delivered.\nProduction Lead Time: The estimated time required to produce the product. Subtracting this from the desired delivery date gives the planned production start (input) date.\nOrder Confirmation Time: The actual time when the order is formally confirmed. Ideally, this should be before the planned production start date, but it often occurs later.\n\nThe most critical of these for time management is the production lead time. A common problem is that the actual production lead time often exceeds the estimated lead time. Why does this happen? One major reason is the tendency to set extremely tight lead times to meet customer demands, especially in competitive environments.\nThe production lead time consists primarily of flow time (the time from input to completion) and administrative processing time before input. Flow time, in turn, is composed of processing time and waiting time. Waiting time dramatically increases when the load factor (utilization rate) of the production line approaches around 80%. Flow time exhibits a similar curve, and this phenomenon is referred to as the “flow time jump.” The more significant the variability, the steeper this jump becomes. Figure 1 illustrates this relationship:\n\n\n\nFigure 1: Change in Waiting Time vs. Load Factor\n\n\nFigure 1: Change in Waiting Time vs. Load Factor\nWhile a high utilization rate is generally considered desirable (e.g., 90% or even 95%), operating in this high-utilization zone leads to a sharp increase in flow time, making time management extremely difficult. This is a primary factor contributing to poor time and delivery management in MTO production.\n\n\n\n\nBefore discussing time management, it’s essential to address the flow time jump. Without controlling this phenomenon, effective time management is impossible. Two main methods can suppress the flow time jump:\n\nWork-In-Process (WIP) Limit: Implementing an input restriction based on WIP is highly effective. As shown in Figure 2, reducing the WIP limit (e.g., from 50 to 20) significantly flattens the flow time jump curve.\nIncreasing Production Capacity: Raising production capacity to keep the load factor below 80% also helps. In practice, a combination of these two methods is often employed.\n\n\n\n\nFigure 2: Flow Time Jump and WIP Limit\n\n\nFigure 2: Flow Time Jump and WIP Limit\n\n\n\nWith methods to control the flow time jump in place, we can now return to time management. Traditional MTO production relies on production plans, schedules, and detailed timelines as benchmarks. However, as we’ve repeatedly seen, these are often unable to keep pace with demand fluctuations. So, what should be our benchmark?\nThe production lead time itself can serve as a benchmark, which we will call Standard Manufacturing Lead-time (SML). This concept is analogous to standard work time or standard man-hours.\nFor SML to function effectively, its definition must be precise. The longest component of SML is typically the flow time. This is why controlling the flow time jump is so crucial. A key consideration is determining the maximum acceptable flow time. Flow time comprises processing time (for operations like machining and assembly) and waiting time (for machine availability, material arrival, or human-induced delays like batch processing or consolidation). Figure 3 shows an example of a Time In Process (TIP) chart, illustrating the flow time progression from production start to completion for a product group.\nCompletion times in MTO production vary and are generally approximated by a normal distribution, characterized by its mean and variance (standard deviation). Since MTO orders often have unique specifications, estimating SML for each individual order is ideal. However, for products with minor specification differences but largely repetitive production, grouping them by specification and setting SMLs for each group can be effective. The upper limit of flow time depends on the desired completion probability and is closely related to the degree of input restriction.\nIt’s also important to account for potential delays in input (start). Various factors, such as input restrictions to control flow time jump, material arrival delays, machine setup, or partial specification changes, can delay the planned production start. SML should incorporate an allowance for these anticipated delays.\nThus, SML can be expressed as:\n\\[SML = \\text{Flow Time Upper Limit} + \\text{Allowable Input Delay}\\]\n\n\n\nIn a dynamic MTO system, the SML becomes the primary control parameter. Instead of rigid schedules, the system focuses on managing the flow of orders to ensure they adhere to their SML. This involves:\n\nReal-time Progress Monitoring: Continuously tracking the progress of each order against its SML. Deviations trigger alerts, allowing for timely intervention.\nPriority Control: When multiple orders compete for resources, a dynamic priority control mechanism is needed. The website suggests using the ratio of elapsed time from input to the standard flow time as a priority criterion. Orders that are falling behind their SML would automatically gain higher priority.\n\nLet \\(T_{elapsed}\\) be the elapsed time since input and \\(SML_{order}\\) be the SML for a specific order. The priority \\(P\\) could be defined as:\n\\[P = \\frac{T_{elapsed}}{SML_{order}}\\]\nOrders with a higher \\(P\\) value would be prioritized. This allows for autonomous priority control on the shop floor.\n\n\n\nEffective input management is crucial. Orders should only be input into the production system if they have a reasonable chance of meeting their SML. This requires a clear understanding of current capacity and WIP levels. Progress tracking should be granular, allowing for visibility into each order’s status at every stage of production.\n\n\n\nEstablishing clear rules for dynamic MTO is essential. These rules govern how orders are prioritized, how resources are allocated, and how deviations are handled. Regular maintenance of these rules and the underlying system is necessary to ensure continued effectiveness.\n\n\n\nA robust information processing mechanism is the backbone of dynamic MTO. This system needs to:\n\nCollect Real-time Data: Gather data on order status, WIP levels, machine availability, and material status.\nCalculate SML and Priorities: Automatically compute SMLs for new orders and update priorities for ongoing orders.\nProvide Alerts and Insights: Generate alerts for potential delays or bottlenecks and provide insights into system performance.\n\n\n\n\nFinally, the performance of the dynamic MTO system must be continuously evaluated. Key metrics would include:\n\nOn-time Delivery Rate: The percentage of orders delivered by their promised date.\nActual vs. SML Deviation: The difference between actual production lead time and SML.\nFlow Time Variability: The consistency of flow times.\nResource Utilization: While not the sole focus, efficient resource utilization remains important.\n\n\n\n\nDynamic Make-to-Order production shifts the focus from rigid, static planning to the dynamic management of time and flow. By understanding and controlling the factors that influence flow time, setting realistic Standard Manufacturing Lead Times, and implementing intelligent priority and input control mechanisms, businesses can significantly improve their on-time delivery performance and overall responsiveness. This approach, rooted in real-time data and autonomous decision-making, is vital for navigating the complexities of modern MTO environments."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div5_makeToOrder.html#managing-the-flow-of-time-in-make-to-order-production",
    "href": "posts/DynamicInventoryManagement/div5_makeToOrder.html#managing-the-flow-of-time-in-make-to-order-production",
    "title": "Dynamic Make-to-Order Production: Mastering the Flow of Time",
    "section": "",
    "text": "In our series on dynamic production management, we have explored the distinctions between Make-to-Order (MTO) and Make-to-Stock (MTS) production, and the challenges associated with traditional production planning. This post delves deeper into Make-to-Order (MTO) production, focusing on the critical aspect of time management and how a dynamic approach can overcome the inherent problems.\n\n\nWhile MTO production, where products are made only after a customer order is confirmed, might seem ideal (no unsold inventory, no complaints about stockouts), it presents its own set of significant challenges. A common issue is the ambiguity surrounding the definition of a “confirmed order.” This can lead to “premature starts” where production begins based on tentative orders, unconfirmed specifications, or unclear delivery dates. These premature starts are often driven by the pressure to meet tight deadlines, as delaying production until all details are finalized could mean missing the customer’s requested delivery date.\nThe low on-time delivery rate is a persistent problem in MTO production. Is this due to poor time management? To understand why time management is so difficult in MTO, we need to examine the underlying issues.\n\n\nSeveral time elements are at play in MTO production:\n\nCustomer’s Desired Delivery Date: The date by which the customer expects the product to be completed and delivered.\nProduction Lead Time: The estimated time required to produce the product. Subtracting this from the desired delivery date gives the planned production start (input) date.\nOrder Confirmation Time: The actual time when the order is formally confirmed. Ideally, this should be before the planned production start date, but it often occurs later.\n\nThe most critical of these for time management is the production lead time. A common problem is that the actual production lead time often exceeds the estimated lead time. Why does this happen? One major reason is the tendency to set extremely tight lead times to meet customer demands, especially in competitive environments.\nThe production lead time consists primarily of flow time (the time from input to completion) and administrative processing time before input. Flow time, in turn, is composed of processing time and waiting time. Waiting time dramatically increases when the load factor (utilization rate) of the production line approaches around 80%. Flow time exhibits a similar curve, and this phenomenon is referred to as the “flow time jump.” The more significant the variability, the steeper this jump becomes. Figure 1 illustrates this relationship:\n\n\n\nFigure 1: Change in Waiting Time vs. Load Factor\n\n\nFigure 1: Change in Waiting Time vs. Load Factor\nWhile a high utilization rate is generally considered desirable (e.g., 90% or even 95%), operating in this high-utilization zone leads to a sharp increase in flow time, making time management extremely difficult. This is a primary factor contributing to poor time and delivery management in MTO production.\n\n\n\n\nBefore discussing time management, it’s essential to address the flow time jump. Without controlling this phenomenon, effective time management is impossible. Two main methods can suppress the flow time jump:\n\nWork-In-Process (WIP) Limit: Implementing an input restriction based on WIP is highly effective. As shown in Figure 2, reducing the WIP limit (e.g., from 50 to 20) significantly flattens the flow time jump curve.\nIncreasing Production Capacity: Raising production capacity to keep the load factor below 80% also helps. In practice, a combination of these two methods is often employed.\n\n\n\n\nFigure 2: Flow Time Jump and WIP Limit\n\n\nFigure 2: Flow Time Jump and WIP Limit\n\n\n\nWith methods to control the flow time jump in place, we can now return to time management. Traditional MTO production relies on production plans, schedules, and detailed timelines as benchmarks. However, as we’ve repeatedly seen, these are often unable to keep pace with demand fluctuations. So, what should be our benchmark?\nThe production lead time itself can serve as a benchmark, which we will call Standard Manufacturing Lead-time (SML). This concept is analogous to standard work time or standard man-hours.\nFor SML to function effectively, its definition must be precise. The longest component of SML is typically the flow time. This is why controlling the flow time jump is so crucial. A key consideration is determining the maximum acceptable flow time. Flow time comprises processing time (for operations like machining and assembly) and waiting time (for machine availability, material arrival, or human-induced delays like batch processing or consolidation). Figure 3 shows an example of a Time In Process (TIP) chart, illustrating the flow time progression from production start to completion for a product group.\nCompletion times in MTO production vary and are generally approximated by a normal distribution, characterized by its mean and variance (standard deviation). Since MTO orders often have unique specifications, estimating SML for each individual order is ideal. However, for products with minor specification differences but largely repetitive production, grouping them by specification and setting SMLs for each group can be effective. The upper limit of flow time depends on the desired completion probability and is closely related to the degree of input restriction.\nIt’s also important to account for potential delays in input (start). Various factors, such as input restrictions to control flow time jump, material arrival delays, machine setup, or partial specification changes, can delay the planned production start. SML should incorporate an allowance for these anticipated delays.\nThus, SML can be expressed as:\n\\[SML = \\text{Flow Time Upper Limit} + \\text{Allowable Input Delay}\\]\n\n\n\nIn a dynamic MTO system, the SML becomes the primary control parameter. Instead of rigid schedules, the system focuses on managing the flow of orders to ensure they adhere to their SML. This involves:\n\nReal-time Progress Monitoring: Continuously tracking the progress of each order against its SML. Deviations trigger alerts, allowing for timely intervention.\nPriority Control: When multiple orders compete for resources, a dynamic priority control mechanism is needed. The website suggests using the ratio of elapsed time from input to the standard flow time as a priority criterion. Orders that are falling behind their SML would automatically gain higher priority.\n\nLet \\(T_{elapsed}\\) be the elapsed time since input and \\(SML_{order}\\) be the SML for a specific order. The priority \\(P\\) could be defined as:\n\\[P = \\frac{T_{elapsed}}{SML_{order}}\\]\nOrders with a higher \\(P\\) value would be prioritized. This allows for autonomous priority control on the shop floor.\n\n\n\nEffective input management is crucial. Orders should only be input into the production system if they have a reasonable chance of meeting their SML. This requires a clear understanding of current capacity and WIP levels. Progress tracking should be granular, allowing for visibility into each order’s status at every stage of production.\n\n\n\nEstablishing clear rules for dynamic MTO is essential. These rules govern how orders are prioritized, how resources are allocated, and how deviations are handled. Regular maintenance of these rules and the underlying system is necessary to ensure continued effectiveness.\n\n\n\nA robust information processing mechanism is the backbone of dynamic MTO. This system needs to:\n\nCollect Real-time Data: Gather data on order status, WIP levels, machine availability, and material status.\nCalculate SML and Priorities: Automatically compute SMLs for new orders and update priorities for ongoing orders.\nProvide Alerts and Insights: Generate alerts for potential delays or bottlenecks and provide insights into system performance.\n\n\n\n\nFinally, the performance of the dynamic MTO system must be continuously evaluated. Key metrics would include:\n\nOn-time Delivery Rate: The percentage of orders delivered by their promised date.\nActual vs. SML Deviation: The difference between actual production lead time and SML.\nFlow Time Variability: The consistency of flow times.\nResource Utilization: While not the sole focus, efficient resource utilization remains important.\n\n\n\n\nDynamic Make-to-Order production shifts the focus from rigid, static planning to the dynamic management of time and flow. By understanding and controlling the factors that influence flow time, setting realistic Standard Manufacturing Lead Times, and implementing intelligent priority and input control mechanisms, businesses can significantly improve their on-time delivery performance and overall responsiveness. This approach, rooted in real-time data and autonomous decision-making, is vital for navigating the complexities of modern MTO environments."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div4_makeToStock.html",
    "href": "posts/DynamicInventoryManagement/div4_makeToStock.html",
    "title": "Dynamic Make-to-Stock Production: Producing Only What Sells",
    "section": "",
    "text": "In our ongoing exploration of dynamic inventory and production theories, we now turn our attention to Make-to-Stock (MTS) production, often referred to as “mikomi seisan” in Japanese. This approach stands in contrast to Make-to-Order (MTO) production, and a clear understanding of their distinctions is crucial for effective production management. This post will delve into the nuances of MTS, its objectives, and how a dynamic approach can optimize its effectiveness.\n\n\nThe core difference between MTO and MTS lies in the timing of production initiation relative to order confirmation. In Make-to-Order, production begins after a customer order is confirmed. Conversely, in Make-to-Stock, production commences before an order is received, based on anticipated demand. From a production management perspective, this distinction is paramount because it dictates fundamentally different management mechanisms.\nWhile some might argue that their factories handle both MTO and MTS with a unified management system, this often leads to ambiguity and inefficiencies. The key is to precisely define “production start” and “order confirmation.”\n\nProduction Start: This refers to the initiation of activities to complete a specific product. It typically excludes long-term activities like factory construction or equipment installation, focusing instead on repetitive production cycles. It can range from starting design, procuring parts, or beginning assembly.\nOrder Confirmation: This is when all critical aspects of a customer order are finalized. Ideally, this includes:\n\nThe buyer (payer) is determined.\nProduct specifications are set.\nDelivery timing (when needed) is established.\nQuantity to be purchased is fixed.\nPrice is agreed upon.\n\n\nWhile some flexibility in these criteria might be acceptable in commercial transactions, a strict definition is essential for production management. The ambiguity often arises when companies, despite claiming to be MTO, initiate production activities based on sales forecasts rather than confirmed orders. This leads to a blend of confirmed and forecasted orders within the production plan.\nFor instance, in a factory operating on a monthly production cycle, a confirmed MTO order might only be incorporated into the production plan in the following month, leading to significant delays if lead times are short. To circumvent this, many “MTO” factories resort to informal workarounds, such as procuring materials or even starting initial production steps for unconfirmed orders, blurring the lines between MTO and MTS. This informal “make-to-forecast” within an MTO framework is a primary culprit for the ambiguity.\nIn Dynamic Production Management (DPM), this ambiguity is eliminated. MTO strictly means production begins after order confirmation, and MTS means production begins before order confirmation. This principle is maintained without compromise. It’s also important to note that distinctions like “MTO is multi-product, small-lot; MTS is small-product, large-lot” are tendencies, not fundamental defining characteristics.\n\n\n\nThe definition of MTS is now clear: producing before knowing who, when, or how much will be bought. This inherently carries risks of both stockouts and excess inventory, which are undesirable from a business management perspective. So, why engage in MTS production?\nThe primary objective of MTS is not merely to achieve economies of scale through planned, leveled production, as is often assumed. While mass production can reduce costs, this benefit is often negated by the risks of obsolescence and overstocking if demand is misjudged. The true purpose of MTS, from a dynamic perspective, is to produce what sells – to have the necessary products available when and where the customer needs them, before they place an order.\nIf production only begins after an order is confirmed, the customer might have already moved on, resulting in a lost sale. Therefore, the ultimate goal of MTS is to proactively meet customer demand. This leads us to a crucial insight: production must follow market demand. The market (customer) determines what, when, and how much is needed. Thus, the direction is clear: produce according to market demand.\n\n\n\nTo achieve demand-following MTS, a dynamic system is required that can adapt to real-time market signals. This involves a shift from rigid, forecast-driven planning to a more agile, responsive approach. The website highlights the importance of a system that can quickly react to changes in demand, rather than being constrained by long planning cycles.\nOne of the key concepts introduced is the idea of a “Dynamic inventory” or “inventory flow management” system, which we discussed in a previous post. In the context of MTS, this means managing inventory not as static quantities, but as dynamic flows that respond to actual consumption. The system needs to be able to:\n\nMonitor Real-time Demand: Continuously track sales and consumption data to understand actual market off-take.\nAdjust Production Rates Dynamically: Based on real-time demand, adjust production output to match consumption, preventing both stockouts and overproduction.\nMaintain Optimal Inventory Levels: Instead of fixed “safety stocks,” the system aims to maintain a dynamic inventory level that minimizes holding costs while ensuring product availability.\n\nThis dynamic adjustment is crucial. If demand increases, the system should automatically trigger an increase in production. If demand drops, production should be scaled back. This responsiveness is what differentiates dynamic MTS from traditional, forecast-driven MTS.\n\n\n\nIn a dynamic MTS system, the production line is directly linked to inventory replenishment. Instead of producing to a fixed schedule and then pushing products into a warehouse, the production line pulls demand from the inventory. This means that as products are sold and inventory levels drop, a signal is sent back to the production line to replenish those specific items. This is a pull-based system, in contrast to a push-based system.\nConsider a simplified model of this replenishment process:\n\n\n\n\n\ngraph TD\n    A[Customer Demand] --&gt; B{Inventory Level}\n    B -- Below Threshold --&gt; C[Production Trigger]\n    C --&gt; D[Production Line]\n    D --&gt; E[Finished Goods Inventory]\n    E --&gt; B\n\n\n\n\n\n\nThis diagram illustrates a basic feedback loop. When customer demand reduces the inventory level below a certain threshold, it triggers the production line to replenish the stock. This continuous feedback ensures that production is always aligned with actual consumption.\n\n\n\nDynamic MTS also requires flexible production capacity and intelligent input restrictions. Production capacity needs to be adaptable to fluctuations in demand. This might involve:\n\nFlexible Workforce: Cross-training employees or utilizing temporary staff to adjust labor capacity.\nModular Production Lines: Designing production lines that can be easily reconfigured or scaled up/down.\nStrategic Outsourcing: Leveraging external partners for overflow production during peak demand.\n\nFurthermore, input restrictions are critical to prevent overproduction and excessive Work-In-Process (WIP). This is where the concept of a “WIP cap” or “input limit” becomes important. By setting a maximum limit on the amount of WIP in the system, it prevents the production line from producing more than what the downstream processes or customer demand can absorb. This helps to maintain a smooth flow and prevent bottlenecks.\n\n\n\nWhen customer lead time (the time between order placement and delivery) is a factor, the dynamic MTS system needs to account for it. If the lead time is significant, the system must anticipate future demand further in advance. This can be incorporated into the replenishment trigger, where the threshold for triggering production is adjusted based on the lead time. For example, if the lead time is ‘L’ days, the system might trigger production when the inventory level is sufficient to cover ‘L’ days of anticipated demand.\n\n\n\nThe website introduces the concept of an S-Unit as a core component of dynamic make-to-stock production. While the details are not fully elaborated on the page, the S-Unit appears to be a standardized unit or module that facilitates flexible and responsive production. It likely represents a smallest common denominator in terms of production or inventory, allowing for granular control and rapid adjustments.\nWithout explicit equations on the website, we can infer some fundamental relationships that govern dynamic MTS. Let’s consider the basic inventory balance equation:\n\\[I_{t} = I_{t-1} + P_{t} - D_{t}\\]\nWhere: * \\(I_{t}\\) = Inventory at the end of period \\(t\\) * \\(I_{t-1}\\) = Inventory at the end of period \\(t-1\\) * \\(P_{t}\\) = Production in period \\(t\\) * \\(D_{t}\\) = Demand in period \\(t\\)\nIn a dynamic MTS system, the goal is to keep \\(I_t\\) within a desired range. This means \\(P_t\\) needs to be responsive to \\(D_t\\). A simple control rule could be:\n\\[P_{t} = D_{t} + k(I_{target} - I_{t-1})\\]\nWhere: * \\(I_{target}\\) = Desired target inventory level * \\(k\\) = A control parameter (e.g., a fraction of the inventory deviation to correct in the next period)\nThis equation suggests that production in period \\(t\\) is driven by current demand (\\(D_t\\)) and an adjustment based on the deviation of the previous period’s inventory (\\(I_{t-1}\\)) from the target inventory (\\(I_{target}\\)). The parameter \\(k\\) would determine how aggressively the system tries to correct inventory deviations. A higher \\(k\\) would lead to faster adjustments but could also introduce instability.\nFor the WIP cap, we can define:\n\\[WIP_{t} \\le WIP_{max}\\]\nWhere: * \\(WIP_{t}\\) = Work-in-process at the end of period \\(t\\) * \\(WIP_{max}\\) = Maximum allowable work-in-process\nThis constraint would directly influence the production trigger, ensuring that new production is only initiated if the WIP level is below the maximum. This helps to prevent bottlenecks and maintain flow.\n\n\n\nDynamic Make-to-Stock production represents a significant evolution from traditional MTS, moving away from rigid, forecast-driven planning towards a highly responsive, demand-following system. By embracing real-time data, flexible capacity, and intelligent input restrictions, businesses can achieve a more agile and efficient production process that minimizes both stockouts and excess inventory. The underlying principle is to produce only what sells, driven by actual market demand rather than speculative forecasts.\nThis approach aligns with the broader principles of Dynamic Production Management and the ongoing digital transformation in manufacturing. In the next post, we will explore further practical applications and advanced concepts related to these dynamic systems."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div4_makeToStock.html#understanding-make-to-stock-production-a-dynamic-perspective",
    "href": "posts/DynamicInventoryManagement/div4_makeToStock.html#understanding-make-to-stock-production-a-dynamic-perspective",
    "title": "Dynamic Make-to-Stock Production: Producing Only What Sells",
    "section": "",
    "text": "In our ongoing exploration of dynamic inventory and production theories, we now turn our attention to Make-to-Stock (MTS) production, often referred to as “mikomi seisan” in Japanese. This approach stands in contrast to Make-to-Order (MTO) production, and a clear understanding of their distinctions is crucial for effective production management. This post will delve into the nuances of MTS, its objectives, and how a dynamic approach can optimize its effectiveness.\n\n\nThe core difference between MTO and MTS lies in the timing of production initiation relative to order confirmation. In Make-to-Order, production begins after a customer order is confirmed. Conversely, in Make-to-Stock, production commences before an order is received, based on anticipated demand. From a production management perspective, this distinction is paramount because it dictates fundamentally different management mechanisms.\nWhile some might argue that their factories handle both MTO and MTS with a unified management system, this often leads to ambiguity and inefficiencies. The key is to precisely define “production start” and “order confirmation.”\n\nProduction Start: This refers to the initiation of activities to complete a specific product. It typically excludes long-term activities like factory construction or equipment installation, focusing instead on repetitive production cycles. It can range from starting design, procuring parts, or beginning assembly.\nOrder Confirmation: This is when all critical aspects of a customer order are finalized. Ideally, this includes:\n\nThe buyer (payer) is determined.\nProduct specifications are set.\nDelivery timing (when needed) is established.\nQuantity to be purchased is fixed.\nPrice is agreed upon.\n\n\nWhile some flexibility in these criteria might be acceptable in commercial transactions, a strict definition is essential for production management. The ambiguity often arises when companies, despite claiming to be MTO, initiate production activities based on sales forecasts rather than confirmed orders. This leads to a blend of confirmed and forecasted orders within the production plan.\nFor instance, in a factory operating on a monthly production cycle, a confirmed MTO order might only be incorporated into the production plan in the following month, leading to significant delays if lead times are short. To circumvent this, many “MTO” factories resort to informal workarounds, such as procuring materials or even starting initial production steps for unconfirmed orders, blurring the lines between MTO and MTS. This informal “make-to-forecast” within an MTO framework is a primary culprit for the ambiguity.\nIn Dynamic Production Management (DPM), this ambiguity is eliminated. MTO strictly means production begins after order confirmation, and MTS means production begins before order confirmation. This principle is maintained without compromise. It’s also important to note that distinctions like “MTO is multi-product, small-lot; MTS is small-product, large-lot” are tendencies, not fundamental defining characteristics.\n\n\n\nThe definition of MTS is now clear: producing before knowing who, when, or how much will be bought. This inherently carries risks of both stockouts and excess inventory, which are undesirable from a business management perspective. So, why engage in MTS production?\nThe primary objective of MTS is not merely to achieve economies of scale through planned, leveled production, as is often assumed. While mass production can reduce costs, this benefit is often negated by the risks of obsolescence and overstocking if demand is misjudged. The true purpose of MTS, from a dynamic perspective, is to produce what sells – to have the necessary products available when and where the customer needs them, before they place an order.\nIf production only begins after an order is confirmed, the customer might have already moved on, resulting in a lost sale. Therefore, the ultimate goal of MTS is to proactively meet customer demand. This leads us to a crucial insight: production must follow market demand. The market (customer) determines what, when, and how much is needed. Thus, the direction is clear: produce according to market demand.\n\n\n\nTo achieve demand-following MTS, a dynamic system is required that can adapt to real-time market signals. This involves a shift from rigid, forecast-driven planning to a more agile, responsive approach. The website highlights the importance of a system that can quickly react to changes in demand, rather than being constrained by long planning cycles.\nOne of the key concepts introduced is the idea of a “Dynamic inventory” or “inventory flow management” system, which we discussed in a previous post. In the context of MTS, this means managing inventory not as static quantities, but as dynamic flows that respond to actual consumption. The system needs to be able to:\n\nMonitor Real-time Demand: Continuously track sales and consumption data to understand actual market off-take.\nAdjust Production Rates Dynamically: Based on real-time demand, adjust production output to match consumption, preventing both stockouts and overproduction.\nMaintain Optimal Inventory Levels: Instead of fixed “safety stocks,” the system aims to maintain a dynamic inventory level that minimizes holding costs while ensuring product availability.\n\nThis dynamic adjustment is crucial. If demand increases, the system should automatically trigger an increase in production. If demand drops, production should be scaled back. This responsiveness is what differentiates dynamic MTS from traditional, forecast-driven MTS.\n\n\n\nIn a dynamic MTS system, the production line is directly linked to inventory replenishment. Instead of producing to a fixed schedule and then pushing products into a warehouse, the production line pulls demand from the inventory. This means that as products are sold and inventory levels drop, a signal is sent back to the production line to replenish those specific items. This is a pull-based system, in contrast to a push-based system.\nConsider a simplified model of this replenishment process:\n\n\n\n\n\ngraph TD\n    A[Customer Demand] --&gt; B{Inventory Level}\n    B -- Below Threshold --&gt; C[Production Trigger]\n    C --&gt; D[Production Line]\n    D --&gt; E[Finished Goods Inventory]\n    E --&gt; B\n\n\n\n\n\n\nThis diagram illustrates a basic feedback loop. When customer demand reduces the inventory level below a certain threshold, it triggers the production line to replenish the stock. This continuous feedback ensures that production is always aligned with actual consumption.\n\n\n\nDynamic MTS also requires flexible production capacity and intelligent input restrictions. Production capacity needs to be adaptable to fluctuations in demand. This might involve:\n\nFlexible Workforce: Cross-training employees or utilizing temporary staff to adjust labor capacity.\nModular Production Lines: Designing production lines that can be easily reconfigured or scaled up/down.\nStrategic Outsourcing: Leveraging external partners for overflow production during peak demand.\n\nFurthermore, input restrictions are critical to prevent overproduction and excessive Work-In-Process (WIP). This is where the concept of a “WIP cap” or “input limit” becomes important. By setting a maximum limit on the amount of WIP in the system, it prevents the production line from producing more than what the downstream processes or customer demand can absorb. This helps to maintain a smooth flow and prevent bottlenecks.\n\n\n\nWhen customer lead time (the time between order placement and delivery) is a factor, the dynamic MTS system needs to account for it. If the lead time is significant, the system must anticipate future demand further in advance. This can be incorporated into the replenishment trigger, where the threshold for triggering production is adjusted based on the lead time. For example, if the lead time is ‘L’ days, the system might trigger production when the inventory level is sufficient to cover ‘L’ days of anticipated demand.\n\n\n\nThe website introduces the concept of an S-Unit as a core component of dynamic make-to-stock production. While the details are not fully elaborated on the page, the S-Unit appears to be a standardized unit or module that facilitates flexible and responsive production. It likely represents a smallest common denominator in terms of production or inventory, allowing for granular control and rapid adjustments.\nWithout explicit equations on the website, we can infer some fundamental relationships that govern dynamic MTS. Let’s consider the basic inventory balance equation:\n\\[I_{t} = I_{t-1} + P_{t} - D_{t}\\]\nWhere: * \\(I_{t}\\) = Inventory at the end of period \\(t\\) * \\(I_{t-1}\\) = Inventory at the end of period \\(t-1\\) * \\(P_{t}\\) = Production in period \\(t\\) * \\(D_{t}\\) = Demand in period \\(t\\)\nIn a dynamic MTS system, the goal is to keep \\(I_t\\) within a desired range. This means \\(P_t\\) needs to be responsive to \\(D_t\\). A simple control rule could be:\n\\[P_{t} = D_{t} + k(I_{target} - I_{t-1})\\]\nWhere: * \\(I_{target}\\) = Desired target inventory level * \\(k\\) = A control parameter (e.g., a fraction of the inventory deviation to correct in the next period)\nThis equation suggests that production in period \\(t\\) is driven by current demand (\\(D_t\\)) and an adjustment based on the deviation of the previous period’s inventory (\\(I_{t-1}\\)) from the target inventory (\\(I_{target}\\)). The parameter \\(k\\) would determine how aggressively the system tries to correct inventory deviations. A higher \\(k\\) would lead to faster adjustments but could also introduce instability.\nFor the WIP cap, we can define:\n\\[WIP_{t} \\le WIP_{max}\\]\nWhere: * \\(WIP_{t}\\) = Work-in-process at the end of period \\(t\\) * \\(WIP_{max}\\) = Maximum allowable work-in-process\nThis constraint would directly influence the production trigger, ensuring that new production is only initiated if the WIP level is below the maximum. This helps to prevent bottlenecks and maintain flow.\n\n\n\nDynamic Make-to-Stock production represents a significant evolution from traditional MTS, moving away from rigid, forecast-driven planning towards a highly responsive, demand-following system. By embracing real-time data, flexible capacity, and intelligent input restrictions, businesses can achieve a more agile and efficient production process that minimizes both stockouts and excess inventory. The underlying principle is to produce only what sells, driven by actual market demand rather than speculative forecasts.\nThis approach aligns with the broader principles of Dynamic Production Management and the ongoing digital transformation in manufacturing. In the next post, we will explore further practical applications and advanced concepts related to these dynamic systems."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div6_zaikoshisan.html",
    "href": "posts/DynamicInventoryManagement/div6_zaikoshisan.html",
    "title": "Optimal Inventory Calculation: A Dynamic Approach",
    "section": "",
    "text": "In our ongoing series on dynamic production and inventory management, we now delve into the practical aspect of optimal inventory calculation. This post will explore the methodology for determining appropriate inventory levels. The goal is to understand how various factors influence optimal inventory and how a dynamic approach can lead to more efficient supply chain management.\n\n\nThe tocken.com website provides a tool for calculating optimal inventory based on various input parameters related to demand, lead time, and ordering policies. This tool is rooted in the “Inventory Dynamic Model,” which emphasizes the dynamic nature of inventory rather than static stock levels. For those unfamiliar with the foundational concepts, it is recommended to review the “Inventory Dynamic Management Basics” and related literature on the website.\n\n\n\nThe calculation of optimal inventory requires several key input parameters, which are categorized as follows:\n\n\nThis section captures information about customer orders and their variability.\n\nData Aggregation Unit Time (データ集計単位時間): This defines the time unit for collecting order data (e.g., daily, weekly, monthly). If order data is collected daily, the unit time is 1 day.\nAverage Number of Orders (平均受注件数): The average number of orders received per unit time.\nStandard Deviation of Number of Orders (件数・標準偏差): The standard deviation of the number of orders received per unit time, indicating the variability in order frequency.\nAverage Order Quantity per Order (平均受注数量/件): The average quantity of items per order. This should ideally be calculated from the same data used for the number of orders.\nStandard Deviation of Order Quantity per Order (量/件・標準偏差): The standard deviation of the quantity per order, reflecting variability in order size.\n\n\n\n\nThis refers to the time from placing an order until the goods are received in inventory.\n\nAverage Delivery Lead Time (平均納入リードタイム): The average time it takes for an order to be delivered. This should be in the same time unit as the data aggregation unit time.\nStandard Deviation of Delivery Lead Time (標準偏差): The standard deviation of the delivery lead time. If there is no variability, this value should be 0.\n\nNote: While the website primarily uses standard deviation for variability, it mentions that if variance or coefficient of variation data is available, it can be converted to standard deviation using the formula: Standard Deviation = \\(\\sqrt{\\text{Variance}}\\) or Standard Deviation = Average Value \\(\\times\\) Coefficient of Variation.\n\n\n\nThis section involves setting the service level and defining the ordering policy.\n\nService Rate (サービス率): This is the desired percentage of customer demand to be met from stock, ranging from 50% to 99%. It is inversely related to the stockout rate (Service Rate = 100% - Stockout Rate).\nOrder Interval Guideline (発注間隔目安): An approximate frequency of ordering, based on current practice or experience. This helps in setting initial values for the ordering methods.\nApply Conditions (条件反映): Clicking this button populates the fields for Quantity-Based Ordering, Time-Based Ordering, and Order-Count Based Ordering with equivalent values derived from the “Order Interval Guideline.”\n\nQuantity-Based Order Quantity (定量発注量): The fixed quantity to order when inventory reaches a reorder point.\nTime-Based Order Interval (定期発注間隔): The fixed time interval between orders.\nOrder-Count Based Order Count (定件発注件数): The fixed number of orders to trigger a replenishment.\n\nNote: While all three ordering methods can be compared simultaneously, unnecessary methods can be set to 0.\n\nClicking the “Calculate Execution” (計算実行) button displays the calculation results for “Replenishment Inventory” (平均 and 分散) and “Optimal Inventory” (適正在庫).\n\n\n\n\nThe website provides an example calculation with the following conditions:\n\nData aggregation unit time: 1 day (24 hours)\nAverage number of orders: 2 orders/day, Standard deviation: 1.41\nAverage quantity per order: 5 units, Standard deviation: 1.25\nDelivery lead time: 10 days (constant, so standard deviation: 0)\nOrder cycle guideline: 4 days\n\nInputting these values and clicking “Apply Conditions” yields:\n\nQuantity-Based Order Quantity: 40.0 (calculated as 4 days * 2 orders/day * 5 units/order)\nTime-Based Order Interval: 4.0\nOrder-Count Based Order Count: 8.0 (calculated as 4 days * 2 orders/day)\n\nSetting the service rate to 99% and clicking “Calculate Execution” produces the following results:\n\n\n\nOrdering Method\nAverage\nVariance\nOptimal Inventory\n\n\n\n\nQuantity-Based\n122.5\n674.1\n183\n\n\nTime-Based\n120.0\n773.5\n185\n\n\nOrder-Count Based\n117.5\n672.5\n178\n\n\n\n\n\n\nThe website provides the following formulas for calculating the average and variance of replenishment inventory for each ordering method:\n定件発注 (Order-Count Based Ordering)\n\\[\\text{Average} = \\bar{Q} \\cdot \\left(\\bar{N} + \\frac{N_c - 1}{2}\\right)\\]\n\\[\\text{Variance} = \\bar{Q}^2 \\cdot (V_n + \\bar{N}^2 \\cdot C_t^2) + (\\bar{N} + N_c - 1) \\cdot V_q + \\frac{(\\bar{Q} \\cdot N_c)^2}{12}\\]\n定期発注 (Time-Based Ordering)\n\\[\\text{Average} = \\bar{Q} \\cdot \\left(\\bar{N} + \\frac{\\bar{N_y}}{2}\\right)\\]\n\\[\\text{Variance} = \\bar{Q}^2 \\cdot (V_n + \\bar{N}^2 \\cdot C_t^2) + (\\bar{N} + \\bar{N_y}) \\cdot V_q + \\frac{(\\bar{Q} \\cdot \\bar{N_y})^2}{12}\\]\n定量発注 (Quantity-Based Ordering)\n\\[\\text{Average} = \\bar{Q} \\cdot \\left(\\bar{N} + \\frac{1}{2}\\right) + \\frac{O_c}{2}\\]\n\\[\\text{Variance} = \\bar{Q}^2 \\cdot (V_n + \\bar{N}^2 \\cdot C_t^2) + (\\bar{N} + \\frac{1}{2}) \\cdot V_q + \\frac{O_c^2}{12}\\]\nWhere: * \\(N\\): Number of orders per delivery lead time * \\(V_n\\): Variance of \\(N\\) * \\(Q\\): Quantity per order * \\(V_q\\): Variance of \\(Q\\) * \\(N_c\\): Fixed number of orders for order-count based ordering * \\(N_y\\): Number of orders per order cycle (interval) * \\(V_{ny}\\): Variance of \\(N_y\\) * \\(C_t\\): Coefficient of variation of delivery lead time * \\(O_c\\): Fixed order quantity for quantity-based ordering * \\(\\bar{X}\\): Average value of \\(X\\)\nThese formulas are crucial for understanding the mathematical basis of the optimal inventory calculation. They highlight how the variability in demand and lead time, along with the chosen ordering policy, directly impacts the required replenishment inventory and, consequently, the optimal inventory level.\n\n\n\nThis interactive tool allows you to calculate optimal inventory levels based on various demand and lead time parameters.\n\n\nlibrary(shiny)\n\nfluidPage(\n  titlePanel(\"Optimal Inventory Calculator\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      h4(\"1. Order Quantity\"),\n      numericInput(\"data_unit_time\", \"Data Aggregation Unit Time (days)\", value = 1, min = 1),\n      numericInput(\"avg_orders\", \"Average Number of Orders (per day)\", value = 2, min = 0),\n      numericInput(\"sd_orders\", \"Standard Deviation of Number of Orders\", value = 1.41, min = 0),\n      numericInput(\"avg_qty_per_order\", \"Average Quantity per Order\", value = 5, min = 0),\n      numericInput(\"sd_qty_per_order\", \"Standard Deviation of Quantity per Order\", value = 1.25, min = 0),\n      \n      h4(\"2. Delivery Lead Time\"),\n      numericInput(\"avg_lead_time\", \"Average Delivery Lead Time (days)\", value = 10, min = 0),\n      numericInput(\"sd_lead_time\", \"Standard Deviation of Delivery Lead Time\", value = 0, min = 0),\n      \n      h4(\"3. Calculation Execution\"),\n      sliderInput(\"service_rate\", \"Service Rate (%)\", min = 50, max = 99, value = 99),\n      numericInput(\"order_interval_guideline\", \"Order Interval Guideline (days)\", value = 4, min = 0),\n      actionButton(\"apply_conditions\", \"Apply Conditions\"),\n      \n      h5(\"Derived Ordering Parameters:\"),\n      numericInput(\"qty_based_order_qty\", \"Quantity-Based Order Quantity\", value = 40, min = 0),\n      numericInput(\"time_based_order_interval\", \"Time-Based Order Interval\", value = 4, min = 0),\n      numericInput(\"order_count_based_order_count\", \"Order-Count Based Order Count\", value = 8, min = 0),\n      \n      actionButton(\"calculate\", \"Calculate Optimal Inventory\"),\n      actionButton(\"reset\", \"Reset\")\n    ),\n    \n    mainPanel(\n      h4(\"Calculation Results\"),\n      tableOutput(\"results_table\")\n    )\n  )\n)\n\n\n\n\n\n\n\n\nData Aggregation Unit Time (days)\n\n\n\nAverage Number of Orders (per day)\n\n\n\nStandard Deviation of Number of Orders\n\n\n\nAverage Quantity per Order\n\n\n\nStandard Deviation of Quantity per Order\n\n\n\n\nAverage Delivery Lead Time (days)\n\n\n\nStandard Deviation of Delivery Lead Time\n\n\n\n\nService Rate (%)\n\n\n\nOrder Interval Guideline (days)\n\n\nApply Conditions\n\n\nQuantity-Based Order Quantity\n\n\n\nTime-Based Order Interval\n\n\n\nOrder-Count Based Order Count\n\n\nCalculate Optimal Inventory\nReset\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(shiny)\n\nfunction(input, output, session) {\n  \n  observeEvent(input$apply_conditions, {\n    req(input$data_unit_time, input$avg_orders, input$avg_qty_per_order, input$order_interval_guideline)\n    \n    qty_based &lt;- input$order_interval_guideline * input$avg_orders * input$avg_qty_per_order\n    order_count_based &lt;- input$order_interval_guideline * input$avg_orders\n    \n    updateNumericInput(session, \"qty_based_order_qty\", value = qty_based)\n    updateNumericInput(session, \"time_based_order_interval\", value = input$order_interval_guideline)\n    updateNumericInput(session, \"order_count_based_order_count\", value = order_count_based)\n  })\n  \n  observeEvent(input$reset, {\n    updateNumericInput(session, \"data_unit_time\", value = 1)\n    updateNumericInput(session, \"avg_orders\", value = 2)\n    updateNumericInput(session, \"sd_orders\", value = 1.41)\n    updateNumericInput(session, \"avg_qty_per_order\", value = 5)\n    updateNumericInput(session, \"sd_qty_per_order\", value = 1.25)\n    updateNumericInput(session, \"avg_lead_time\", value = 10)\n    updateNumericInput(session, \"sd_lead_time\", value = 0)\n    updateSliderInput(session, \"service_rate\", value = 99)\n    updateNumericInput(session, \"order_interval_guideline\", value = 4)\n    updateNumericInput(session, \"qty_based_order_qty\", value = 40)\n    updateNumericInput(session, \"time_based_order_interval\", value = 4)\n    updateNumericInput(session, \"order_count_based_order_count\", value = 8)\n    output$results_table &lt;- renderTable(NULL)\n  })\n  \n  output$results_table &lt;- renderTable({\n    req(input$calculate)\n    \n    # Input parameters\n    data_unit_time &lt;- input$data_unit_time\n    avg_orders &lt;- input$avg_orders\n    sd_orders &lt;- input$sd_orders\n    avg_qty_per_order &lt;- input$avg_qty_per_order\n    sd_qty_per_order &lt;- input$sd_qty_per_order\n    avg_lead_time &lt;- input$avg_lead_time\n    sd_lead_time &lt;- input$sd_lead_time\n    service_rate &lt;- input$service_rate / 100\n    \n    # Calculate N and Vn (Number of orders per delivery LT)\n    N_bar &lt;- avg_orders * avg_lead_time\n    Vn &lt;- (sd_orders^2 * avg_lead_time) + (avg_orders^2 * sd_lead_time^2)\n    \n    # Calculate Q_bar and Vq (Quantity per order)\n    Q_bar &lt;- avg_qty_per_order\n    Vq &lt;- sd_qty_per_order^2\n    \n    # Calculate Ct (Coefficient of variation of delivery lead time)\n    Ct &lt;- ifelse(avg_lead_time &gt; 0, sd_lead_time / avg_lead_time, 0)\n    \n    results &lt;- data.frame(\n      \"Ordering Method\" = character(),\n      \"Average\" = numeric(),\n      \"Variance\" = numeric(),\n      \"Optimal Inventory\" = numeric(),\n      stringsAsFactors = FALSE\n    )\n    \n    # Quantity-Based Ordering\n    Oc &lt;- input$qty_based_order_qty\n    if (Oc &gt; 0) {\n      mean_qty_based &lt;- Q_bar * (N_bar + 0.5) + Oc / 2\n      var_qty_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + (N_bar + 0.5) * Vq + Oc^2 / 12\n      optimal_qty_based &lt;- mean_qty_based + qnorm(service_rate) * sqrt(var_qty_based)\n      results &lt;- rbind(results, data.frame(\n        \"Ordering Method\" = \"Quantity-Based\",\n        \"Average\" = round(mean_qty_based, 1),\n        \"Variance\" = round(var_qty_based, 1),\n        \"Optimal Inventory\" = round(optimal_qty_based)\n      ))\n    }\n    \n    # Time-Based Ordering\n    Ny_bar &lt;- input$time_based_order_interval * avg_orders\n    Vny &lt;- input$time_based_order_interval * sd_orders^2\n    if (input$time_based_order_interval &gt; 0) {\n      mean_time_based &lt;- Q_bar * (N_bar + Ny_bar / 2)\n      var_time_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + (N_bar + Ny_bar) * Vq + (Q_bar * Ny_bar)^2 / 12\n      optimal_time_based &lt;- mean_time_based + qnorm(service_rate) * sqrt(var_time_based)\n      results &lt;- rbind(results, data.frame(\n        \"Ordering Method\" = \"Time-Based\",\n        \"Average\" = round(mean_time_based, 1),\n        \"Variance\" = round(var_time_based, 1),\n        \"Optimal Inventory\" = round(optimal_time_based)\n      ))\n    }\n    \n    # Order-Count Based Ordering\n    Nc &lt;- input$order_count_based_order_count\n    if (Nc &gt; 0) {\n      mean_order_count_based &lt;- Q_bar * (N_bar + (Nc - 1) / 2)\n      var_order_count_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + (N_bar + Nc - 1) * Vq + (Q_bar * Nc)^2 / 12\n      optimal_order_count_based &lt;- mean_order_count_based + qnorm(service_rate) * sqrt(var_order_count_based)\n      results &lt;- rbind(results, data.frame(\n        \"Ordering Method\" = \"Order-Count Based\",\n        \"Average\" = round(mean_order_count_based, 1),\n        \"Variance\" = round(var_order_count_based, 1),\n        \"Optimal Inventory\" = round(optimal_order_count_based)\n      ))\n    }\n    \n    results\n  })\n}\n\nfunction (input, output, session) \n{\n    observeEvent(input$apply_conditions, {\n        req(input$data_unit_time, input$avg_orders, input$avg_qty_per_order, \n            input$order_interval_guideline)\n        qty_based &lt;- input$order_interval_guideline * input$avg_orders * \n            input$avg_qty_per_order\n        order_count_based &lt;- input$order_interval_guideline * \n            input$avg_orders\n        updateNumericInput(session, \"qty_based_order_qty\", value = qty_based)\n        updateNumericInput(session, \"time_based_order_interval\", \n            value = input$order_interval_guideline)\n        updateNumericInput(session, \"order_count_based_order_count\", \n            value = order_count_based)\n    })\n    observeEvent(input$reset, {\n        updateNumericInput(session, \"data_unit_time\", value = 1)\n        updateNumericInput(session, \"avg_orders\", value = 2)\n        updateNumericInput(session, \"sd_orders\", value = 1.41)\n        updateNumericInput(session, \"avg_qty_per_order\", value = 5)\n        updateNumericInput(session, \"sd_qty_per_order\", value = 1.25)\n        updateNumericInput(session, \"avg_lead_time\", value = 10)\n        updateNumericInput(session, \"sd_lead_time\", value = 0)\n        updateSliderInput(session, \"service_rate\", value = 99)\n        updateNumericInput(session, \"order_interval_guideline\", \n            value = 4)\n        updateNumericInput(session, \"qty_based_order_qty\", value = 40)\n        updateNumericInput(session, \"time_based_order_interval\", \n            value = 4)\n        updateNumericInput(session, \"order_count_based_order_count\", \n            value = 8)\n        output$results_table &lt;- renderTable(NULL)\n    })\n    output$results_table &lt;- renderTable({\n        req(input$calculate)\n        data_unit_time &lt;- input$data_unit_time\n        avg_orders &lt;- input$avg_orders\n        sd_orders &lt;- input$sd_orders\n        avg_qty_per_order &lt;- input$avg_qty_per_order\n        sd_qty_per_order &lt;- input$sd_qty_per_order\n        avg_lead_time &lt;- input$avg_lead_time\n        sd_lead_time &lt;- input$sd_lead_time\n        service_rate &lt;- input$service_rate/100\n        N_bar &lt;- avg_orders * avg_lead_time\n        Vn &lt;- (sd_orders^2 * avg_lead_time) + (avg_orders^2 * \n            sd_lead_time^2)\n        Q_bar &lt;- avg_qty_per_order\n        Vq &lt;- sd_qty_per_order^2\n        Ct &lt;- ifelse(avg_lead_time &gt; 0, sd_lead_time/avg_lead_time, \n            0)\n        results &lt;- data.frame(`Ordering Method` = character(), \n            Average = numeric(), Variance = numeric(), `Optimal Inventory` = numeric(), \n            stringsAsFactors = FALSE)\n        Oc &lt;- input$qty_based_order_qty\n        if (Oc &gt; 0) {\n            mean_qty_based &lt;- Q_bar * (N_bar + 0.5) + Oc/2\n            var_qty_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + \n                (N_bar + 0.5) * Vq + Oc^2/12\n            optimal_qty_based &lt;- mean_qty_based + qnorm(service_rate) * \n                sqrt(var_qty_based)\n            results &lt;- rbind(results, data.frame(`Ordering Method` = \"Quantity-Based\", \n                Average = round(mean_qty_based, 1), Variance = round(var_qty_based, \n                  1), `Optimal Inventory` = round(optimal_qty_based)))\n        }\n        Ny_bar &lt;- input$time_based_order_interval * avg_orders\n        Vny &lt;- input$time_based_order_interval * sd_orders^2\n        if (input$time_based_order_interval &gt; 0) {\n            mean_time_based &lt;- Q_bar * (N_bar + Ny_bar/2)\n            var_time_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + \n                (N_bar + Ny_bar) * Vq + (Q_bar * Ny_bar)^2/12\n            optimal_time_based &lt;- mean_time_based + qnorm(service_rate) * \n                sqrt(var_time_based)\n            results &lt;- rbind(results, data.frame(`Ordering Method` = \"Time-Based\", \n                Average = round(mean_time_based, 1), Variance = round(var_time_based, \n                  1), `Optimal Inventory` = round(optimal_time_based)))\n        }\n        Nc &lt;- input$order_count_based_order_count\n        if (Nc &gt; 0) {\n            mean_order_count_based &lt;- Q_bar * (N_bar + (Nc - \n                1)/2)\n            var_order_count_based &lt;- Q_bar^2 * (Vn + N_bar^2 * \n                Ct^2) + (N_bar + Nc - 1) * Vq + (Q_bar * Nc)^2/12\n            optimal_order_count_based &lt;- mean_order_count_based + \n                qnorm(service_rate) * sqrt(var_order_count_based)\n            results &lt;- rbind(results, data.frame(`Ordering Method` = \"Order-Count Based\", \n                Average = round(mean_order_count_based, 1), Variance = round(var_order_count_based, \n                  1), `Optimal Inventory` = round(optimal_order_count_based)))\n        }\n        results\n    })\n}\n\n\n\n\n\n\nCalculating optimal inventory is a critical aspect of effective supply chain management. By understanding the various input parameters, the underlying mathematical formulas, and embracing a dynamic approach, businesses can minimize costs associated with holding excess inventory while ensuring high service levels. The integration of interactive tools, such as a Shiny application, can further enhance the understanding and practical application of these concepts."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div6_zaikoshisan.html#calculating-optimal-inventory-a-dynamic-perspective",
    "href": "posts/DynamicInventoryManagement/div6_zaikoshisan.html#calculating-optimal-inventory-a-dynamic-perspective",
    "title": "Optimal Inventory Calculation: A Dynamic Approach",
    "section": "",
    "text": "In our ongoing series on dynamic production and inventory management, we now delve into the practical aspect of optimal inventory calculation. This post will explore the methodology for determining appropriate inventory levels. The goal is to understand how various factors influence optimal inventory and how a dynamic approach can lead to more efficient supply chain management.\n\n\nThe tocken.com website provides a tool for calculating optimal inventory based on various input parameters related to demand, lead time, and ordering policies. This tool is rooted in the “Inventory Dynamic Model,” which emphasizes the dynamic nature of inventory rather than static stock levels. For those unfamiliar with the foundational concepts, it is recommended to review the “Inventory Dynamic Management Basics” and related literature on the website.\n\n\n\nThe calculation of optimal inventory requires several key input parameters, which are categorized as follows:\n\n\nThis section captures information about customer orders and their variability.\n\nData Aggregation Unit Time (データ集計単位時間): This defines the time unit for collecting order data (e.g., daily, weekly, monthly). If order data is collected daily, the unit time is 1 day.\nAverage Number of Orders (平均受注件数): The average number of orders received per unit time.\nStandard Deviation of Number of Orders (件数・標準偏差): The standard deviation of the number of orders received per unit time, indicating the variability in order frequency.\nAverage Order Quantity per Order (平均受注数量/件): The average quantity of items per order. This should ideally be calculated from the same data used for the number of orders.\nStandard Deviation of Order Quantity per Order (量/件・標準偏差): The standard deviation of the quantity per order, reflecting variability in order size.\n\n\n\n\nThis refers to the time from placing an order until the goods are received in inventory.\n\nAverage Delivery Lead Time (平均納入リードタイム): The average time it takes for an order to be delivered. This should be in the same time unit as the data aggregation unit time.\nStandard Deviation of Delivery Lead Time (標準偏差): The standard deviation of the delivery lead time. If there is no variability, this value should be 0.\n\nNote: While the website primarily uses standard deviation for variability, it mentions that if variance or coefficient of variation data is available, it can be converted to standard deviation using the formula: Standard Deviation = \\(\\sqrt{\\text{Variance}}\\) or Standard Deviation = Average Value \\(\\times\\) Coefficient of Variation.\n\n\n\nThis section involves setting the service level and defining the ordering policy.\n\nService Rate (サービス率): This is the desired percentage of customer demand to be met from stock, ranging from 50% to 99%. It is inversely related to the stockout rate (Service Rate = 100% - Stockout Rate).\nOrder Interval Guideline (発注間隔目安): An approximate frequency of ordering, based on current practice or experience. This helps in setting initial values for the ordering methods.\nApply Conditions (条件反映): Clicking this button populates the fields for Quantity-Based Ordering, Time-Based Ordering, and Order-Count Based Ordering with equivalent values derived from the “Order Interval Guideline.”\n\nQuantity-Based Order Quantity (定量発注量): The fixed quantity to order when inventory reaches a reorder point.\nTime-Based Order Interval (定期発注間隔): The fixed time interval between orders.\nOrder-Count Based Order Count (定件発注件数): The fixed number of orders to trigger a replenishment.\n\nNote: While all three ordering methods can be compared simultaneously, unnecessary methods can be set to 0.\n\nClicking the “Calculate Execution” (計算実行) button displays the calculation results for “Replenishment Inventory” (平均 and 分散) and “Optimal Inventory” (適正在庫).\n\n\n\n\nThe website provides an example calculation with the following conditions:\n\nData aggregation unit time: 1 day (24 hours)\nAverage number of orders: 2 orders/day, Standard deviation: 1.41\nAverage quantity per order: 5 units, Standard deviation: 1.25\nDelivery lead time: 10 days (constant, so standard deviation: 0)\nOrder cycle guideline: 4 days\n\nInputting these values and clicking “Apply Conditions” yields:\n\nQuantity-Based Order Quantity: 40.0 (calculated as 4 days * 2 orders/day * 5 units/order)\nTime-Based Order Interval: 4.0\nOrder-Count Based Order Count: 8.0 (calculated as 4 days * 2 orders/day)\n\nSetting the service rate to 99% and clicking “Calculate Execution” produces the following results:\n\n\n\nOrdering Method\nAverage\nVariance\nOptimal Inventory\n\n\n\n\nQuantity-Based\n122.5\n674.1\n183\n\n\nTime-Based\n120.0\n773.5\n185\n\n\nOrder-Count Based\n117.5\n672.5\n178\n\n\n\n\n\n\nThe website provides the following formulas for calculating the average and variance of replenishment inventory for each ordering method:\n定件発注 (Order-Count Based Ordering)\n\\[\\text{Average} = \\bar{Q} \\cdot \\left(\\bar{N} + \\frac{N_c - 1}{2}\\right)\\]\n\\[\\text{Variance} = \\bar{Q}^2 \\cdot (V_n + \\bar{N}^2 \\cdot C_t^2) + (\\bar{N} + N_c - 1) \\cdot V_q + \\frac{(\\bar{Q} \\cdot N_c)^2}{12}\\]\n定期発注 (Time-Based Ordering)\n\\[\\text{Average} = \\bar{Q} \\cdot \\left(\\bar{N} + \\frac{\\bar{N_y}}{2}\\right)\\]\n\\[\\text{Variance} = \\bar{Q}^2 \\cdot (V_n + \\bar{N}^2 \\cdot C_t^2) + (\\bar{N} + \\bar{N_y}) \\cdot V_q + \\frac{(\\bar{Q} \\cdot \\bar{N_y})^2}{12}\\]\n定量発注 (Quantity-Based Ordering)\n\\[\\text{Average} = \\bar{Q} \\cdot \\left(\\bar{N} + \\frac{1}{2}\\right) + \\frac{O_c}{2}\\]\n\\[\\text{Variance} = \\bar{Q}^2 \\cdot (V_n + \\bar{N}^2 \\cdot C_t^2) + (\\bar{N} + \\frac{1}{2}) \\cdot V_q + \\frac{O_c^2}{12}\\]\nWhere: * \\(N\\): Number of orders per delivery lead time * \\(V_n\\): Variance of \\(N\\) * \\(Q\\): Quantity per order * \\(V_q\\): Variance of \\(Q\\) * \\(N_c\\): Fixed number of orders for order-count based ordering * \\(N_y\\): Number of orders per order cycle (interval) * \\(V_{ny}\\): Variance of \\(N_y\\) * \\(C_t\\): Coefficient of variation of delivery lead time * \\(O_c\\): Fixed order quantity for quantity-based ordering * \\(\\bar{X}\\): Average value of \\(X\\)\nThese formulas are crucial for understanding the mathematical basis of the optimal inventory calculation. They highlight how the variability in demand and lead time, along with the chosen ordering policy, directly impacts the required replenishment inventory and, consequently, the optimal inventory level.\n\n\n\nThis interactive tool allows you to calculate optimal inventory levels based on various demand and lead time parameters.\n\n\nlibrary(shiny)\n\nfluidPage(\n  titlePanel(\"Optimal Inventory Calculator\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      h4(\"1. Order Quantity\"),\n      numericInput(\"data_unit_time\", \"Data Aggregation Unit Time (days)\", value = 1, min = 1),\n      numericInput(\"avg_orders\", \"Average Number of Orders (per day)\", value = 2, min = 0),\n      numericInput(\"sd_orders\", \"Standard Deviation of Number of Orders\", value = 1.41, min = 0),\n      numericInput(\"avg_qty_per_order\", \"Average Quantity per Order\", value = 5, min = 0),\n      numericInput(\"sd_qty_per_order\", \"Standard Deviation of Quantity per Order\", value = 1.25, min = 0),\n      \n      h4(\"2. Delivery Lead Time\"),\n      numericInput(\"avg_lead_time\", \"Average Delivery Lead Time (days)\", value = 10, min = 0),\n      numericInput(\"sd_lead_time\", \"Standard Deviation of Delivery Lead Time\", value = 0, min = 0),\n      \n      h4(\"3. Calculation Execution\"),\n      sliderInput(\"service_rate\", \"Service Rate (%)\", min = 50, max = 99, value = 99),\n      numericInput(\"order_interval_guideline\", \"Order Interval Guideline (days)\", value = 4, min = 0),\n      actionButton(\"apply_conditions\", \"Apply Conditions\"),\n      \n      h5(\"Derived Ordering Parameters:\"),\n      numericInput(\"qty_based_order_qty\", \"Quantity-Based Order Quantity\", value = 40, min = 0),\n      numericInput(\"time_based_order_interval\", \"Time-Based Order Interval\", value = 4, min = 0),\n      numericInput(\"order_count_based_order_count\", \"Order-Count Based Order Count\", value = 8, min = 0),\n      \n      actionButton(\"calculate\", \"Calculate Optimal Inventory\"),\n      actionButton(\"reset\", \"Reset\")\n    ),\n    \n    mainPanel(\n      h4(\"Calculation Results\"),\n      tableOutput(\"results_table\")\n    )\n  )\n)\n\n\n\n\n\n\n\n\nData Aggregation Unit Time (days)\n\n\n\nAverage Number of Orders (per day)\n\n\n\nStandard Deviation of Number of Orders\n\n\n\nAverage Quantity per Order\n\n\n\nStandard Deviation of Quantity per Order\n\n\n\n\nAverage Delivery Lead Time (days)\n\n\n\nStandard Deviation of Delivery Lead Time\n\n\n\n\nService Rate (%)\n\n\n\nOrder Interval Guideline (days)\n\n\nApply Conditions\n\n\nQuantity-Based Order Quantity\n\n\n\nTime-Based Order Interval\n\n\n\nOrder-Count Based Order Count\n\n\nCalculate Optimal Inventory\nReset\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(shiny)\n\nfunction(input, output, session) {\n  \n  observeEvent(input$apply_conditions, {\n    req(input$data_unit_time, input$avg_orders, input$avg_qty_per_order, input$order_interval_guideline)\n    \n    qty_based &lt;- input$order_interval_guideline * input$avg_orders * input$avg_qty_per_order\n    order_count_based &lt;- input$order_interval_guideline * input$avg_orders\n    \n    updateNumericInput(session, \"qty_based_order_qty\", value = qty_based)\n    updateNumericInput(session, \"time_based_order_interval\", value = input$order_interval_guideline)\n    updateNumericInput(session, \"order_count_based_order_count\", value = order_count_based)\n  })\n  \n  observeEvent(input$reset, {\n    updateNumericInput(session, \"data_unit_time\", value = 1)\n    updateNumericInput(session, \"avg_orders\", value = 2)\n    updateNumericInput(session, \"sd_orders\", value = 1.41)\n    updateNumericInput(session, \"avg_qty_per_order\", value = 5)\n    updateNumericInput(session, \"sd_qty_per_order\", value = 1.25)\n    updateNumericInput(session, \"avg_lead_time\", value = 10)\n    updateNumericInput(session, \"sd_lead_time\", value = 0)\n    updateSliderInput(session, \"service_rate\", value = 99)\n    updateNumericInput(session, \"order_interval_guideline\", value = 4)\n    updateNumericInput(session, \"qty_based_order_qty\", value = 40)\n    updateNumericInput(session, \"time_based_order_interval\", value = 4)\n    updateNumericInput(session, \"order_count_based_order_count\", value = 8)\n    output$results_table &lt;- renderTable(NULL)\n  })\n  \n  output$results_table &lt;- renderTable({\n    req(input$calculate)\n    \n    # Input parameters\n    data_unit_time &lt;- input$data_unit_time\n    avg_orders &lt;- input$avg_orders\n    sd_orders &lt;- input$sd_orders\n    avg_qty_per_order &lt;- input$avg_qty_per_order\n    sd_qty_per_order &lt;- input$sd_qty_per_order\n    avg_lead_time &lt;- input$avg_lead_time\n    sd_lead_time &lt;- input$sd_lead_time\n    service_rate &lt;- input$service_rate / 100\n    \n    # Calculate N and Vn (Number of orders per delivery LT)\n    N_bar &lt;- avg_orders * avg_lead_time\n    Vn &lt;- (sd_orders^2 * avg_lead_time) + (avg_orders^2 * sd_lead_time^2)\n    \n    # Calculate Q_bar and Vq (Quantity per order)\n    Q_bar &lt;- avg_qty_per_order\n    Vq &lt;- sd_qty_per_order^2\n    \n    # Calculate Ct (Coefficient of variation of delivery lead time)\n    Ct &lt;- ifelse(avg_lead_time &gt; 0, sd_lead_time / avg_lead_time, 0)\n    \n    results &lt;- data.frame(\n      \"Ordering Method\" = character(),\n      \"Average\" = numeric(),\n      \"Variance\" = numeric(),\n      \"Optimal Inventory\" = numeric(),\n      stringsAsFactors = FALSE\n    )\n    \n    # Quantity-Based Ordering\n    Oc &lt;- input$qty_based_order_qty\n    if (Oc &gt; 0) {\n      mean_qty_based &lt;- Q_bar * (N_bar + 0.5) + Oc / 2\n      var_qty_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + (N_bar + 0.5) * Vq + Oc^2 / 12\n      optimal_qty_based &lt;- mean_qty_based + qnorm(service_rate) * sqrt(var_qty_based)\n      results &lt;- rbind(results, data.frame(\n        \"Ordering Method\" = \"Quantity-Based\",\n        \"Average\" = round(mean_qty_based, 1),\n        \"Variance\" = round(var_qty_based, 1),\n        \"Optimal Inventory\" = round(optimal_qty_based)\n      ))\n    }\n    \n    # Time-Based Ordering\n    Ny_bar &lt;- input$time_based_order_interval * avg_orders\n    Vny &lt;- input$time_based_order_interval * sd_orders^2\n    if (input$time_based_order_interval &gt; 0) {\n      mean_time_based &lt;- Q_bar * (N_bar + Ny_bar / 2)\n      var_time_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + (N_bar + Ny_bar) * Vq + (Q_bar * Ny_bar)^2 / 12\n      optimal_time_based &lt;- mean_time_based + qnorm(service_rate) * sqrt(var_time_based)\n      results &lt;- rbind(results, data.frame(\n        \"Ordering Method\" = \"Time-Based\",\n        \"Average\" = round(mean_time_based, 1),\n        \"Variance\" = round(var_time_based, 1),\n        \"Optimal Inventory\" = round(optimal_time_based)\n      ))\n    }\n    \n    # Order-Count Based Ordering\n    Nc &lt;- input$order_count_based_order_count\n    if (Nc &gt; 0) {\n      mean_order_count_based &lt;- Q_bar * (N_bar + (Nc - 1) / 2)\n      var_order_count_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + (N_bar + Nc - 1) * Vq + (Q_bar * Nc)^2 / 12\n      optimal_order_count_based &lt;- mean_order_count_based + qnorm(service_rate) * sqrt(var_order_count_based)\n      results &lt;- rbind(results, data.frame(\n        \"Ordering Method\" = \"Order-Count Based\",\n        \"Average\" = round(mean_order_count_based, 1),\n        \"Variance\" = round(var_order_count_based, 1),\n        \"Optimal Inventory\" = round(optimal_order_count_based)\n      ))\n    }\n    \n    results\n  })\n}\n\nfunction (input, output, session) \n{\n    observeEvent(input$apply_conditions, {\n        req(input$data_unit_time, input$avg_orders, input$avg_qty_per_order, \n            input$order_interval_guideline)\n        qty_based &lt;- input$order_interval_guideline * input$avg_orders * \n            input$avg_qty_per_order\n        order_count_based &lt;- input$order_interval_guideline * \n            input$avg_orders\n        updateNumericInput(session, \"qty_based_order_qty\", value = qty_based)\n        updateNumericInput(session, \"time_based_order_interval\", \n            value = input$order_interval_guideline)\n        updateNumericInput(session, \"order_count_based_order_count\", \n            value = order_count_based)\n    })\n    observeEvent(input$reset, {\n        updateNumericInput(session, \"data_unit_time\", value = 1)\n        updateNumericInput(session, \"avg_orders\", value = 2)\n        updateNumericInput(session, \"sd_orders\", value = 1.41)\n        updateNumericInput(session, \"avg_qty_per_order\", value = 5)\n        updateNumericInput(session, \"sd_qty_per_order\", value = 1.25)\n        updateNumericInput(session, \"avg_lead_time\", value = 10)\n        updateNumericInput(session, \"sd_lead_time\", value = 0)\n        updateSliderInput(session, \"service_rate\", value = 99)\n        updateNumericInput(session, \"order_interval_guideline\", \n            value = 4)\n        updateNumericInput(session, \"qty_based_order_qty\", value = 40)\n        updateNumericInput(session, \"time_based_order_interval\", \n            value = 4)\n        updateNumericInput(session, \"order_count_based_order_count\", \n            value = 8)\n        output$results_table &lt;- renderTable(NULL)\n    })\n    output$results_table &lt;- renderTable({\n        req(input$calculate)\n        data_unit_time &lt;- input$data_unit_time\n        avg_orders &lt;- input$avg_orders\n        sd_orders &lt;- input$sd_orders\n        avg_qty_per_order &lt;- input$avg_qty_per_order\n        sd_qty_per_order &lt;- input$sd_qty_per_order\n        avg_lead_time &lt;- input$avg_lead_time\n        sd_lead_time &lt;- input$sd_lead_time\n        service_rate &lt;- input$service_rate/100\n        N_bar &lt;- avg_orders * avg_lead_time\n        Vn &lt;- (sd_orders^2 * avg_lead_time) + (avg_orders^2 * \n            sd_lead_time^2)\n        Q_bar &lt;- avg_qty_per_order\n        Vq &lt;- sd_qty_per_order^2\n        Ct &lt;- ifelse(avg_lead_time &gt; 0, sd_lead_time/avg_lead_time, \n            0)\n        results &lt;- data.frame(`Ordering Method` = character(), \n            Average = numeric(), Variance = numeric(), `Optimal Inventory` = numeric(), \n            stringsAsFactors = FALSE)\n        Oc &lt;- input$qty_based_order_qty\n        if (Oc &gt; 0) {\n            mean_qty_based &lt;- Q_bar * (N_bar + 0.5) + Oc/2\n            var_qty_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + \n                (N_bar + 0.5) * Vq + Oc^2/12\n            optimal_qty_based &lt;- mean_qty_based + qnorm(service_rate) * \n                sqrt(var_qty_based)\n            results &lt;- rbind(results, data.frame(`Ordering Method` = \"Quantity-Based\", \n                Average = round(mean_qty_based, 1), Variance = round(var_qty_based, \n                  1), `Optimal Inventory` = round(optimal_qty_based)))\n        }\n        Ny_bar &lt;- input$time_based_order_interval * avg_orders\n        Vny &lt;- input$time_based_order_interval * sd_orders^2\n        if (input$time_based_order_interval &gt; 0) {\n            mean_time_based &lt;- Q_bar * (N_bar + Ny_bar/2)\n            var_time_based &lt;- Q_bar^2 * (Vn + N_bar^2 * Ct^2) + \n                (N_bar + Ny_bar) * Vq + (Q_bar * Ny_bar)^2/12\n            optimal_time_based &lt;- mean_time_based + qnorm(service_rate) * \n                sqrt(var_time_based)\n            results &lt;- rbind(results, data.frame(`Ordering Method` = \"Time-Based\", \n                Average = round(mean_time_based, 1), Variance = round(var_time_based, \n                  1), `Optimal Inventory` = round(optimal_time_based)))\n        }\n        Nc &lt;- input$order_count_based_order_count\n        if (Nc &gt; 0) {\n            mean_order_count_based &lt;- Q_bar * (N_bar + (Nc - \n                1)/2)\n            var_order_count_based &lt;- Q_bar^2 * (Vn + N_bar^2 * \n                Ct^2) + (N_bar + Nc - 1) * Vq + (Q_bar * Nc)^2/12\n            optimal_order_count_based &lt;- mean_order_count_based + \n                qnorm(service_rate) * sqrt(var_order_count_based)\n            results &lt;- rbind(results, data.frame(`Ordering Method` = \"Order-Count Based\", \n                Average = round(mean_order_count_based, 1), Variance = round(var_order_count_based, \n                  1), `Optimal Inventory` = round(optimal_order_count_based)))\n        }\n        results\n    })\n}\n\n\n\n\n\n\nCalculating optimal inventory is a critical aspect of effective supply chain management. By understanding the various input parameters, the underlying mathematical formulas, and embracing a dynamic approach, businesses can minimize costs associated with holding excess inventory while ensuring high service levels. The integration of interactive tools, such as a Shiny application, can further enhance the understanding and practical application of these concepts."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div3_prod.html",
    "href": "posts/DynamicInventoryManagement/div3_prod.html",
    "title": "Dynamic Production Management: Shifting from Centralized Control to Demand-Driven Autonomy",
    "section": "",
    "text": "In our ongoing series on Dynamic Inventory Theories, we previously explored the challenges of traditional inventory management and introduced the concept of an Inventory Dynamic Model. This post shifts our focus to the broader domain of production management, examining why conventional, centralized approaches often struggle in today’s volatile markets and how a shift towards demand-driven, decentralized production management (DPM) offers a more resilient solution.\n\n\nTraditional production management is fundamentally based on establishing a production plan, ensuring its execution, and then cycling through a Plan-Do-See (PDS) management loop. While seemingly straightforward, the reality for many companies is a constant struggle to keep up with rapidly changing demand. Experts often suggest shortening planning and production cycles, even advocating for daily planning, to address this issue.\nHowever, the situation often worsens. Despite meticulous efforts in creating detailed medium-to-long-term plans, and then breaking them down into monthly, weekly, daily, and even minute-by-minute schedules for each process, these plans are frequently disrupted by changes. The production floor, which is set up to operate according to these precise plans, faces significant disruption when changes occur. This raises a critical question: Is the inability to execute plans as intended a sign of low management levels or insufficient effort?\nProduction management textbooks consistently emphasize the importance of production plans. Yet, the constant changes to these plans lead to significant confusion on the production floor. Production managers often find themselves caught between the demands of the shop floor and the upper management, struggling to reconcile the two.\n\n\n\nSo, what is the fundamental problem? The core issue lies in the inability of planning and production cycles to keep pace with shortening product lifecycles and intensifying demand fluctuations. While it might be easy to change a plan on a computer, the reality of the production floor involves a complex interplay of people, machinery, work-in-process (WIP), external suppliers, and material deliveries. The physical, economic, organizational, institutional, and psychological inertia within this system creates immense resistance to implementing plan changes.\nTherefore, attributing the failure of production plan-centric management to a lack of effort or low management levels is a misdiagnosis. A system that controls production activities based on a production plan can only truly function effectively under the condition that “the initial production plan does not change.” While some industries or companies might operate in environments close to this ideal, a far greater number of businesses are constantly grappling with the chaos of plan changes.\n\n\n\nLet’s examine the relationship between production plans and actual production activities in more detail. A typical production management cycle involves reviewing market information, sales data, and internal production/inventory information to create a 4-6 month plan. The production plan for the immediate month is finalized, broken down into a detailed schedule, and production orders are issued to the shop floor. This plan is then rolled over monthly, with adjustments made to the unconfirmed months.\nThis system, where the production plan dictates the majority of basic production activities, can be termed a “centralized control system.” While this method works well for some companies, particularly those in relatively stable markets like the automotive industry that maintain vast supply chains, it often fails for others. For instance, third and fourth-tier subcontractors in the automotive industry, who are exposed to various fluctuating factors, often find centralized production management ineffective. This is even more pronounced for companies operating in environments with highly volatile demand.\nDespite the common refrain that “planning and production cycles must be shortened simultaneously, even daily if necessary,” and the hopes placed on IT technologies like Advanced Planning & Scheduling (APS) to achieve this, the reality is that the production system possesses enormous inertia. Simply increasing information processing speed through IT will not shorten cycles if the fundamental control system remains centralized. The core problem lies in the centralized nature of the management system itself. Recognizing this points towards a solution: decentralization.\n\n\n\nWhat happens when we separate and decentralize the centralized production management system? Let’s imagine how the production floor would operate:\n\nAutonomous Input: At the start of the day, operators check their terminal screens for the input plan, which lists orders (workpieces) in their input sequence. After confirming that necessary materials are available, they proceed with inputting items in order. Once processed, items are moved to the next stage. The input plan is updated in real-time, so operators continuously check it. If the total WIP in the input process reaches a set limit, an instruction to stop input is issued.\nFirst-In, First-Out (FIFO) Processing: Each process operates on a FIFO basis. Workpieces that arrive are processed in that order and sent to the next process. In certain processes (typically those where WIP tends to accumulate), processing is done according to a priority order displayed on the terminal. There are no production plans or detailed schedules dictating these actions.\n\nThis vision of the production floor highlights a shift from being controlled by a central plan to operating autonomously based on real-time conditions and local rules. The production plan, in this decentralized model, becomes a reference rather than a strict command. The focus shifts from adherence to a fixed plan to dynamic adaptation and responsiveness.\n\n\n\nThis decentralized approach is at the heart of Dynamic Production Management (DPM). DPM aims to create a production system that can autonomously respond to demand fluctuations and unforeseen events without constant top-down intervention. It recognizes that the production floor, with its inherent complexities and real-time information, is best positioned to make immediate operational decisions.\nKey aspects of DPM include:\n\nReal-time Information: Utilizing up-to-the-minute data on demand, WIP, and resource availability to guide production activities.\nLocal Decision-Making: Empowering individual processes and operators to make decisions based on local conditions and established rules, rather than rigid schedules.\nSelf-Correction: The system is designed to self-correct and adapt to minor deviations, with human intervention reserved for significant anomalies or capacity issues.\nFlow Optimization: Focusing on optimizing the flow of materials and workpieces through the system, rather than simply maximizing individual process utilization.\n\nThis shift from a centralized, plan-driven system to a decentralized, demand-driven one is not merely a technological upgrade; it’s a fundamental change in philosophy. It acknowledges the limitations of human planning in highly dynamic environments and leverages the power of real-time data and autonomous decision-making to create a more agile and resilient production system.\n\n\n\nThe challenges faced by traditional production management systems stem from their centralized, plan-driven nature and their inability to cope with the inherent inertia of complex production environments in the face of rapid change. The solution lies in embracing decentralization and adopting the principles of Dynamic Production Management. By empowering the production floor with real-time information and autonomous decision-making capabilities, businesses can achieve greater responsiveness, efficiency, and adaptability.\nThis approach aligns with the broader trends of Industry 4.0, where interconnected systems and intelligent automation enable a more fluid and responsive manufacturing landscape. In future posts, we will continue to explore the practical implications and further concepts related to DPM, providing a comprehensive understanding for navigating the complexities of modern production and inventory management."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div3_prod.html#the-evolution-of-production-management-beyond-static-plans",
    "href": "posts/DynamicInventoryManagement/div3_prod.html#the-evolution-of-production-management-beyond-static-plans",
    "title": "Dynamic Production Management: Shifting from Centralized Control to Demand-Driven Autonomy",
    "section": "",
    "text": "In our ongoing series on Dynamic Inventory Theories, we previously explored the challenges of traditional inventory management and introduced the concept of an Inventory Dynamic Model. This post shifts our focus to the broader domain of production management, examining why conventional, centralized approaches often struggle in today’s volatile markets and how a shift towards demand-driven, decentralized production management (DPM) offers a more resilient solution.\n\n\nTraditional production management is fundamentally based on establishing a production plan, ensuring its execution, and then cycling through a Plan-Do-See (PDS) management loop. While seemingly straightforward, the reality for many companies is a constant struggle to keep up with rapidly changing demand. Experts often suggest shortening planning and production cycles, even advocating for daily planning, to address this issue.\nHowever, the situation often worsens. Despite meticulous efforts in creating detailed medium-to-long-term plans, and then breaking them down into monthly, weekly, daily, and even minute-by-minute schedules for each process, these plans are frequently disrupted by changes. The production floor, which is set up to operate according to these precise plans, faces significant disruption when changes occur. This raises a critical question: Is the inability to execute plans as intended a sign of low management levels or insufficient effort?\nProduction management textbooks consistently emphasize the importance of production plans. Yet, the constant changes to these plans lead to significant confusion on the production floor. Production managers often find themselves caught between the demands of the shop floor and the upper management, struggling to reconcile the two.\n\n\n\nSo, what is the fundamental problem? The core issue lies in the inability of planning and production cycles to keep pace with shortening product lifecycles and intensifying demand fluctuations. While it might be easy to change a plan on a computer, the reality of the production floor involves a complex interplay of people, machinery, work-in-process (WIP), external suppliers, and material deliveries. The physical, economic, organizational, institutional, and psychological inertia within this system creates immense resistance to implementing plan changes.\nTherefore, attributing the failure of production plan-centric management to a lack of effort or low management levels is a misdiagnosis. A system that controls production activities based on a production plan can only truly function effectively under the condition that “the initial production plan does not change.” While some industries or companies might operate in environments close to this ideal, a far greater number of businesses are constantly grappling with the chaos of plan changes.\n\n\n\nLet’s examine the relationship between production plans and actual production activities in more detail. A typical production management cycle involves reviewing market information, sales data, and internal production/inventory information to create a 4-6 month plan. The production plan for the immediate month is finalized, broken down into a detailed schedule, and production orders are issued to the shop floor. This plan is then rolled over monthly, with adjustments made to the unconfirmed months.\nThis system, where the production plan dictates the majority of basic production activities, can be termed a “centralized control system.” While this method works well for some companies, particularly those in relatively stable markets like the automotive industry that maintain vast supply chains, it often fails for others. For instance, third and fourth-tier subcontractors in the automotive industry, who are exposed to various fluctuating factors, often find centralized production management ineffective. This is even more pronounced for companies operating in environments with highly volatile demand.\nDespite the common refrain that “planning and production cycles must be shortened simultaneously, even daily if necessary,” and the hopes placed on IT technologies like Advanced Planning & Scheduling (APS) to achieve this, the reality is that the production system possesses enormous inertia. Simply increasing information processing speed through IT will not shorten cycles if the fundamental control system remains centralized. The core problem lies in the centralized nature of the management system itself. Recognizing this points towards a solution: decentralization.\n\n\n\nWhat happens when we separate and decentralize the centralized production management system? Let’s imagine how the production floor would operate:\n\nAutonomous Input: At the start of the day, operators check their terminal screens for the input plan, which lists orders (workpieces) in their input sequence. After confirming that necessary materials are available, they proceed with inputting items in order. Once processed, items are moved to the next stage. The input plan is updated in real-time, so operators continuously check it. If the total WIP in the input process reaches a set limit, an instruction to stop input is issued.\nFirst-In, First-Out (FIFO) Processing: Each process operates on a FIFO basis. Workpieces that arrive are processed in that order and sent to the next process. In certain processes (typically those where WIP tends to accumulate), processing is done according to a priority order displayed on the terminal. There are no production plans or detailed schedules dictating these actions.\n\nThis vision of the production floor highlights a shift from being controlled by a central plan to operating autonomously based on real-time conditions and local rules. The production plan, in this decentralized model, becomes a reference rather than a strict command. The focus shifts from adherence to a fixed plan to dynamic adaptation and responsiveness.\n\n\n\nThis decentralized approach is at the heart of Dynamic Production Management (DPM). DPM aims to create a production system that can autonomously respond to demand fluctuations and unforeseen events without constant top-down intervention. It recognizes that the production floor, with its inherent complexities and real-time information, is best positioned to make immediate operational decisions.\nKey aspects of DPM include:\n\nReal-time Information: Utilizing up-to-the-minute data on demand, WIP, and resource availability to guide production activities.\nLocal Decision-Making: Empowering individual processes and operators to make decisions based on local conditions and established rules, rather than rigid schedules.\nSelf-Correction: The system is designed to self-correct and adapt to minor deviations, with human intervention reserved for significant anomalies or capacity issues.\nFlow Optimization: Focusing on optimizing the flow of materials and workpieces through the system, rather than simply maximizing individual process utilization.\n\nThis shift from a centralized, plan-driven system to a decentralized, demand-driven one is not merely a technological upgrade; it’s a fundamental change in philosophy. It acknowledges the limitations of human planning in highly dynamic environments and leverages the power of real-time data and autonomous decision-making to create a more agile and resilient production system.\n\n\n\nThe challenges faced by traditional production management systems stem from their centralized, plan-driven nature and their inability to cope with the inherent inertia of complex production environments in the face of rapid change. The solution lies in embracing decentralization and adopting the principles of Dynamic Production Management. By empowering the production floor with real-time information and autonomous decision-making capabilities, businesses can achieve greater responsiveness, efficiency, and adaptability.\nThis approach aligns with the broader trends of Industry 4.0, where interconnected systems and intelligent automation enable a more fluid and responsive manufacturing landscape. In future posts, we will continue to explore the practical implications and further concepts related to DPM, providing a comprehensive understanding for navigating the complexities of modern production and inventory management."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div1_intr.html",
    "href": "posts/DynamicInventoryManagement/div1_intr.html",
    "title": "Dynamic Inventory Theories: An Introduction",
    "section": "",
    "text": "In the ever-evolving landscape of modern business, effective inventory management stands as a critical pillar for operational efficiency and profitability. Traditional approaches, often rooted in static production plans, are increasingly challenged by the volatile and unpredictable nature of today’s markets. This series will explore the concept of Dynamic Production Management (DPM), a paradigm shift from conventional production planning to a demand-centric approach.\n\n\nMany businesses, particularly those in industries with fluctuating demand, find themselves in a perpetual struggle with their production plans. Despite meticulous efforts and the adoption of advanced software, the reality often involves constant revisions, leading to confusion and inefficiency. The core issue lies in the inherent rigidity of a system that attempts to predict and control demand rather than adapt to it.\nWhile some industries with stable markets (e.g., automotive manufacturing) may thrive under a production plan-centric model, the vast majority of businesses face dynamic market conditions where such a model proves inadequate. The fundamental challenge is to bridge the gap between production capabilities and the unpredictable ebb and flow of customer demand.\n\n\n\nDynamic Production Management (DPM) proposes a revolutionary shift: moving from a production plan-driven system to one that is demand-driven. This approach recognizes that in most scenarios, controlling demand from the production side is nearly impossible; instead, the production function must be agile enough to follow and respond to demand fluctuations. To achieve this, a deep understanding of the characteristics of the production function is essential.\n\n\n\nDPM emphasizes three primary elements that define the dynamic characteristics of production:\n\nProduction Rate: This refers to the quantity of goods produced per unit of time. In a dynamic environment, the ability to adjust this rate quickly in response to demand signals is crucial.\nFlow Time: This is the total time elapsed from the initiation of production (input) to the completion of the final product. Minimizing and controlling flow time is vital for responsiveness.\nWIP (Work In Process): This represents the number of items currently undergoing production. While not directly related to customer demand, an increase in WIP often leads to a proportional increase in flow time, making its management critical for efficiency.\n\nThe interplay between these three elements is central to understanding and controlling the dynamic behaviour of a production system. External factors such as demand variability, equipment breakdowns, labour availability, and material shortages constantly impact these elements, necessitating a dynamic rather than static perspective.\n\n\n\nThe principles of DPM offer distinct control methodologies for different production strategies:\n\nMake-to-Order (受注生産): In this model, production commences only after a sale is confirmed (who, when, how many, at what price). The primary focus is on time management. Progress is autonomously controlled based on the ratio of elapsed time to a predetermined standard flow time for each order or input.\nMake-to-Stock (見込生産): This strategy involves producing goods before a sale is confirmed, often for inventory replenishment. While it primarily manages physical\n\n“things” (WIP, warehouse inventory), it also requires underlying time management. Priority control is autonomously determined by the position within the fluid inventory, which includes input plans, WIP, and warehouse inventory.\nWhile DPM emphasizes autonomous control, it also acknowledges the need for human intervention when production capacity is exceeded or risks like shortages and missed deadlines arise due to factors like equipment failure. In such cases, alarms are triggered, prompting human oversight.\n\n\n\nThe concept of DPM aligns seamlessly with the ongoing Fourth Industrial Revolution, driven by the Internet of Everything (IoE) and the Internet of Things (IoT). This era promises revolutionary changes in manufacturing, with initiatives like Industry 4.0 gaining momentum. The traditional monthly production management cycles, reliant on static production plans, are becoming obsolete.\nIn the future envisioned by DPM, shop floor activities will autonomously follow order information, and production plans will be generated in near real-time based on the latest available data. This eliminates the need for rigid, periodic planning cycles, fostering a more responsive and adaptive manufacturing environment. Embracing DPM is presented as a crucial step in preparing for this imminent industrial transformation.\n\n\n\nThis introductory post has laid the groundwork for understanding Dynamic Production Management as a response to the complexities of modern inventory challenges. By shifting focus from static production plans to dynamic demand-driven control, businesses can enhance their adaptability and responsiveness. In subsequent posts, we will delve deeper into the specific theories and methodologies that underpin DPM, providing practical insights for optimizing inventory management in a dynamic world."
  },
  {
    "objectID": "posts/DynamicInventoryManagement/div1_intr.html#the-evolution-of-inventory-management-from-static-plans-to-dynamic-demand",
    "href": "posts/DynamicInventoryManagement/div1_intr.html#the-evolution-of-inventory-management-from-static-plans-to-dynamic-demand",
    "title": "Dynamic Inventory Theories: An Introduction",
    "section": "",
    "text": "In the ever-evolving landscape of modern business, effective inventory management stands as a critical pillar for operational efficiency and profitability. Traditional approaches, often rooted in static production plans, are increasingly challenged by the volatile and unpredictable nature of today’s markets. This series will explore the concept of Dynamic Production Management (DPM), a paradigm shift from conventional production planning to a demand-centric approach.\n\n\nMany businesses, particularly those in industries with fluctuating demand, find themselves in a perpetual struggle with their production plans. Despite meticulous efforts and the adoption of advanced software, the reality often involves constant revisions, leading to confusion and inefficiency. The core issue lies in the inherent rigidity of a system that attempts to predict and control demand rather than adapt to it.\nWhile some industries with stable markets (e.g., automotive manufacturing) may thrive under a production plan-centric model, the vast majority of businesses face dynamic market conditions where such a model proves inadequate. The fundamental challenge is to bridge the gap between production capabilities and the unpredictable ebb and flow of customer demand.\n\n\n\nDynamic Production Management (DPM) proposes a revolutionary shift: moving from a production plan-driven system to one that is demand-driven. This approach recognizes that in most scenarios, controlling demand from the production side is nearly impossible; instead, the production function must be agile enough to follow and respond to demand fluctuations. To achieve this, a deep understanding of the characteristics of the production function is essential.\n\n\n\nDPM emphasizes three primary elements that define the dynamic characteristics of production:\n\nProduction Rate: This refers to the quantity of goods produced per unit of time. In a dynamic environment, the ability to adjust this rate quickly in response to demand signals is crucial.\nFlow Time: This is the total time elapsed from the initiation of production (input) to the completion of the final product. Minimizing and controlling flow time is vital for responsiveness.\nWIP (Work In Process): This represents the number of items currently undergoing production. While not directly related to customer demand, an increase in WIP often leads to a proportional increase in flow time, making its management critical for efficiency.\n\nThe interplay between these three elements is central to understanding and controlling the dynamic behaviour of a production system. External factors such as demand variability, equipment breakdowns, labour availability, and material shortages constantly impact these elements, necessitating a dynamic rather than static perspective.\n\n\n\nThe principles of DPM offer distinct control methodologies for different production strategies:\n\nMake-to-Order (受注生産): In this model, production commences only after a sale is confirmed (who, when, how many, at what price). The primary focus is on time management. Progress is autonomously controlled based on the ratio of elapsed time to a predetermined standard flow time for each order or input.\nMake-to-Stock (見込生産): This strategy involves producing goods before a sale is confirmed, often for inventory replenishment. While it primarily manages physical\n\n“things” (WIP, warehouse inventory), it also requires underlying time management. Priority control is autonomously determined by the position within the fluid inventory, which includes input plans, WIP, and warehouse inventory.\nWhile DPM emphasizes autonomous control, it also acknowledges the need for human intervention when production capacity is exceeded or risks like shortages and missed deadlines arise due to factors like equipment failure. In such cases, alarms are triggered, prompting human oversight.\n\n\n\nThe concept of DPM aligns seamlessly with the ongoing Fourth Industrial Revolution, driven by the Internet of Everything (IoE) and the Internet of Things (IoT). This era promises revolutionary changes in manufacturing, with initiatives like Industry 4.0 gaining momentum. The traditional monthly production management cycles, reliant on static production plans, are becoming obsolete.\nIn the future envisioned by DPM, shop floor activities will autonomously follow order information, and production plans will be generated in near real-time based on the latest available data. This eliminates the need for rigid, periodic planning cycles, fostering a more responsive and adaptive manufacturing environment. Embracing DPM is presented as a crucial step in preparing for this imminent industrial transformation.\n\n\n\nThis introductory post has laid the groundwork for understanding Dynamic Production Management as a response to the complexities of modern inventory challenges. By shifting focus from static production plans to dynamic demand-driven control, businesses can enhance their adaptability and responsiveness. In subsequent posts, we will delve deeper into the specific theories and methodologies that underpin DPM, providing practical insights for optimizing inventory management in a dynamic world."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html",
    "href": "posts/create a Quarto blog/index.html",
    "title": "Create a Quarto Blog",
    "section": "",
    "text": "This is a tutorial on how to create a new Quarto blog using the RStudio interface.\nThis tutorial is based on this website, and inspired by this blog and this video.\nThe procedure for publishing a Quarto blog using GitHub Pages and GitHub Actions is as follows:"
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#create-project-in-rstudio",
    "href": "posts/create a Quarto blog/index.html#create-project-in-rstudio",
    "title": "Create a Quarto Blog",
    "section": "1. Create Project in RStudio",
    "text": "1. Create Project in RStudio\n\nCreate a new Quarto blog project in RStudio.\nName the directory with the desired URL (e.g., “yourusername.github.io” for a personal blog).\nEnsure “Create a git repository” and “Use renv with this project” are checked."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#set-up-github-repository",
    "href": "posts/create a Quarto blog/index.html#set-up-github-repository",
    "title": "Create a Quarto Blog",
    "section": "2. Set up GitHub Repository",
    "text": "2. Set up GitHub Repository\n\nCreate a new public repository on GitHub with the same name, “yourusername.github.io”, as the RStudio project directory."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#push-local-to-github",
    "href": "posts/create a Quarto blog/index.html#push-local-to-github",
    "title": "Create a Quarto Blog",
    "section": "3. Push Local to GitHub",
    "text": "3. Push Local to GitHub\n\nUse terminal to link the local project folder to the new GitHub repository.\nCommit and push the initial project files to the main branch on GitHub."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#create-gh-pages-branch",
    "href": "posts/create a Quarto blog/index.html#create-gh-pages-branch",
    "title": "Create a Quarto Blog",
    "section": "4. Create gh-pages Branch",
    "text": "4. Create gh-pages Branch\n\nOn GitHub, create a new branch named “gh-pages”."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#set-up-github-actions-workflow",
    "href": "posts/create a Quarto blog/index.html#set-up-github-actions-workflow",
    "title": "Create a Quarto Blog",
    "section": "5. Set up GitHub Actions Workflow",
    "text": "5. Set up GitHub Actions Workflow\n\nIn your RStudio project, create the folder structure: “.github/workflows/”.\nInside workflows, create a new text file named publish.yml.\nCopy the R EnV example code chunk from the Quarto documentation for publishing to GitHub Pages and paste it into publish.yml.\nSave, commit, and push these changes to the main branch on GitHub."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#configure-github-pages",
    "href": "posts/create a Quarto blog/index.html#configure-github-pages",
    "title": "Create a Quarto Blog",
    "section": "6. Configure GitHub Pages",
    "text": "6. Configure GitHub Pages\n\nOn GitHub, go to your repository’s Settings, then Pages.\nUnder “Branch”, select gh-pages and click Save."
  },
  {
    "objectID": "posts/create a Quarto blog/index.html#monitore-workflow-and-visit-site",
    "href": "posts/create a Quarto blog/index.html#monitore-workflow-and-visit-site",
    "title": "Create a Quarto Blog",
    "section": "7. Monitore Workflow and Visit Site",
    "text": "7. Monitore Workflow and Visit Site\n\nGo to the Actions tab on GitHub to monitor the running workflows.\nOnce all workflows have successfully completed, return to Settings and then Pages.\nClick “Visit site” to view your live Quarto website.\n\nAfter this setup, any future changes committed to the repository will automatically update the blog without local rendering."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html",
    "href": "posts/UNBC thesis in latex/thesis0.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "As a fresh master’s graduate, I have found that one of the most important skills isn’t just knowing how to run a regression, but how to ensure my entire research process is transparent and reproducible. This means others can easily access and replicate my work, from the raw data to the final presentation.\nThis workflow, inspired by the principles of reproducible research, helped me efficiently manage my data, analysis, and writing. This series will guide you through setting up and using a similar workflow for your own research projects.\nIn this first post, we’ll cover the foundational concepts of a reproducible research workflow and how to effectively manage your project files.\n\n\nA reproducible research workflow is a system for organizing your entire research project—from data gathering to final presentation—in a way that allows you, or others, to easily and exactly reproduce your results. The key idea is to automate and link every step of the process, ensuring that your final paper is a direct, verifiable product of your data and analysis.\n\n\n\nBefore we dive into organizing our folders, let’s establish two guiding principles that are the bedrock of this entire workflow, as emphasized in Christopher Gandrud’s “Reproducible Research with R and RStudio”.\n\n\nThe ideal is to store every component of your research in plain text formats.\n\nData: Use text-based formats like comma-separated values (.csv).\nAnalysis & Documents: This is where R scripts (.R), Quarto/R Markdown (.qmd/.Rmd), LaTeX (.tex), and BibTeX (.bib) files shine. They are all plain text.\n\nWhy is this so important? Text files are universal and future-proof. They can be opened by virtually any software on any computer. Proprietary file formats can become obsolete, locking you out of your own work. Text files are also easily tracked by version control systems like Git.\n\n\n\nJust because a computer can read your file doesn’t mean a person can understand it. The goal is to make your files clear to two very important people: your future self and your collaborators.\n\nCommenting Your Code: Liberally add comments to your R scripts. Explain the why behind your code, not just the what.\nClear Naming Conventions: Give your files and variables descriptive names. A consistent style, like the camelCase we use here (e.g., mainAnalysis.qmd), makes files easy to read and reference.\nLiterate Programming: This is the key. Tools like Quarto allow you to weave your code, its output, and your explanatory text together in a single document.\n\n\n\n\n\nA research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- explns/\n|   |   `-- 00_explanatory.R\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html#what-is-a-reproducible-research-workflow",
    "href": "posts/UNBC thesis in latex/thesis0.html#what-is-a-reproducible-research-workflow",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A reproducible research workflow is a system for organizing your entire research project—from data gathering to final presentation—in a way that allows you, or others, to easily and exactly reproduce your results. The key idea is to automate and link every step of the process, ensuring that your final paper is a direct, verifiable product of your data and analysis."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html#core-principles-text-files-and-human-readability",
    "href": "posts/UNBC thesis in latex/thesis0.html#core-principles-text-files-and-human-readability",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Before we dive into organizing our folders, let’s establish two guiding principles that are the bedrock of this entire workflow, as emphasized in Christopher Gandrud’s “Reproducible Research with R and RStudio”.\n\n\nThe ideal is to store every component of your research in plain text formats.\n\nData: Use text-based formats like comma-separated values (.csv).\nAnalysis & Documents: This is where R scripts (.R), Quarto/R Markdown (.qmd/.Rmd), LaTeX (.tex), and BibTeX (.bib) files shine. They are all plain text.\n\nWhy is this so important? Text files are universal and future-proof. They can be opened by virtually any software on any computer. Proprietary file formats can become obsolete, locking you out of your own work. Text files are also easily tracked by version control systems like Git.\n\n\n\nJust because a computer can read your file doesn’t mean a person can understand it. The goal is to make your files clear to two very important people: your future self and your collaborators.\n\nCommenting Your Code: Liberally add comments to your R scripts. Explain the why behind your code, not just the what.\nClear Naming Conventions: Give your files and variables descriptive names. A consistent style, like the camelCase we use here (e.g., mainAnalysis.qmd), makes files easy to read and reference.\nLiterate Programming: This is the key. Tools like Quarto allow you to weave your code, its output, and your explanatory text together in a single document."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis0.html#structuring-research-the-toulmin-model",
    "href": "posts/UNBC thesis in latex/thesis0.html#structuring-research-the-toulmin-model",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "A research paper is, at its core, a formal argument. To structure our project files for maximum clarity and reproducibility, we can borrow from philosopher Stephen Toulmin’s model of argumentation. The model provides a powerful framework for understanding the components of a persuasive argument. The three core components are:\n\nThe Claim: This is the assertion you are making, the conclusion you want your audience to accept. In academic terms, this is your core finding.\nThe Evidence (or Data/Grounds): These are the facts, data, and established knowledge that provide the foundation for your claim. In our context, the evidence is twofold: the raw data we analyze and the scholarly literature upon which our work is built.\nThe Warrant: This is the crucial, logical connection that links the evidence to the claim. The warrant explains how the evidence supports the claim. It’s the reasoning, the methodology, the analysis.\n\nA reproducible workflow, therefore, should be structured to make the relationships between your Claim, Evidence, and Warrant transparent and verifiable. We can organize our project files to directly mirror this logical structure.\nyourProjectName/\n|\n|-- 1_evidence/\n|   |-- rawData/\n|   |   |-- rawDataSource1.csv\n|   |   `-- rawDataSource2.csv\n|   `-- literature.bib\n|\n|-- 2_warrantAndAnalysis/\n|   |-- explns/\n|   |   `-- 00_explanatory.R\n|   |-- models/\n|   |   |-- 01_linearRegression.R\n|   |   `-- 02_bayesianModel.R\n|   |-- results/\n|   |   |-- tables/\n|   |   |   `-- regressionTable.tex\n|   |   `-- figures/\n|   |       `-- scatterPlot.png\n|   |-- referenceNotes.md\n|   `-- mainAnalysis.qmd\n|\n|-- 3_claimAndPresentation/\n|   |-- paper.tex\n|   |-- slides.tex\n|   `-- website.qmd\n|\n`-- README.md\nHere’s how the file structure maps to the Toulmin model:\n\n1_evidence/: This directory holds the unassailable grounds for your argument.\n\nrawData/: Your empirical evidence.\nliterature.bib: Your scholarly evidence.\n\n2_warrantAndAnalysis/: This directory is your warrant. It contains the entire logical and computational process that connects your evidence to your claim.\n\nmainAnalysis.qmd: The master narrative of your analysis. It sources the models, generates all outputs, and is the engine of the project.\nmodels/: The specific, modular R scripts that execute the statistical tests.\nresults/: The direct outputs of your analysis—tables and figures—logically live here, alongside the scripts that created them.\nreferenceNotes.txt: Your explicit reasoning about the literature.\n\n3_claimAndPresentation/: This directory contains the various final formats for your Claim. These are communication documents, not analysis documents. They are lightweight and focused on presentation.\n\npaper.tex: Your formal academic paper in LaTeX.\nslides.tex: Your conference presentation, likely using LaTeX Beamer.\nwebsite.qmd: A Quarto file for a blog post or project website.\nAll three of these files will pull their content (citations from literature.bib, tables and figures from results/) from the other directories.\n\n\nBy organizing your project this way, anyone can clearly trace the path from your foundational evidence, through your analytical warrant, to your final claim. This is the essence of a truly reproducible and defensible research project."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html",
    "href": "posts/UNBC thesis in latex/thesis1.html",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "Welcome back to our series on building a reproducible research workflow.\nIn Part 1, we established a philosophical and structural foundation for our project using the Toulmin model.\nWe set up a directory structure to house our Evidence, Warrant, and Claim.\nNow, we’ll focus on the first and most critical step of the research process: the Evidence.\nSpecifically, how do we gather and store our data and references in a way that is robust, transparent, and reproducible?\nThis post will cover the “how-to” for populating our 1_evidence/ directory.\n\n\nThe 1_evidence/rawData/ directory is the most important folder in your entire project.\nIt should be treated as a write-once, read-many-times archive.\nThe Golden Rule: Never, ever, ever manually edit your raw data files. Do not open rawDataSource1.csv in Excel to “quickly fix” a typo or delete a column.\nWhy is this so critical?\n\nTraceability: Any change to the data must be documented and scripted. If you manually edit the raw file, you lose the record of that change forever.\nIntegrity: It guarantees that you can always return to the original state of your data if you make a mistake in your data processing scripts.\nReproducibility: Anyone seeking to reproduce your work needs access to the exact same starting point you had.\n\nAll data cleaning, filtering, and transformation should happen programmatically in the scripts we will build later in the 2_warrantAndAnalysis/ directory.\n\n\n\nThe most reproducible way to get data is to script its collection. This creates a perfect, runnable record of where your evidence came from.\n\n\nSometimes, your data might come from a non-digital source, like a historical text or your own experimental observations. Instead of putting it in Excel first, you can enter it directly into R using a data.frame(). This is the most transparent way to digitize data.\n```{r createDataManually}\n# Creating a small dataset of hypothetical inventory data\ninventoryData &lt;- data.frame(\n  productId = c(\"A1\", \"A2\", \"B1\", \"B2\"),\n  category = c(\"Fruit\", \"Fruit\", \"Dairy\", \"Dairy\"),\n  stockLevel = c(150, 200, 80, 120),\n  storageTemp = c(4, 4, 2, 2)\n)\n\n# Save this manually created raw data for the record\n# Note: This is one of the few times we write to the evidence folder,\n# and only to create a reproducible starting point.\nwrite.csv(inventoryData, \"../1_evidence/rawData/manualInventory.csv\", row.names = FALSE)\n```\n\n\n\nIf data is hosted on a website in a plain-text format (like .csv), you can download it directly. This avoids manual “download and drop” and ensures you’re getting the data from the original source every time.\n```{r downloadFromWeb}\n# URL of a raw CSV file on the web\nurl &lt;- \"http://www.some-data-repository.com/food-data.csv\"\n\n# Use rio::import( ) to read the data directly into R\n# The rio package is great at handling various file types from a URL\nlibrary(rio)\nwebData &lt;- import(url, format = \"csv\")\n```\n\n\n\nGitHub is a common place to find data.\nTo download a file, it is similar to Method 2, make sure you use the “Raw” file URL.\n```{r downloadFromGitHub}\n# URL for the \"Raw\" version of a CSV file on GitHub\ngithubUrl &lt;- \"https://raw.githubusercontent.com/someuser/somerepo/main/data.csv\"\n\ngithubData &lt;- import(githubUrl, format = \"csv\" )\n```\n\n\n\nFor larger, more structured projects, data is often stored in a relational database (like PostgreSQL, MySQL, or SQL Server). R can connect directly to these databases and run SQL queries. This is an extremely powerful and reproducible method.\nTo do this, you’ll need two key packages:\n\nDBI: The standard database interface for R.\nA specific package for your database type (e.g., RPostgres for PostgreSQL, RMariaDB for MySQL/MariaDB).\n\nThe process involves three steps: connect, query, and disconnect.\n``{r downloadFromSQL, eval=false} # NOTE: This chunk is not evaluated (eval=false`) because it requires a live database. # 1. Load the necessary libraries library(DBI) library(RPostgres) # Or the package for your specific database"
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#the-sanctity-of-raw-data",
    "href": "posts/UNBC thesis in latex/thesis1.html#the-sanctity-of-raw-data",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "The 1_evidence/rawData/ directory is the most important folder in your entire project.\nIt should be treated as a write-once, read-many-times archive.\nThe Golden Rule: Never, ever, ever manually edit your raw data files. Do not open rawDataSource1.csv in Excel to “quickly fix” a typo or delete a column.\nWhy is this so critical?\n\nTraceability: Any change to the data must be documented and scripted. If you manually edit the raw file, you lose the record of that change forever.\nIntegrity: It guarantees that you can always return to the original state of your data if you make a mistake in your data processing scripts.\nReproducibility: Anyone seeking to reproduce your work needs access to the exact same starting point you had.\n\nAll data cleaning, filtering, and transformation should happen programmatically in the scripts we will build later in the 2_warrantAndAnalysis/ directory."
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#gathering-data-scripting-the-collection",
    "href": "posts/UNBC thesis in latex/thesis1.html#gathering-data-scripting-the-collection",
    "title": "Reproducible Research Reporting",
    "section": "",
    "text": "The most reproducible way to get data is to script its collection. This creates a perfect, runnable record of where your evidence came from.\n\n\nSometimes, your data might come from a non-digital source, like a historical text or your own experimental observations. Instead of putting it in Excel first, you can enter it directly into R using a data.frame(). This is the most transparent way to digitize data.\n```{r createDataManually}\n# Creating a small dataset of hypothetical inventory data\ninventoryData &lt;- data.frame(\n  productId = c(\"A1\", \"A2\", \"B1\", \"B2\"),\n  category = c(\"Fruit\", \"Fruit\", \"Dairy\", \"Dairy\"),\n  stockLevel = c(150, 200, 80, 120),\n  storageTemp = c(4, 4, 2, 2)\n)\n\n# Save this manually created raw data for the record\n# Note: This is one of the few times we write to the evidence folder,\n# and only to create a reproducible starting point.\nwrite.csv(inventoryData, \"../1_evidence/rawData/manualInventory.csv\", row.names = FALSE)\n```\n\n\n\nIf data is hosted on a website in a plain-text format (like .csv), you can download it directly. This avoids manual “download and drop” and ensures you’re getting the data from the original source every time.\n```{r downloadFromWeb}\n# URL of a raw CSV file on the web\nurl &lt;- \"http://www.some-data-repository.com/food-data.csv\"\n\n# Use rio::import( ) to read the data directly into R\n# The rio package is great at handling various file types from a URL\nlibrary(rio)\nwebData &lt;- import(url, format = \"csv\")\n```\n\n\n\nGitHub is a common place to find data.\nTo download a file, it is similar to Method 2, make sure you use the “Raw” file URL.\n```{r downloadFromGitHub}\n# URL for the \"Raw\" version of a CSV file on GitHub\ngithubUrl &lt;- \"https://raw.githubusercontent.com/someuser/somerepo/main/data.csv\"\n\ngithubData &lt;- import(githubUrl, format = \"csv\" )\n```\n\n\n\nFor larger, more structured projects, data is often stored in a relational database (like PostgreSQL, MySQL, or SQL Server). R can connect directly to these databases and run SQL queries. This is an extremely powerful and reproducible method.\nTo do this, you’ll need two key packages:\n\nDBI: The standard database interface for R.\nA specific package for your database type (e.g., RPostgres for PostgreSQL, RMariaDB for MySQL/MariaDB).\n\nThe process involves three steps: connect, query, and disconnect.\n``{r downloadFromSQL, eval=false} # NOTE: This chunk is not evaluated (eval=false`) because it requires a live database. # 1. Load the necessary libraries library(DBI) library(RPostgres) # Or the package for your specific database"
  },
  {
    "objectID": "posts/UNBC thesis in latex/thesis1.html#storing-your-literature-the-power-of-bibtex",
    "href": "posts/UNBC thesis in latex/thesis1.html#storing-your-literature-the-power-of-bibtex",
    "title": "Reproducible Research Reporting",
    "section": "Storing Your Literature: The Power of BibTeX",
    "text": "Storing Your Literature: The Power of BibTeX\nJust as your raw data is empirical evidence, your references are scholarly evidence. The best way to manage them is with a BibTeX file (.bib).\nA BibTeX file is a plain-text database of all your citations. Each entry has a unique key and contains all the necessary metadata for a reference.\nWhy use BibTeX?\n\nSingle Source of Truth: All your references are in one place. No more copying and pasting citations between documents.\nConsistency: It ensures every citation in your paper and every entry in your bibliography is formatted perfectly and consistently according to your chosen style.\nPortability: It’s a plain text file, making it easy to share, version control, and use across different projects and documents.\n\n\nBibTeX Examples\nLet’s look at a few common entry types you might have in your 1_evidence/literature.bib file.\n1. A Book: The @book type is for a complete book. Note that the pages field is generally not used for a whole book, but for a specific chapter (see next example).\n@book{Gandrud2020,\n  author    = {Christopher Gandrud},\n  title     = {Reproducible Research with R and RStudio},\n  edition   = {Third},\n  year      = {2020},\n  publisher = {CRC Press}\n}\n2. A Journal Article: The @article type is for a peer-reviewed journal article. It requires journal and volume fields.\n@article{Donoho2010,\n  author  = {David L. Donoho},\n  title   = {An invitation to reproducible computational research},\n  journal = {Biostatistics},\n  year    = {2010},\n  volume  = {11},\n  number  = {3},\n  pages   = {385--388}\n}\n3. A Chapter in an Edited Book: The @incollection type is perfect for citing a single chapter from a book with multiple authors and an editor. This is where the pages field is essential.\n@incollection{Buckheit1995,\n  author    = {Jonathan B. Buckheit and David L. Donoho},\n  title     = {Wavelab and Reproducible Research},\n  booktitle = {Wavelets and Statistics},\n  editor    = {Anestis Antoniadis},\n  year      = {1995},\n  publisher = {Springer},\n  address   = {New York},\n  pages     = {55--81}\n}\nThe unique key (e.g., Gandrud2020, Donoho2010) is what you’ll use to cite the work in your documents.\nBy the end of this stage, your 1_evidence/ directory should be populated with your raw, untouched data files and a comprehensive literature.bib file. You now have a solid, verifiable foundation of evidence.\nAfter these steps, you have a clean, well-documented, and fully-reproducible analytical dataset, ready for the statistical modeling we will cover in the next part.\nIn our next post, we will move on to Part 3: The Warrant - Processing Data and Weaving the Analysis, where we’ll start working in the 2_warrantAndAnalysis/ directory. We’ll write our first R scripts to read in the raw data, clean it, and prepare it for the main analysis."
  },
  {
    "objectID": "posts/academic reporting in tex 2/index.html",
    "href": "posts/academic reporting in tex 2/index.html",
    "title": "Crafting Academic Reports: A LaTeX and Overleaf Guide",
    "section": "",
    "text": "Procedure for Creating Your Academic Report in Overleaf\nHere’s a detailed, step-by-step procedure to recreate your academic report using Overleaf:\n\n\nStep 1: Create a New Project in Overleaf\nGo to Overleaf.com and log in or sign up.\nOn your dashboard, click “New Project”.\nSelect “Blank Project”.\nGive your project a meaningful name, e.g., “Academic Report Example.”\n\n\nStep 2: Set Up the Document Class and Packages (The Preamble)\nIn the main .tex file (usually main.tex), you’ll see a basic structure. Delete everything in main.tex.\nCopy and paste the following lines from your example code into the very beginning of your main.tex file. These lines define the document type and load essential packages:\nCode snippet\n\\documentclass{article}[12pt]\n\n\\usepackage[margin=1in]{geometry}\n\\usepackage[utf8]{inputenc}\n\\usepackage{bm,amsmath}\n\\usepackage{amsthm}\n\\usepackage{amsmath, amsfonts}\n\\usepackage[english]{babel}\n\\usepackage{comment}\n\\usepackage{tikz}\n\\usepackage{indentfirst}\n\\usepackage{apacite}\n\\usepackage{natbib}\n\\usepackage{setspace}\n\\usepackage{comment}\n\\usepackage{float}\n\\usepackage{bibentry}\n\\usetikzlibrary[patterns]\nExplanation:\n\n\\documentclass{article}\\[12pt\\]: Sets the document as an article with a 12pt font size.\n\\usepackage[margin=1in]{geometry}: Sets the page margins to 1 inch.\n\\usepackage[utf8]{inputenc}: Essential for handling various characters.\n\\usepackage{bm,amsmath},\\usepackage{amsthm},\\usepackage{amsmath, amsfonts}: These are crucial for advanced mathematical typesetting, including bold symbols and theorem environments.\n\\usepackage[english]{babel}: Sets the language to English for hyphenation and other linguistic rules.\n\\usepackage{comment}: Allows you to comment out large blocks of text.\n\\usepackage{tikz} and \\usetikzlibrary[patterns]: These are for creating high-quality vector graphics and diagrams directly within your LaTeX document, like the supply and demand curves in your example.\n\\usepackage{indentfirst}: Ensures the first paragraph of a section is indented.\n\\usepackage{apacite}, \\usepackage{natbib}, \\usepackage{bibentry}: These packages are for managing citations and bibliographies, specifically for APA style (apacite) and general flexible citation management (natbib).\n\\usepackage{setspace}: For controlling line spacing.\n\\usepackage{float}: Provides more control over figure and table placement.\n\n\n\nStep 3: Define Custom Commands and Environments (Still in the Preamble)\nImmediately after the\n\\usepackage\ncommands, add your custom definitions. These make your code more concise and consistent.\nCode snippet\n\\DeclareUnicodeCharacter{2212}{-}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\definecolor{solnblue}{rgb}{0,0,1}\n\\newenvironment{soln}{\\color{solnblue}}{}\n\\newcommand\\widebar[1]{\\mathop{\\overline{#1}}}\n\\renewcommand{\\vec}[1]{\\bm{#1}}\n\\newcommand{\\mat}[1]{\\bm{#1}}\n\\newcommand{\\tran}{^{\\mathstrut\\scriptscriptstyle\\top}}\nExplanation:\n\\DeclareUnicodeCharacter{2212}{-}: Handles a specific unicode character for a minus sign.\n\\DeclareMathOperator{\\E}{\\mathbb{E}}: Defines a custom operator for mathematical expectation.\n\\definecolor{solnblue}{rgb}{0,0,1} and \\newenvironment{soln}{\\color{solnblue}}{}: This creates a custom environment called soln that will display its content in blue, which is excellent for highlighting solutions or specific sections.\n\\\\newcommand and \\\\renewcommand: These define shorthand commands for common mathematical notations like a wide bar, vector, matrix, and transpose.\n\n\nStep 4: Add Title Information\nBefore \\begin{document}, include your title, authors, and date:\nCode snippet\n\\title{FRE 460: Assignment 5}\n\\author{Angie Pan, $13236147$\\\\\nAkihiko Mori, $50120161$}\n\\date{April 9, 2021}\nExplanation:\n\\title, \\author, and\n\\date define these standard report elements. The \\\\ in \\author creates a line break.\n\n\nStep 5: Start the Document Body\nEvery LaTeX document’s main content goes between\n\\begin{document} and \\end{document}. Add these lines:\nCode snippet\n\\begin{document}\n\\maketitle\n\\vspace{1em}\n\n% Part A content will go here\n\n\\end{document}\nExplanation:\n\\maketitle: Generates the title page based on the \\title, \\author, and \\date commands.\n\\vspace{1em}: Adds a small vertical space.\n\n\nStep 6: Add Sections and Subsections\nYou can structure your report using \\section* (for unnumbered sections) and \\subsection* (for unnumbered subsections). Insert these within your \\begin{document} and \\end{document}:\nCode snippet\n\\section*{Part A: Fair Trade}\n% ... content for Part A ...\n\\subsection*{A-1. Supply and Demand Curves: No Fair-Trade}\n% ... content for A-1 ...\n\\subsection*{A-2. Equilibrium: No Fair-Trade}\n% ... content for A-2 ...\nExplanation: The * after \\section or \\subsection prevents them from being numbered, which is common in assignment reports.\n\n\nStep 7: Incorporate Text and Equations\nType your regular text as you would in any document.\nFor equations, use environments like align* for multi-line, aligned equations. Notice how & is used for alignment and \\\\ for new lines within the equation.\nCode snippet\n\\noindent\\textit{If rounding is necessary, round only the final answer; don’t round at intermediate steps.}\\\\\nThe industry-level marginal cost curve for producing an agricultural product is as follows:\n$MPC_n = 2Q$, where $Q$ is tonnes of the product produced.\nGlobal consumers get use benefits from consuming the product, with $MPB_n=8000-3Q$.\n\n% Example of equation from A-2:\n\\bgroup \\color{solnblue} % This is your custom blue solution environment\n\\noindent For $Q_n$,\n\\begin{align*}\n    MPB_n(Q_n) &= MPC_n(Q_n)\\\\\n    8000-3Q_n  &= 2Q_n\\\\\n    Q_n &= \\frac{8000}{5}\\\\\n    &= 1600.\n\\end{align*}\nFor $P_n$,\n\\begin{align*}\n    P_n &= MPC_n(Q_n)\\\\\n      &= 2Q_n\\\\\n     &= 2\\cdot 1600\\\\\n    &= 3200.\n\\end{align*}\nTherefore, the equilibrium if there is no Fair-Trade label in the agriculture product is $(Q_n, P_n) = (1,600,\\$3,200)$.\n\\egroup \nExplanation: \\noindent prevents indentation. Math mode is entered with $ for inline math (MPC_n=2Q) or environments like align* for displayed equations.\n\n\nStep 8: Create Figures with TikZ\nFor your detailed graphs, use the tikzpicture environment within a figure environment. This allows you to draw shapes, lines, and add text precisely.\nCode snippet\n\\bgroup \\color{solnblue}\n    \\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.4]\n                \\draw[thick,&lt;-&gt;] (0,13) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                % ... rest of your TikZ code ...\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product}\n            \\label{fig:A-1}\n        \\end{center}\n    \\end{figure}\n\\egroup \nExplanation:\n\\begin{figure}[H]: Defines a floating figure. [H] attempts to place it “here” (exactly where it’s defined in the code).\n\\begin{center}: Centers the content within the figure.\n\\begin{tikzpicture}[scale=0.4]: Starts the drawing environment. scale=0.4 adjusts the size of the drawing.\n\\draw, \\node: Fundamental TikZ commands for drawing lines/arrows and placing text/labels.\n\\caption{...}: Adds a caption to your figure.\n\\label{fig:A-1}: Allows you to cross-reference the figure later using \\ref{fig:A-1}.\n\n\nStep 9: Add Tables (if applicable)\nYour code includes a tabular environment for a table. This is how you’d structure it:\nCode snippet\n\\bgroup \\color{solnblue}\n\\begin{table}[H]\n        \\begin{center}\n            \\begin{tabular}{llll}\n               & Before & After & $\\Delta$\\\\ \\hline \\\\\n            CS & b+c+f  & a+b   & a-(c+f) \\\\\n            PS & g+h    & c+d+g & c+d-h   \\\\ \\hline \\\\\n            TS &        &       & a+d-(h+f) \n        \\end{tabular}\n        \\end{center}\n    \\end{table}\n% ... rest of your table explanation text ...\n\\egroup\nExplanation:\n\\begin{table}[H]: Defines a floating table.\n\\begin{tabular}{llll}: Creates the table structure. l means left-aligned column, r for right, c for center. llll means four left-aligned columns.\n\\\\: New row in a table.\n&: Column separator.\n\\hline: Horizontal line in the table.\n\n\nStep 10: Compile Your Document\nIn Overleaf, click the “Recompile” button (or it might recompile automatically if “Auto Compile” is on).\nThe right panel will display the generated PDF. Check for errors in the “Logs and output files” section if the compilation fails.\n\n\nStep 11: Continue Adding Content\nRepeat steps 6-9 for each section of your report, referencing your .tex code as a guide.\nMake sure to close all environments (e.g., every \\bgroup \\color{solnblue} needs a \\egroup).\nUse % for single-line comments in your .tex file to explain your code, as you’ve done in your example (%Question Part A).\n\\documentclass{article}[12pt]\n\n\\usepackage[margin=1in]{geometry}\n\\usepackage[utf8]{inputenc}\n\\usepackage{bm,amsmath}\n\\usepackage{amsthm}\n\\usepackage{amsmath, amsfonts}\n\\usepackage[english]{babel}\n\\usepackage{comment}\n\\usepackage{tikz}\n\\usepackage{indentfirst}\n\\usepackage{apacite}\n\\usepackage{natbib}\n\\usepackage{setspace}\n\\usepackage{comment}\n\\usepackage{float}\n\\usepackage{bibentry}\n\\usetikzlibrary[patterns]\n\n\n\\DeclareUnicodeCharacter{2212}{-}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\n\\definecolor{solnblue}{rgb}{0,0,1}\n\\newenvironment{soln}{\\color{solnblue}}{}\n\n\\newcommand\\widebar[1]{\\mathop{\\overline{#1}}}\n\\renewcommand{\\vec}[1]{\\bm{#1}}\n\\newcommand{\\mat}[1]{\\bm{#1}}\n\\newcommand{\\tran}{^{\\mathstrut\\scriptscriptstyle\\top}} \n%\\setlength\n%\\parindent{0pt}\n\n\\title{FRE 460: Assignment 5}\n\\author{Angie Pan, $13236147$\\\\\nAkihiko Mori, $50120161$}\n\\date{April 9, 2021}\n\n\\begin{document}\n\\maketitle\n\\vspace{1em}\n\n%Part A\n%\\clearpage\n\\section*{Part A: Fair Trade}\n%Question Part A\n\\noindent\\textit{If rounding is necessary, round only the final answer; don’t round at intermediate steps.}\\\\\nThe industry-level marginal cost curve for producing an agricultural product is as follows: \n$MPC_n = 2Q$, where $Q$ is tonnes of the product produced.\nGlobal consumers get use benefits from consuming the product, with $MPB_n=8000-3Q$.\n\n%Question Q1\n\\subsection*{A-1. Supply and Demand Curves: No Fair-Trade}\n\\noindent (10 points) In the space below, depict the supply and demand curves for the product.\n%Question end\n%Start Answer\n\\begin{soln}\n    \\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.4]\n                \\draw[thick,&lt;-&gt;] (0,13) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \\node [below] at (5,0) {$Q_n=1600$};\n                \\node [left] at (0,5) {$P_n=3200$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\draw(0,0)--(9,9) node[right]{$MPC_n=S$};\n                \\draw(0,10)--(10,0);\n                \\node[above] at (11,1) {$MPB_n=D$};\n                \\draw[dashed](0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product}\n            \\label{fig:A-1}\n        \\end{center}\n    \\end{figure}\n\\end{soln}\n%Answer End\n%Q2\n\\clearpage\n\\subsection*{A-2. Equilibrium: No Fair-Trade}\n\\noindent (5 points) Calculate the equilibrium price and quantity; show your work. Denote these values as $P_n$ and $Q_n$. Indicate these values in your graph above. \n\n%Answer Start\n\\begin{soln}\n\\noindent For $Q_n$,\n\\begin{align*}\n    MPB_n(Q_n) &= MPC_n(Q_n)\\\\\n    8000-3Q_n  &= 2Q_n\\\\\n    Q_n &= \\frac{8000}{5}\\\\\n    &= 1600.\n\\end{align*}\nFor $P_n$,\n\\begin{align*}\n    P_n &= MPC_n(Q_n)\\\\\n      &= 2Q_n\\\\\n     &= 2\\cdot 1600\\\\\n    &= 3200.\n\\end{align*}\nTherefore, the equilibrium if there is no Fair-Trade label in the agriculture product is $(Q_n, P_n) = (1,600,\\$3,200)$.\n\\end{soln}\n%Answer End\n%Q3\n%\\clearpage\n\\subsection*{A-3. Graph: Fair Trade}\n\\noindent (10 points) Suppose a third-party Fair-Trade label becomes available, for which all producers are eligible at a transaction cost of $\\$150$/tonne. Assume that every consumer values the Fair Trade label and receives $\\$550$/tonne of “warm glow” from consuming a Fair Trade certified product (in addition to their use benefits from consumption).\nIn the space below, reproduce your figure---including $P_n$ and $Q_n$---from the previous page, and overlay the demand and supply curves associated with the Fair-Trade product on this new graph.\n%Start Answer\n\\begin{soln}\n\\noindent Let $w$ and $t$ be the value of warm glow and the cost of transaction cost for producers, respectively. Then,\n    \\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.45]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,7.5)  {$P_{ft}$};\n                \\node [left] at (0,5) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node [below] at (13,0) {$2850$};\n                \n                \\node at (2.5,9)   {$a$};\n                \\node at (0.8,8.2) {$b$};\n                \\node at (1.5,6.2) {$c$};\n                \\node at (4,6.8)   {$d$};\n                \\node at (5.1,6)   {$e$};\n                \\node at (4,5.4)   {$f$};\n                \\node at (1,4)     {$g$};\n                \\node at (2,3)     {$h$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (5.5,7.5)--(5.5,0);\n                \\draw [dashed] (5,5)--(5,0);\n                \\draw [] (0,7.5)--(5.5,7.5);\n                \\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product with Fair Trade label}\n            \\label{fig:A-3}\n        \\end{center}\n    \\end{figure}\n    \\noindent If no Fair-Trade label exists, the equilibrium is $(Q_n, P_n) = (1600,3200)$, but if Fair-Trade certification is introduced, new equilibrium point is $(Q_{ft}, P_{ft}) = (1680,3510)$.\n\\end{soln}\n% %Answer End\n\\vspace{1em}\n%Q4\n\\subsection*{A-4. Equilibrium: Fair Trade}\n\\noindent (10 points) Calculate the equilibrium price $P_{ft}$ and quantity $Q_{ft}$ for the fair-trade product; show your work. Indicate these values in your graph above.\n\n%Start Answer\n\\begin{soln}\n\\noindent Given $w$ is $\\$550$ per tonne, and $t$ is $\\$150$ per tonne. Let a matrix $\\mathbf{A}$, vectors $\\mathbf{b}$, and $\\mathbf{c}$ be \n$\\begin{pmatrix}\n    3 & 1\\\\ 2 & -1\n\\end{pmatrix}$, \n$\\begin{pmatrix}\n    Q_{ft}\\\\ P_{ft}\n\\end{pmatrix}$, and\n$\\begin{pmatrix}\n    8550\\\\ -150\n\\end{pmatrix}$, respectively.\nThen, $\\mathbf{Ab}=\\mathbf{c}$ is the system of demand and supply equations in the market with Fair Trade label. Solving for $\\mathbf{b}$ brings us the equilibrium point. Since $\\mathbf{b}=\\mathbf{A}^{-1}\\mathbf{c}$\nand $\\mathbf{A}^{-1} = \\frac{1}{5}\n\\begin{pmatrix}\n    1 & 1\\\\2 &-3\n\\end{pmatrix}$,\nthen\n\n\\begin{align*}\n    \\mathbf{b} = \\begin{pmatrix}\n    Q_{ft}\\\\ P_{ft}\n\\end{pmatrix} &=\n\\frac{1}{5}\n\\begin{pmatrix}\n     1 & 1\\\\2 &-3\n\\end{pmatrix}\n\\begin{pmatrix}\n    8550\\\\ -150\n\\end{pmatrix}\\\\\n    &= \\frac{1}{5}\\begin{pmatrix}\n        8400 \\\\ 17550\n    \\end{pmatrix}\n    = \\begin{pmatrix}\n        1680 \\\\ 3510\n    \\end{pmatrix}\n\\end{align*}\n% \\begin{align*}\n%     MPB_n(Q_{ft})+w &= MPC_n(Q_{ft})+t\\\\\n%     8550-3Q_{ft}  &= 2Q_{ft}+150\\\\\n%     Q_{ft} &= \\frac{8400}{5}\\\\\n%     &= 1680.\n% \\end{align*}\n% For $P_{ft}$,\n% \\begin{align*}\n%     P_{ft} &= MPC_n(Q_{ft}) + t\\\\\n%       &= 2Q_{ft} + 150\\\\\n%      &= 2\\cdot 1600 + 150\\\\\n%     &= 3510.\n% \\end{align*}\nThe equilibrium of Fair-Trade label introduced is $(Q_{ft}, P_{ft}) = (1,680,\\$3,510)$.\n\\end{soln}\n%End Answer\n\n%Q5\n%\\clearpage\n\\subsection*{A-5. $\\Delta$CS, $\\Delta$PS, Warm Glow, and Transaction Costs in Graph}\n\\noindent (5 points) On your graph from question 3, label the areas corresponding to the change in CS, the change in PS, the total increase in transaction costs, and the total increase in warm glow. Report these graphical sums (i.e., the areas represented by labels, not the numerical values) here:\n\n\\vspace{1em}\n\\begin{soln}\n\\begin{tabular}{l l}\n    $\\Delta CS$ &= $(a+b) - (b+c+f) = a - (c+f) &gt; 0$\\\\\n    $\\Delta PS$ &= $(c+d+g)-(g+h) = c+d - h &gt;0$\n\\end{tabular}\n\\end{soln}\n\\vspace{1em}\n\n\\noindent Also indicate the areas corresponding to the total amount of warm glow and transaction costs associated with fair trade labeling:\n\n\\vspace{1em}\n\\begin{soln}\n\\begin{tabular}{l l}\n    Warm Glow         &= $a+ d$\\\\\n    Transaction Costs &= $h+f$ \n\\end{tabular}\n\\end{soln}\n\\vspace{1em}\n\n%Start Answer\n\\begin{soln}\n\\begin{table}[H]\n        \\begin{center}\n            \\begin{tabular}{llll}\n               & Before & After & $\\Delta$\\\\ \\hline \\\\\n            CS & b+c+f  & a+b   & a-(c+f) \\\\\n            PS & g+h    & c+d+g & c+d-h   \\\\ \\hline \\\\\n            TS &        &       & a+d-(h+f) \n        \\end{tabular}\n        \\end{center}\n    \\end{table}\n\\noindent The change in total welfare is decomposed into two parts: warm glow and transaction costs. The area of $a+d$ is the gain from net warm glow capture, and the area of $h+f$ is net loss of transaction costs.\n\\end{soln}\n%End Answer\n\n\\clearpage\n\\subsection*{A-6. $\\Delta$CS, $\\Delta$PS, and $\\Delta$W with Math}\n\\noindent (10 points) Calculate the following mathematically (show your work):\n\n\\vspace{1em}\n\\begin{soln}\n\\begin{tabular}{l l}\n    $\\Delta CS$ &= +\\$393,600\\\\\n    $\\Delta PS$ &= +\\$262,400\\\\\n    $\\Delta W $ &= +\\$656,000\n\\end{tabular}\n\\end{soln}\n\\vspace{1em}\n\n%Answer Start\n\\begin{soln}\n\\noindent For $\\Delta CS$, the surplus of consumer before the introduction of Fair-Trade label is defined as the area of $b+c+f$, which is\n$$\\frac{(8000-3200)\\cdot1600}{2} = \\$3,840,000.$$\nAnd the consumer surplus after the introduction of Fair-Trade label is the are of $a+b$, which is\n$$\\frac{(8550-3510)\\cdot1680}{2} = \\$4,233,600.$$\nTherefore, the change of $\\Delta CS$ is found to be:\n\\begin{align*}\n    \\Delta CS &= a+b - (b+c+f)\\\\\n    &= a-(c+f)\\\\\n    &= 4,233,600 - 3,840,000\\\\\n    &= +\\$393,600.\n\\end{align*}\n\\noindent For $\\Delta PS$, the surplus of producer before the introduction of Fair-Trade label is the area of $g+h$, which is\n$$\\frac{(3200-0)1600}{2} = \\$2,560,000.$$\nAnd the producer surplus after the introduction of Fair-Trade label is the are of $c+d+g$, which is\n$$\\frac{(3510-150)1680}{2} = \\$2,822,400.$$\nTherefore, the change of $\\Delta PS$ is\n\\begin{align*}\n    \\Delta PS &= c+d+g-(g+h)\\\\\n    &= c+d-h\\\\\n    &= 2,822,400 - 2,560,000\\\\\n    &= +\\$262,400.\n\\end{align*}\n\\noindent Overall, the change of the total welfare of the introduction of Fair-Trade label is\n\\begin{align*}\n    \\Delta W &= \\Delta CS + \\Delta PS\\\\\n    &= a-(c+f)+(c+d-h)\\\\\n    &= a+d-(f+h)\\\\\n    &= 393,600 + 262,400\\\\\n    &= +\\$656,000.\n\\end{align*}\n\\end{soln}\n%Answer End\n\n%Q7\n\\clearpage\n\\subsection*{A-7. Insufficient Warm-Glow Premium Received}\n\\noindent (5 points) In your own words, explain in economic terms why/whether $P_{ft}$ is not $\\$550$/tonne higher than the $P_n$. \nBonus: how does the increase in production cost for Fair-Trade products contribute to the change the price?\n\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.4]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,7.7)  {$P_{ft}$};\n                \\node [left] at (0,6.7)  {$P'$};\n                \\node [left] at (0,5.7)  {$P_{ft}-t$};\n                \\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                \\draw [dashed] (0,5)--(5,5)--(5,0);\n                \\draw [dashed] (0,5.5)--(5.5,5.5);\n                \\draw [dashed] (0,6.5)--(6.5,6.5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Fair Trade and Price Premium}\n            \\label{fig:A-7}\n        \\end{center}\n    \\end{figure}\n\n\\doublespacing\nAlthough the price $P_{ft}$ becomes higher, the rise is not by the full amount of the warm glow. This amount received is lower than the amount paid because of two effects: consumption and production sides.\n\nFrom the consumption's point of view, consumers buy more products with seeking a lower price. From the equilibrium point $(Q_n, P_n)$, after Fair-Trade label implemented, consumer's willingness-to-pay increases by warm glow, $w=\\$550$, shifting their demand up $P_n+w$. Since consumers seek lower price by increasing their consumption, price moves along the demand curve downward. Due to the upward sloping marginal private production, at the new equilibrium point, new price $P'$ is higher than the old price $P_n$, but $P'&lt;P_n+w$.\n\nFrom the production standpoint, as long as the supply is upward-sloping curve, warm glow allows new equilibrium, $P'$, to be higher price and quantity. As the quantity supplied increases, more opportunity cost is incurred with the price hiking. Now, an additional production cost, such as certification or transaction costs, brings the product price to rise to $P'+t$. Then, consumers reduce their consumption and farmers decrease their quantity supplied as well. At the end point, the price converges to $P_{ft}$, where $P_{ft}&gt;P'&gt;P_n$.\n\nFigrure \\ref{fig:A-7} indicates that $P_n+w \\geq P_{ft}$ or $P_n+w-(t+P_n) \\geq P_{ft} -(t+P_n)$, so $w-t \\geq P_{ft} -(t+P_n)$. If warm glow is sufficiently higher than transaction costs, $w&gt;t$, then it is likely to have positive price premium. However, if $w&lt;t$, then the price premium must be negative. In this case which $w=\\$550&gt;t=\\$150$, farmer could gain positive price premium, $P_{ft}-t-P_n = \\$160$ at most, which is obviously less than $w=\\$550$.\n\n\\end{soln}\n%Answer End\n\n%Q8\n\\clearpage\n\\subsection*{A-8. Fair-Trade Label Analysis with Perfect Elastic Demand}\n\\noindent (20 points) Suppose that, instead of a downward-sloping MPB curve, the MPB curve for consuming the good is flat at $\\$A$/tonne. Assuming still that the transaction cost is $\\$150$/tonne and consumers’ warm glow is $\\$550$/tonne, show mathematically and graphically that the label raises producer welfare but has no effect on consumer surplus. Assume that the MPC remains the same as $MPC = 2Q$. (I.e., compare CS, PS with and without the Fair-Trade label when the MPB curve is flat in both scenarios.)\n\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.5]\n                \\draw[thick,&lt;-&gt;] (0,12) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                %\\node [left] at (0,13) {$8550$};\n                %\\node [left] at (0,10) {$8000$};\n                %\\node [left] at (0,7.5)  {$P_{ft}$};\n                %\\node [left] at (0,5.7)  {$P_{ft}-t$};\n                %\\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,8) {$A+w=P_{ft}$};\n                \\node [left] at (0,5) {$A=P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (5,0) {$Q_n$};\n                \\node [below] at (6.2,0) {$Q_{ft}$};\n                %\\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node at (2.5,6.5) {$a$};\n                \\node at (0.9,4.1) {$b$};\n                \\node at (2,3) {$c$};\n                \\node at (5.4,6.5){$d$};\n                \\node at (5.5,4.5){$E$};\n                \\node at (6,8.5){$E'$};\n                \n                \\draw (0,0)--(9,9)  node[right] {$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,8)--(9,8) node [right] {$MPB_n+w$};\n                \\draw (0,5)--(9,5) node [right] {$MPB_n$};\n                %\\node [above] at (10,1) {$MPB_n$};\n                %\\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (6,8)--(6,0);\n                \\draw [dashed] (5,5)--(5,0);\n                %\\draw [dashed] (0,5.5)--(5,5.5);\n            \\end{tikzpicture}\n            \\caption{Perfect Elastic Demand and Supply for An Agricultural Product with Fair Trade}\n            \\label{fig:A-8}\n        \\end{center}\n    \\end{figure}\n    \\begin{table}[H]\n        \\begin{center}\n            \\begin{tabular}{llll}\n               & Before & After & $\\Delta$\\\\ \\hline \\\\\n            CS & 0 & 0 & 0 \\\\\n            PS & b+c & a+b & a-c \\\\ \\hline \\\\\n            TS &  &  & a-c \n        \\end{tabular}\n        \\end{center}\n    \\end{table}\n\n\\noindent  Suppose MPB is flat, so the demand function of the agriculture product is $A=P=MPB$ and also $MPC$ is a linear function of quantity, Q, $MPC = B+sQ$, where $B=0$ if no Fair-Trade label, $B=\\$150$ otherwise. Also Let $w$ and $t$ be \"warm glow\" and \"transaction costs\", respectively.\\\\\nThen, there an equilibrium $E$ exists such that $E = (Q_n, A)$ if no Fair-Trade label, and $E'$ at $(Q_{ft}, A+w)$ otherwise. Then, before introducing Fair-Trade label, for $CS_{before}$,\n\\begin{align*}\n    CS_{before} &= \\int\\displaylimits_{Q\\leq Q_n} MPB\\left(Q\\right) \\mathrm{d}Q - P_n\\cdot Q_n\\\\\n    &= \\int_0^{Q_n} A\\mathrm{d}Q - A\\cdot Q_n\\\\\n    &= A\\cdot Q_n - A\\cdot Q_n\\\\\n    &= 0.\n\\end{align*}\n\\noindent For $PS_{before}$,\n\\begin{align*}\n    PS_{before} &= P_n\\cdot Q_n - \\int\\displaylimits_{Q\\leq Q_n} MPC\\left(Q\\right) \\mathrm{d}Q\\\\\n    &= P_n\\cdot Q_n - \\int_0^{Q_n} sQ\\mathrm{d}Q\\\\\n    &= A\\cdot Q_n - \\left[\\frac{s}{2}Q_n^2\\right]\\\\\n    &= b+c.\n\\end{align*}\n\\noindent Now by \"warm glow\", $w$, $MPB_n$ is shifted to $MPB_n+ w$ and by \"transation costs\", $t$, $MPC_n$ is also shifted to $MPC_n+t$. Then, after introducing Fair-Trade label, for $CS_{after}$,\n\\begin{align*}\n    CS_{after} &= \\int\\displaylimits_{Q\\leq Q_{ft}} MPB'\\left(Q\\right) \\mathrm{d}Q - P_{ft}\\cdot Q_{ft}\\\\\n    &= \\int_0^{Q_{ft}} A+w\\mathrm{d}Q - (A+w)\\cdot Q_{ft}\\\\\n    &= (A+w)\\cdot Q_{ft} - (a+w)\\cdot Q_{ft}\\\\\n    &= 0.\n\\end{align*}\n\\noindent For $PS_{after}$,\n\\begin{align*}\n    PS_{after} &= P_{ft}\\cdot Q_{ft} - \\int\\displaylimits_{Q\\leq Q_{ft}} MPC\\left(Q\\right) \\mathrm{d}Q\\\\\n    &= P_{ft}\\cdot Q_{ft} - \\int_0^{Q_{ft}} B+sQ\\mathrm{d}Q\\\\\n    &= (A+w)\\cdot Q_{ft} - \\left[BQ_{ft}+\\frac{s}{2}Q_{ft}^2\\right]\\\\\n    &= a+b.\n\\end{align*}\nTherefore, the changes of $CS$ and $PS$ are:\n\\begin{align*}\n    \\Delta CS &= CS_{after} -CS_{before}\\\\\n    &= 0 - 0\\\\\n    &= \\$0.\n\\end{align*}\n\\begin{align*}\n    \\Delta PS &= PS_{after} -PS_{before}\\\\\n    &= \\frac{(A+w-t)\\cdot Q_{ft}}{2} - \\frac{A\\cdot Q_n}{2}\\\\\n    &= \\frac{(A+w-t)^2}{4} - \\frac{A^2}{4}\\\\\n    &= \\frac{(2A+w-t)(A+w-t-A)}{4}\\\\\n    &= \\$\\frac{(2A+w-t)(w-t)}{4}\n\\end{align*}\nOverall, given $w=550$ and $t=150$, the change in the total surplus of Fair-Trade labelling is\n\\begin{align*}\n    \\Delta TS &= 0 + 200A+40000 = +\\$200A+40000.\n\\end{align*}\n\\end{soln}\n%Answer End\n\n%Q9\n\\clearpage\n\\subsection*{A-9. Growing Warm Glow}\n\\noindent (15 points) Now suppose that instead of being the same for each unit consumed, the “warm glow” consumers receive is $550 + \\frac{Q}{4}$. Continue to assume that consumers’ use value is $8000-3Q$, the Fair-Trade transaction cost is $\\$150$/tonne, and $MPC = 2Q$. Use a graph and math to calculate the change in CS and PS when the Fair-Trade label is implemented. Also, in your own words, interpret the warm glow function---e.g. is marginal warm glow increasing or decreasing with consumption?---and explain whether you think this is realistic. Eg., for a product such as bananas, do you think the warm glow received from the final unit consumed would be larger or smaller than the warm glow received from the first unit consumed?\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n        \\begin{center}\n            \\begin{tikzpicture}[scale=0.5]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(17,0) node[right]{$Q(tonne)$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,8.1)  {$P_{ft}$};\n                \\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6.5,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node [below] at (16,0) {$\\frac{34200}{11}$};\n                \n                \\node at (2.5,9.5) {$a$};\n                \\node at (0.6,8.7)   {$b$};\n                \\node at (1.5,6.2) {$c$};\n                \\node at (4,7){$d$};\n                \\node at (5.1,6.2){$e$};\n                \\node at (4,5.4) {$f$};\n                \\node at (1,4) {$g$};\n                \\node at (2,3) {$h$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(16,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (15,3) {$MPB_n+w(Q)$};\n                \\draw [dashed] (6.1,8.1)--(6.1,0);\n                \\draw [dashed] (5,5)--(5,0);\n                \\draw [] (0,8.1)--(6.1,8.1);\n                \\draw [] (0,5)--(5,5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Demand and Supply for An Agricultural Product with Fair Trade Increasing Warm Glow}\n            \\label{fig:A-9}\n        \\end{center}\n    \\end{figure}\n    \n\\noindent Before Fair-Trade label implemented, the demand and supply functions are\n$$\n\\begin{cases}\n    MPB &= 8000-3Q\\\\\n    MPC &= 2Q.\n\\end{cases}\n$$\nSo, the equilibrium is where $MPB(Q_n)=MPC(Q_n)$\n\\begin{align*}\n    8000-3Q_n &= 2Q_n\\\\\n    Q_n &= 1600\\\\\n    P_n &= 3200\n\\end{align*}\nOnce the Fair-Trade label is introduced, if warm glow is defined as $550+\\frac{Q}{4}$ and FT transaction cost as $\\$150$, there an new equilibrium exists where $MPB(Q_{ft})+w(Q_{ft}) = MPC(Q_{ft})+t$:\n\\begin{align*}\n    MPB(Q_{ft})+w(Q_{ft}) &= MPC(Q_{ft})+t\\\\\n    8000-3Q_{ft}+550+\\frac{Q_{ft}}{4} &= 2Q_{ft}+150\\\\\n    Q_{ft} &= \\frac{33600}{19} \\approx 1,768.42\\\\\n    P_{ft} &= \\frac{70050}{19} \\approx \\$3,686.84\n\\end{align*}\nThen, calculate the change in consumer surplus:\n\\begin{align*}\n    \\Delta CS &= (a+ b) - (b+c+f)\\\\\n    &= \\frac{1}{2}\\left(8550-\\frac{70050}{19}\\right)\\frac{33600}{19} - \\frac{1}{2}\\left(8000-3200\\right)1600\\\\\n    &=\\frac{166080000}{361} \\approx \\$460,055.40.\n\\end{align*}\nFor the change of producer surplus is:\n\\begin{align*}\n    \\Delta PS &= (c+d+g)-(g+h)\\\\\n    &= \\frac{1}{2}\\left(\\frac{70050}{19}-150\\right)\\frac{33600}{19} - \\frac{1}{2}\\left(3200-0\\right)1600\\\\\n    &= \\frac{204800000}{361} \\approx \\$567,313.02.\n\\end{align*}\n\n\\doublespacing\nIncreasing the warm glow function as the quantity goes up means that the interval $I \\in \\{Q | (0,\\frac{34200}{11})\\}$, the warm glow function $f$ is $f(a) \\geq f(b), \\forall a&gt;b \\in I$, or $f'(Q)&gt;0, \\forall Q \\in I$. In this case, the more quantity demanded, the more warm glow is added; or, as an additional unit of the agriculture product consumed, the dollar value of warm glow increases by $f'(Q) = \\$\\frac{1}{4}$. Overall, the slope of the demand becomes relatively flat compared to a constant warm glow.\n\n\n\\doublespacing\nThis is an unrealistic assumption. However, we believe that it depends on a market product.\n%The higher price elasticity of demand implies that consumers are more willing and able to seek out substitutes for the agriculture product after a price change.\\\\\nOne of the aims at supporting Fair Trade's products is for small-scaled farmers and creators.  For a small quantity of a product, \n%The warm glow received from the final unit consumed would be smaller than the warm glow received from the first unity consumed. \nconsumers are willing to pay a large amount of warm glow, while for a high volume of a product, it is likely to be a small amount of warm glow. It is unlikely that consumers will be willing to pay a lot of warm glow for large quantities of produced goods. Also, with increasing the value of the brand, like the Fair Trade label, by reducing the willingness to search for substitutes, the willingness to pay becomes insensitive as quantity demanded rises. Therefore, decreasing the warm glow function with respect to quantity is realistic assumption for small-quantity in the market.\n\n\\doublespacing\nAnother aim of Fair Trade is to support farmers and workers to get paid a better price or wage and support the associated communities. The most common products, such as coffee and cacao, are produced in large scale with typically exported from the developing counties to the developed countries. For a large-scaled quantity of a product, especially plantations, consumer are likely willing to pay a relative large amount of warm glow by endorsing and consumer more in order to support workers, who work in severe conditions.\nHence, in this case, increasing the warm glow function with respect to quantity is valid assumption for large-quantity in the market.\n\n\\doublespacing\nOverall, we believe that a different product has a different warm glow function; however, we might assume that overall warm glow is a decreasing function with concave up, instead of a constant rate increasing function. For a product of small-scaled production, warm glow function is decreasing, but for large-scaled quantity goods, warm glow function is increasing.\n\n\\begin{figure}[H]\n\\begin{minipage}[c]{0.5\\linewidth}\n\\begin{tikzpicture}[scale=0.4]\n% Axis\n\\draw [thick](0,0) -- (10,0);\n\\draw [thick](0,0) -- (0,8);\n\\node [above] at (-0.2,8) {$Warm Glow (\\$/tonne)$};\n\\node [right] at (10,-0.2) {$Q(tonne)$};\n\\draw [thick] (0,2)--(9,7) node[right]{$w(Q)$};\n\\end{tikzpicture}\n\\caption{Increasing Warm Glow Function}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[c]{0.5\\linewidth}\n\\begin{tikzpicture}[scale=0.4]\n% Axis\n\\draw [thick](0,0) -- (10,0);\n\\draw [thick](0,0) -- (0,8);\n\\node [above] at (-0.2,8) {$Warm Glow (\\$/tonne)$};\n\\node [right] at (10,-0.2) {$Q(tonne)$};\n%Curve\n\\draw [thick] (1,8) to [out=280,in=175] (9,1);\n\\node [right] at (9,1) {$w(Q)$};\n\\end{tikzpicture}\n\\caption{Decreasing Warm Glow with Concave up}\n\\end{minipage}\n\\end{figure}\n\n\\end{soln}\n%Answer End\n\n%\\clearpage\n\\subsection*{A-10. Price Elasticity and Markets}\n\\noindent (10 points) Comparing your results from questions 8 and 9, explain in your own words why the allocation of benefits across consumers and producers differs in the two scenarios. Also conjecture which scenario (that in question 8 or question 9) would be more accurate in the case of a Fair Trade product such as coffee; explain your answer.\n%Answer Start\n\\begin{soln}\n\\begin{figure}[H]\n    \\begin{minipage}[c]{0.5\\linewidth}\n        \\begin{tikzpicture}[scale=0.35]\n            \\draw[thick,&lt;-&gt;] (0,13) node[above]{$\\$/tonne$}--(0,0)--(13,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                %\\node [left] at (0,13) {$8550$};\n                %\\node [left] at (0,10) {$8000$};\n                %\\node [left] at (0,7.5)  {$P_{ft}$};\n                %\\node [left] at (0,5.7)  {$P_{ft}-t$};\n                %\\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,8) {$P_{ft}=A+w$};\n                \\node [left] at (0,5) {$P_n=A$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (5,0) {$Q_n$};\n                \\node [below] at (6.2,0) {$Q_{ft}$};\n                %\\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \\node at (2.5,6.5) {$a$};\n                \\node at (0.9,4.1) {$b$};\n                \\node at (2,3) {$c$};\n                \\node at (5.4,6.5){$d$};\n                \\node at (5.5,4.5){$E$};\n                \\node at (6,8.5){$E'$};\n                \n                \\draw (0,0)--(9,9)  node[right] {$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,8)--(9,8) node [right] {$MPB_n+w$};\n                \\draw (0,5)--(9,5) node [right] {$MPB_n$};\n                %\\node [above] at (10,1) {$MPB_n$};\n                %\\node [above] at (11,4) {$MPB_n+w$};\n                \\draw [dashed] (6,8)--(6,0);\n                \\draw [dashed] (5,5)--(5,0);\n                %\\draw [dashed] (0,5.5)--(5,5.5);\n        \\end{tikzpicture}\n    \\caption{Perfect Elastic Demand with FT}\n    \\label{fig:A-10}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}[c]{0.5\\linewidth}\n        \\begin{tikzpicture}[scale=0.35]\n            \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(17,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                \\node [left] at (0,13) {$8550$};\n                \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,8.1)  {$P_{ft}$};\n                \\node [left] at (0,4.6) {$P_n$};\n                \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.8,0) {$Q_n$};\n                \\node [below] at (6.5,0) {$Q_{ft}$};\n                \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\node at (2.5,9.5) {$a$};\n                \\node at (0.6,8.7)   {$b$};\n                \\node at (1.5,6.2) {$c$};\n                \\node at (4,7){$d$};\n                \\node at (5.1,6.2){$e$};\n                \\node at (4,5.4) {$f$};\n                \\node at (1,4) {$g$};\n                \\node at (2,3) {$h$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(16,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (15,3) {$MPB_n+w(Q)$};\n                \\draw [dashed] (6.1,8.1)--(6.1,0);\n                \\draw [dashed] (5,5)--(5,0);\n                \\draw [] (0,8.1)--(6.1,8.1);\n                \\draw [] (0,5)--(5,5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n        \\end{tikzpicture}\n    \\caption{FT with Increasing Warm Glow}\n    \\label{fig:A-10-1}\n    \\end{minipage}\n\\end{figure}\n% compare elasticity\n\\doublespacing\n\\noindent A main difference between questions 8 and 9 is the price elasticity of demand for the good. In question 8, the price elasticity of demand is infinite, its in question 9 is relatively inelastic, while $MPC$ is same for both questions.\nDue to the price elasticity, the allocation of benefits is different in the two scenarios.\n% price and quantity\nThe introduction of the Fair-Trade label increases the equilibrium prices due to the existence of warm glow and transaction costs; but the equilibrium quantity depends on the amount of warm glow compared to the amount of transaction costs. \n\n\\doublespacing\n% welfare change CS PS warm glow and transaction costs\nAs found in question 8, the equilibrium price is determined by the $A$ and $w$, and the change in consumer surplus is zero, while the change in producer surplus is the area of $a-c$. So, the total change in welfare under the perfect elastic demand is $a-c$. Therefore, the net benefit of warm glow capture is the area of $a$, and the loss of transaction costs is $c$.\nOn the other hand in question 9, the equilibrium price depends on the demand and supply functions as well as the values of $w$ and $t$. And the change in consumer surplus is the area of $a-(c+f)$, while the change in producer surplus is $c+d-h$. So, the total change in welfare under the perfect elastic demand is $a+d-(f+h)$. The net welfare gain of warm glow capture is the area of $a+d$, and the welfare loss due to transaction costs is $f+h$.\n\n\\doublespacing\n% incidence \nFarmers are more likely to bear the transaction costs of Fair Trade if demand is more elastic. Under the perfect price elastic for demand, all of the transaction costs are borne by the farmers, and the premium paid to the farmers are $A+w-t$. However if the elasticity is relatively less, consumers can bear the increased transaction costs. The amount of warm glow greater than the costs, $w&gt;t$, creates a positive premium.\n\n% % compare increasing warm glow\nQuestion 9 is more accurate in the the case of a Fair Trade product, such as coffee. The price elasticity of demand with Fair Trade production, especially for coffee, could be inelastic compared to an infinite elasticity, with less possibility of consumers' substitution to the other products. Hence, an increase in production costs erodes the consumer's warm glow, but the farmers bear less of the burden.\n\n\\end{soln}\n%Answer End\n\n%\\clearpage\n\\vspace{5em}\n\\subsection*{A-11. Significance of Introducing Warm Glow and Truth Disclosure.}\n\\noindent (10 points) In your own words, discuss why/whether it makes sense to include “warm glow” in measures of economic benefits from a market. \nAlso discuss whether economic value is destroyed when consumers are informed that, for products such as coffee, very little of the Fair Trade price premium actually passes through to farmers and farm workers. \n\n%Answer Start\n\\begin{soln}\n\\doublespacing\n% reason of measuring warm glow\nPerfect competitive market allows consumers to have complete or \"perfect\" information about the product being sold and the prices charged by each production any time. So under this conditions, the inclusion of economic benefit of warm glow does not make sense. \nHowever, when consumers believe information, even if based on a false information, that workers on the farm are in a weak position, such as not receiving a fair wage or working under severe conditions, they will continue to purchase the product at a fair price, relative large price, in order to improve the lives of producers and workers in developing communities in a vulnerable position and to achieve self-reliance. And paying with warm glow makes consumers happy, so this increased willingness-to-pay should be measured in the economic benefits.\n%The warm glow represents the selfish joy that comes from \"doing good\" by \"doing your part\" to help others. \nAlso, empirical evidence show that Fair Trade producers gain higher prices than conventional farmers for their products. At the same time, surveys show that the premium paid by consumers is less than the premium received to farmers.\nIntegration of the warm glow into an economic model provides an explanation for the rise in Fair Trade prices and premiums.\n\n% Eroding Demand\n\\begin{figure}[H]\n    \\begin{minipage}[c]{0.5\\linewidth}\n            \\begin{tikzpicture}[scale=0.3]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                % \\node [left] at (0,13) {$8550$};\n                % \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,7.7)  {$P_{ft}$};\n                %\\node [left] at (0,6.7)  {$P'$};\n                \\node [left] at (0,5.7)  {$P_{ft}-t$};\n                \\node [left] at (0,4.6) {$P_n$};\n                % \\node [left] at (0,2) {$150$};\n                \\node [below] at (4.5,0) {$Q_n$};\n                \\node [below] at (6.5,0) {$Q_{ft}$};\n                % \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,2)--(9,11) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (12,4) {$MPB_n+w$};\n                \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                \\draw [dashed] (0,5)--(5,5)--(5,0);\n                \\draw [dashed] (0,5.5)--(5.5,5.5);\n                %\\draw [dashed] (0,6.5)--(6.5,6.5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Warm Glow $\\geq$ Transaction costs}\n            \\label{fig:A-11-1}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}[c]{0.5\\linewidth}\n            \\begin{tikzpicture}[scale=0.3]\n                \\draw[thick,&lt;-&gt;] (0,14) node[above]{$\\$/tonne$}--(0,0)--(15,0) node[right]{$Q$};\n                \\node [below left] at (0,0) {$0$};\n                \n                % \\node [left] at (0,13) {$8550$};\n                % \\node [left] at (0,10) {$8000$};\n                \\node [left] at (0,8.7)  {$P_{ft}$};\n                % \\node [left] at (0,6.7)  {$P'$};\n                \\node [left] at (0,4)  {$P_{ft}-t$};\n                \\node [left] at (0,5.5) {$P_n$};\n                %\\node [left] at (0,2) {$150$};\n                \\node [below] at (4,0) {$Q_{ft}$};\n                \\node [below] at (5.8,0) {$Q_n$};\n                % \\node [below] at (10,0) {$\\frac{8000}{3}$};\n                \n                \\draw (0,0)--(9,9) node[right]{$MPC_n$};\n                \\draw (0,4)--(9,13) node[right] {$MPC_n+t$};\n                \\draw (0,10)--(10,0);\n                \\draw (0,13)--(13,0);\n                \\node [above] at (10,1) {$MPB_n$};\n                \\node [above] at (12,4) {$MPB_n+w$};\n                \\draw [dashed] (0,5)--(5,5)--(5,0);\n                \\draw [dashed] (0,8.5)--(4.5,8.5)--(4.5,0);\n                \\draw [dashed] (0,4.3)--(4.5,4.3);\n                % \\draw [dashed] (0,6.5)--(6.5,6.5);\n                %\\draw [] (0,5)--(5,5);\n                % \\draw [dashed] (0,7.5)--(5.5,7.5)--(5.5,0);\n                % \\draw [dashed] (0,5)--(5,5)--(5,0);\n            \\end{tikzpicture}\n            \\caption{Warm Glow $\\leq$ Transaction Costs}\n            \\label{fig:A-11-2}\n    \\end{minipage}\n\\end{figure}\n\n\\doublespacing\n%w is exogenous\nWarm glow is defined exogenously, with an default value of $w$, which is assumed to be greater than the transaction costs, $t$, due to Fair Trade certification.\n% w&gt;t -&gt; positive premium\nConsumers believe that the all amount of warm glow $w$ goes directly to farmers. However, in question 7, as long as $w&gt;t$, positive price premium, $P_{ft}-t-P_n$ in Figure \\ref{fig:A-11-1}, goes to farmers, where $w$ will be subtracted with $t$. \n% w&lt;t -&gt; negative premium\nWhen consumers are informed that price premium farmers actually gets is less than the warm glow, or that money is not used for an improvement of schools or hospitals, then it shrinks demand with new warm glow $w' \\approx P_{ft}-t-P_n$, or even zero.\nPrice premium become negative if new warm glow, $w'$ is less than the transaction costs per unit $t$. This demand shrink erodes the Fair Trade price premium, and very little or even negative the premium, $P_{ft}-t-P_n$ in Figure \\ref{fig:A-11-2}, is paid to farm workers. Eventually, the economic value of the Fair Trade certification is negative.\n\n\\end{soln}\n%Answer End\n\n\\clearpage\n\\section*{Part B: Stats, Mental Accounting, and Food Stamps}\n\\noindent Using Google Scholar (https://scholar.google.ca/) or similar academic search engine, find a peer-reviewed article that gives statistical evidence of whether recipients of food stamps treat those benefits as fungible with other income and/or whether the recipients engage in mental accounting. You are welcome to review more than one scholarly article when executing this part of the assignment, but you only need to provide the following information for a single article. \n%Q1\n\\subsection*{B-a. Introduction of Study}\n\\noindent (10 points) Bibliographic information (use MLA or APA style)\n%\\clearpage\n%Answer Start\n\\begin{soln}\n\\bibliographystyle{apacite}\n\\bibliography{fre460a5.bib}\n\\nocite{*}\n\\end{soln}\n%Answer End\n\n%Q2\n\\subsection*{B-b. Data Description}\n\\noindent (20 points, approximately 200 words) Description of the data used by the authors, e.g. geographic location, years covered, how the data was generated/collected\\footnote{E.g. scanner data, classroom experiment, field experiment, household survey}, and key variables of interest. \n\n%Answer Start\n\\begin{soln}\n\\doublespacing\n\\citet{hastings2018snap} study how the Supplemental Nutrition Assistance Program (SNAP) influences spending by households. The SNAP program provides eligible households with a monthly electronic benefit that can only be spent on grocery stores. In the paper, the authors track detailed transaction records including method of payment that allows authors to infer to SNAP participation from February 2006 to December 2012 for regular customers at large grocery stores in Rhode Island. The food consumption trend of nearly half a million regular customers of a large US grocery stores is analyzed and studied. The authors are interested in the change of food consumption in response to the access to the SNAP benefit, increase in SNAP benefits and when the benefits run out. Then the authors compare the consumption difference when people receive cash benefits versus SNAP benefit to determine whether people treat different sources of income differently (Cash benefit vs. SNAP benefit). Additionally, the authors also analyze the percentage of SNAP benefit spend on food versus the percentage of SNAP benefit spend on other items to determine the SNAP benefit’s fungibility-shifting the purchase of food to purchase of other items with SNAP benefit. \n\n\\end{soln}\n%Answer End\n%Q3\n\\clearpage\n\\subsection*{B-c. Findings}\n\\noindent (30 points, approximately 300 words) Describe the authors’ central findings, particularly as they relate to the perceived fungibility of benefits received in the form of food stamps and other income.\n\n%Answer Start\n\\begin{soln}\n\\doublespacing\nThe authors develop three approaches to analyze the causal effect between SNAP and household spending. The three approaches include food consumption trends when households cross the eligibility threshold for SNAP, when the benefit ends, and when the SNAP benefit is increased. In all three cases, the marginal propensity to consume SNAP-eligible food (MPCF) out of SNAP benefit is $0.5$ to $0.6$. In the other words, approximately $50$ to $60$ cents out of one additional dollar of SNAP benefits gets spent on food. Compared to people when they get cash benefits or cash income, they spend less than $10$ cents out of every dollar on food. The main finding of the study indicates that the MPCF out of SNAP is greater than the MPCF out of other income sources. In addition, the greater MPCF for SNAP compared to cash is attributed to people’s separate mental account when they treat SNAP. The structured qualitative interviews reveal that some households report that they have planned to spend SNAP differently from cash, and quantitative evidence suggests that household reduce shopping effort for SNAP-eligible product more than for SNAP-ineligible product. The finding of the study has an important implication on economic activity. As people spend quickly after the recipient of SNAP benefit, it has a positive effect on total economic activity. Most importantly, the greater MPCF for SNAP confirms the benefit of SNAP on spending in food in food retail stores, thus improving food consumption among low income households.\n\n\\end{soln}\n%Answer End\n\n%Q4\n\\clearpage\n\\subsection*{B-d. Discussion for BC Food Insecurity}\n\\noindent (30 points, approximately 300 words) Discuss whether you think British Columbia should commence a food stamp program to address food insecurity. Be sure to allocate at least half your answer to discussing the evidence reported in the research study discussed in parts (b) and (c) of this question, but you are also welcome to discuss relevant issues that are commonly ignored by economists---stigma, for example. As always, answers supported with empirical evidence, rather than simply opinion, are the most persuasive. Be sure to cite any additional sources used to support your answers and give full bibliographic information for each\\footnote{Bibliographic information for these additional sources does not count toward the word count (300 words) recommended for this question.}.\n\n%Answer Start\n\\begin{soln}\n\\doublespacing\nI think British Columbia should commence a food stamp program to address the issue of food insecurity. Referring to the main conclusion on the study mentioned in part b, data from large grocery stores in Rhode Island is retrieved and studied in order to draw a relationship between the receipt of SNAP benefits and household consumption trend. Three approaches are developed to draw causal inference, and the study finds that the MPCF out of SNAP benefit is $0.5$ to $0.6$ which is significantly larger than the MPCF for other income sources (e.g. cash). The SNAP program reflects its goal in increasing food purchases among low income households. Additionally, there is another study suggests a positive correlation between SNAP and improved nutritional outcomes and negative correlation between SNAP and health care costs. Adults in households with food insecurity are more likely to use health care. The result of the study shows that $71\\%$ very low food-secure households use health care, compared to only $13\\%$ marginal food-secure households use health care \\citep{hastings2018snap}. Food insecurity imposes a heavy economic burden on society, including reduced productivity and increased health care costs. SNAP is a security net to protect people who are vulnerable to food insecurity. Average of $\\$1.40$ per person per meal in 2017 form a security net for the health and well-being of people with low income, which eventually lifting millions out of poverty and hunger \\citep{hastings2018snap}. The cost of implementing food stamp program is small relative to the cost of not implementing the program. Not only the existed food stamp program accomplished its goal in improving food security, but the program is also feasible from the financial feasibility and economic efficiency perspectives. \n\n\\end{soln}\n%Answer End\n\n%Reference list\n\\clearpage\n\\begin{soln}\n\\singlespacing\n%\\bibliographystyle{apacite}\n\\bibliography{fre460a5.bib}\n\\nocite{*}\n\\end{soln}\n\\end{document}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m a recent graduate with a Master’s degree in Natural Resources and Environmental Studies from the University of Northern BC, where I worked under the supervision of Dr. Balbinder Deo. My research focused on estimating food loss and waste in restaurant operations through an in-depth case study—a topic that bridges my academic interests with real-world sustainability challenges.\nCurrently, I’m seeking opportunities to apply my research skills and passion for environmental studies in a professional setting. My academic journey has equipped me with expertise in quantitative analysis and a deep understanding of resource management, while my hands-on experience has given me practical insights into the food service industry.\n\n\n\nThe word Himagineer blends the Japanese concept of “Hima” (暇), representing leisure or free time, with the English term “Imagineer,” signifying someone who creates and shares innovative ideas. For me, Himagineer encapsulates a personal philosophy: it’s during these moments of “Hima” – my leisure time – that I dedicate myself to “imagineering” pleasant information. This means engaging in activities where I can create, innovate, and share insights, turning personal pursuits into a source of shared knowledge and inspiration.\n\n\n\nEconomics & Analysis\nConsumer Theory in Microeconomics\nManagerial Economics\nCosts and Inventory Analysis\nGenerational Accounting Model\nRegression Analysis in Statistics\nExperimental Design\nState-Space (Bayesian) Model\nEmerging Technologies\nText Analysis in Computational Social Science\nReal-World Application\nCooking!!\nI’ve spent over ten years working in Chinese and Japanese restaurants, gaining invaluable experience in food preparation, kitchen operations, and understanding the practical challenges of food waste reduction.\nThis unique combination of academic rigour and industry experience shapes my perspective on sustainability, economics, and the intersection of theory and practice. Through this blog, I’ll share insights from my research, thoughts on current topics in environmental studies, statistics, and economics, and perhaps a few culinary adventures along the way.\nThanks for stopping by, and I hope you find something here that sparks your interest!"
  },
  {
    "objectID": "about.html#hello-world",
    "href": "about.html#hello-world",
    "title": "About me",
    "section": "",
    "text": "I’m a recent graduate with a Master’s degree in Natural Resources and Environmental Studies from the University of Northern BC, where I worked under the supervision of Dr. Balbinder Deo. My research focused on estimating food loss and waste in restaurant operations through an in-depth case study—a topic that bridges my academic interests with real-world sustainability challenges.\nCurrently, I’m seeking opportunities to apply my research skills and passion for environmental studies in a professional setting. My academic journey has equipped me with expertise in quantitative analysis and a deep understanding of resource management, while my hands-on experience has given me practical insights into the food service industry."
  },
  {
    "objectID": "about.html#what-is-himagineer",
    "href": "about.html#what-is-himagineer",
    "title": "About me",
    "section": "",
    "text": "The word Himagineer blends the Japanese concept of “Hima” (暇), representing leisure or free time, with the English term “Imagineer,” signifying someone who creates and shares innovative ideas. For me, Himagineer encapsulates a personal philosophy: it’s during these moments of “Hima” – my leisure time – that I dedicate myself to “imagineering” pleasant information. This means engaging in activities where I can create, innovate, and share insights, turning personal pursuits into a source of shared knowledge and inspiration."
  },
  {
    "objectID": "about.html#what-drives-my-curiosity",
    "href": "about.html#what-drives-my-curiosity",
    "title": "About me",
    "section": "",
    "text": "Economics & Analysis\nConsumer Theory in Microeconomics\nManagerial Economics\nCosts and Inventory Analysis\nGenerational Accounting Model\nRegression Analysis in Statistics\nExperimental Design\nState-Space (Bayesian) Model\nEmerging Technologies\nText Analysis in Computational Social Science\nReal-World Application\nCooking!!\nI’ve spent over ten years working in Chinese and Japanese restaurants, gaining invaluable experience in food preparation, kitchen operations, and understanding the practical challenges of food waste reduction.\nThis unique combination of academic rigour and industry experience shapes my perspective on sustainability, economics, and the intersection of theory and practice. Through this blog, I’ll share insights from my research, thoughts on current topics in environmental studies, statistics, and economics, and perhaps a few culinary adventures along the way.\nThanks for stopping by, and I hope you find something here that sparks your interest!"
  }
]